
<!DOCTYPE html>


<html lang="ko" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="성능 튜닝 가이드" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/recipes/recipes/tuning_guide.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="저자: Szymon Migacz 역자: 오왕택 성능 튜닝 가이드는 PyTorch에서 딥러닝 모델의 학습이나 추론 속도를 향상시킬 수 있는 최적화 기법과 같은 좋은 예시를 소개합니다. 제시된 기법은 몇 줄의 코드만 변경해서 구현 가능하며, 모든 도메인의 다양한 딥러닝 모델에 적용할 수 있습니다. 일반적인 최적화 기법: 비동기식으로 데이터 가져오기 및 데이터 증강법: torch.utils.data.DataLoader 는 각각 워커의 subprocess에서 비동기식 데이터 로딩과 데이터 증강을 지원합니다. DataLoader 의 n..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="저자: Szymon Migacz 역자: 오왕택 성능 튜닝 가이드는 PyTorch에서 딥러닝 모델의 학습이나 추론 속도를 향상시킬 수 있는 최적화 기법과 같은 좋은 예시를 소개합니다. 제시된 기법은 몇 줄의 코드만 변경해서 구현 가능하며, 모든 도메인의 다양한 딥러닝 모델에 적용할 수 있습니다. 일반적인 최적화 기법: 비동기식으로 데이터 가져오기 및 데이터 증강법: torch.utils.data.DataLoader 는 각각 워커의 subprocess에서 비동기식 데이터 로딩과 데이터 증강을 지원합니다. DataLoader 의 n..." />
<meta property="og:ignore_canonical" content="true" />

    <title>성능 튜닝 가이드 &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../../_static/doctools.js?v=92e14aea"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/translations.js?v=b5f768d8"></script>
    <script src="../../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'recipes/recipes/tuning_guide';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/recipes/recipes/tuning_guide.html" />
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../../genindex.html" />
    <link rel="search" title="검색" href="../../search.html" />
    <link rel="next" title="Timer 빠르게 시작하기" href="timer_quick_start.html" />
    <link rel="prev" title="자동 혼합 정밀도(Automatic Mixed Precision) 가이드" href="amp_recipe.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../_static/js/theme.js"></script>
<script type="text/javascript" src="../../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="defining_a_neural_network.html">Pytorch를 사용해 신경망 정의하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_logs.html">(beta) torch.compile과 함께 TORCH_LOGS 파이썬 API 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="what_is_state_dict.html">PyTorch에서 state_dict란 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="warmstarting_model_using_parameters_from_a_different_model.html">PyTorch에서 다른 모델의 매개변수를 사용하여 빠르게 모델 시작하기(warmstart)</a></li>
<li class="toctree-l1"><a class="reference internal" href="zeroing_out_gradients.html">PyTorch에서 변화도를 0으로 만들기</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler_recipe.html">PyTorch 프로파일러(Profiler)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Captum_Recipe.html">Captum을 사용하여 모델 해석하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_with_pytorch.html">PyTorch로 TensorBoard 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp_recipe.html">자동 혼합 정밀도(Automatic Mixed Precision) 가이드</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">성능 튜닝 가이드</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiling_optimizer.html">(beta) Compiling the optimizer with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="timer_quick_start.html">Timer 빠르게 시작하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_backend_ipex.html">Intel® CPU에서의 Intel® Extension for PyTorch* 백엔드</a></li>
<li class="toctree-l1"><a class="reference internal" href="../zero_redundancy_optimizer.html">Shard Optimizer States with ZeroRedundancyOptimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cuda_rpc.html">Direct Device-to-Device Communication with TensorPipe CUDA RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_comm_debug_mode.html">Getting Started with <code class="docutils literal notranslate"><span class="pre">CommDebugMode</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">PyTorch Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_load_state_dict_tips.html">Tips for Loading an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> from a Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="reasoning_about_shapes.html">PyTorch의 Shape들에 대한 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="swap_tensors.html">Extension points in <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> for <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> and tensor subclasses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_export_aoti_python.html"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_with_pytorch.html">PyTorch로 TensorBoard 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../inference_tuning_on_aws_graviton.html">(Beta) PyTorch Inference Performance Tuning on AWS Graviton Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../amx.html">Leverage Intel® Advanced Matrix Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_torch_function_modes.html">(beta) Utilizing Torch Function modes with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiling_optimizer_lr_scheduler.html">(beta) Running the compiled optimizer with an LR Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../foreach_map.html">Explicit horizontal fusion with foreach_map and torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_user_defined_triton_kernel_tutorial.html">사용자 정의 Triton 커널을 ``torch.compile``과 함께 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_caching_tutorial.html">Compile Time Caching in <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_caching_configuration_tutorial.html">Compile Time Caching Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regional_compilation.html">Reducing torch.compile cold start compilation time with regional compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regional_aot.html">Reducing AoT cold start compilation time with regional compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intel_neural_compressor_for_pytorch.html">Ease-of-use quantization for PyTorch with Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_device_mesh.html">Getting Started with DeviceMesh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_checkpoint_recipe.html">Getting Started with Distributed Checkpoint (DCP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_async_checkpoint_recipe.html">Asynchronous Saving with Distributed Checkpoint (DCP)</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../recipes_index.html" class="nav-link">Recipes</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">성능 튜닝 가이드</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../recipes_index.html">
        <meta itemprop="name" content="Recipes">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="성능 튜닝 가이드">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">recipes/recipes/tuning_guide</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-recipes-recipes-tuning-guide-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="sphx-glr-recipes-recipes-tuning-guide-py">
<span id="id1"></span><h1>성능 튜닝 가이드<a class="headerlink" href="#sphx-glr-recipes-recipes-tuning-guide-py" title="Link to this heading">#</a></h1>
<p><strong>저자</strong>: <a class="reference external" href="https://github.com/szmigacz">Szymon Migacz</a>
<strong>역자</strong>: <a class="reference external" href="https://github.com/ohkingtaek">오왕택</a></p>
<p>성능 튜닝 가이드는 PyTorch에서 딥러닝 모델의 학습이나 추론 속도를 향상시킬 수 있는 최적화 기법과 같은 좋은 예시를 소개합니다.
제시된 기법은 몇 줄의 코드만 변경해서 구현 가능하며, 모든 도메인의 다양한 딥러닝 모델에 적용할 수 있습니다.</p>
<section id="id3">
<h2>일반적인 최적화 기법<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<section id="id4">
<h3>비동기식으로 데이터 가져오기 및 데이터 증강법<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">torch.utils.data.DataLoader</a>
는 각각 워커의 subprocess에서 비동기식 데이터 로딩과 데이터 증강을 지원합니다.
<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 의 num_worker 기본 설정은 <code class="docutils literal notranslate"><span class="pre">num_worker=0</span></code> 으로, 이는 데이터 로딩이
동기적으로 이루어지며 메인 프로세스에서 실행됨을 의미합니다. 결과적으로 메인 학습 프로세스는 데이터를
사용할 수 있을 때까지 기다려야 실행할 수 있습니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">num_workers</span> <span class="pre">&gt;</span> <span class="pre">0</span></code> 으로 설정하면 비동기식 데이터 로딩과 학습과 데이터 로딩의 동시 처리가
가능합니다. <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> 값은 작업량, CPU, GPU, 학습 데이터의 위치에 따라 조정해야
합니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 는 <code class="docutils literal notranslate"><span class="pre">pin_memory</span></code> 인자를 받으며 기본값은 <code class="docutils literal notranslate"><span class="pre">False</span></code> 입니다. GPU를
사용하는 경우 <code class="docutils literal notranslate"><span class="pre">pin_memory=True</span></code> 로 설정하는 것이 좋습니다. 이는 <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 가
고정된 메모리를 사용하고, 호스트에서 GPU로 더 빠르고 비동기적인 메모리 복사합니다.</p>
</section>
<section id="id5">
<h3>검증 및 추론 시 변화도 계산 비활성화하는 방법<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>PyTorch는 변화도가 필요한 Tensor와 관련된 모든 연산의 중간 버퍼를 저장합니다. 하지만 일반적으로
검증이나 추론 단계에서는 변화도가 필요하지 않습니다.
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad">torch.no_grad()</a>
컨텍스트 관리자를 사용하여 특정 코드 블록 내에서 변화도 계산을 비활성화할 수 있습니다.
이를 통해 실행 속도가 빨라지고 필요한 메모리 양이 줄어듭니다.
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad">torch.no_grad()</a>
는 함수 데코레이터로도 사용할 수 있습니다.</p>
</section>
<section id="id7">
<h3>합성곱 계층 이후에 바로 배치 정규화 계층이 오는 경우에 편향을 비활성화하는 방법<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">torch.nn.Conv2d()</a>
함수에는 기본적으로 <code class="docutils literal notranslate"><span class="pre">bias</span></code> 매개변수가 <code class="docutils literal notranslate"><span class="pre">True</span></code> 로 설정되어 있습니다 (이는
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d">Conv1d</a>
및
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d">Conv3d</a>
에서도 동일합니다).</p>
<p><code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> 계층 바로 뒤에 <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm2d</span></code> 계층이 이어진다면 합성곱 계층에서
편향은 필요하지 않으므로 <code class="docutils literal notranslate"><span class="pre">nn.Conv2d(...,</span> <span class="pre">bias=False,</span> <span class="pre">...)</span></code> 로 설정하세요.
<code class="docutils literal notranslate"><span class="pre">BatchNorm2d</span></code> 의 첫 단계에서 평균을 빼주기 때문에 필요하지 않으며, 이는 편향의 효과를
상쇄시킵니다.</p>
<p>이 원리는 1차원 및 3차원 합성곱에서도 동일하게 적용되며 <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> (또는 다른 정규화
계층)이 합성곱의 편향과 동일한 차원을 정규화할 경우에 해당됩니다.</p>
<p><a class="reference external" href="https://github.com/pytorch/vision">torchvision</a> 에서 제공하는 모델은
이미 이 최적화를 구현하고 있습니다.</p>
</section>
<section id="model-zero-grad-optimizer-zero-grad-parameter-grad-none">
<h3>model.zero_grad()나 optimizer.zero_grad() 대신 parameter.grad = None 사용하는 방법<a class="headerlink" href="#model-zero-grad-optimizer-zero-grad-parameter-grad-none" title="Link to this heading">#</a></h3>
<p>다음과 같은 방식으로 변화도를 초기화하는 대신:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="c1"># 또는</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-pytb notranslate"><div class="highlight"><pre><span></span><span class="gt">Traceback (most recent call last):</span>
  File <span class="nb">&quot;/workspace/tutorials-kr/recipes_source/recipes/tuning_guide.py&quot;</span>, line <span class="m">67</span>, in <span class="n">&lt;module&gt;</span>
<span class="w">    </span><span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="w">    </span><span class="pm">^^^^^</span>
<span class="gr">NameError</span>: <span class="n">name &#39;model&#39; is not defined</span>
</pre></div>
</div>
<p>아래와 같은 방법을 대신 사용하세요:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>두 번째 코드는 각 개별 매개변수의 메모리를 0으로 초기화하지 않으며,
이후의 역전파 과정에서 변화도를 저장할 때 더하기 대신 대입 연산을 사용하여 메모리 연산 수를 줄입니다.</p>
<p>변화도를 0으로 설정하는 것과 <code class="docutils literal notranslate"><span class="pre">None</span></code> 으로 설정하는 것은 약간의 수치적 차이가 있으므로 자세한
내용은
<a class="reference external" href="https://pytorch.org/docs/master/optim.html#torch.optim.Optimizer.zero_grad">torch.optim</a>
를 참조하세요.</p>
<p>또는 PyTorch 1.7부터 <code class="docutils literal notranslate"><span class="pre">model</span></code> 이나 <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad(set_to_none=True)</span></code>
를 호출할 수 있습니다.</p>
</section>
<section id="id8">
<h3>연산을 결합하여 최적화하는 방법<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>행렬에서 element-wise 덧셈, 곱셈 같은 연산과 <cite>sin()</cite> , <cite>cos()</cite> , <cite>sigmoid()</cite> 같은 수학
함수 등의 point-wise 연산들은 하나의 커널로 결합할 수 있습니다. 이러한 결합은 메모리 접근과 커널
실행 시간을 줄이는 데 도움이 됩니다. 일반적으로 point-wise 연산은 메모리에 바인딩됩니다.
PyTorch의 eager-mode에서는 각 연산마다 커널을 실행하므로 메모리에서 데이터를 불러와 연산을
수행하고 (종종 가장 시간이 적게 걸리는 단계) 결과를 다시 메모리에 쓰는 과정이 필요합니다.</p>
<p>결합된 연산자를 사용하면 여러 point-wise 연산을 위해 단 하나의 커널만 실행되고, 데이터는 한
번만 불러오고 저장됩니다. 특히 이러한 효율적인 방법은 활성화 함수, 옵티마이저, 직접 수정한 RNN 셀
등에서 유용합니다.</p>
<p>PyTorch 2에서는 TorchInductor라는 컴파일러를 통해 자동으로 커널을 결합하는 compile-mode를
도입했습니다. TorchInductor는 단순한 element-wise 연산뿐만 아니라 최적의 성능을 위해
point-wise 연산과 축소(reduction) 연산을 고급 결합할 수 있는 기능을 제공하여 성능을
최적화합니다.</p>
<p>가장 간단한 경우, 연산 결합은 함수 정의에
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html">torch.compile</a>
데코레이터를 적용하여 활성화할 수 있습니다. 예시:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">erf</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="mf">1.41421</span><span class="p">))</span>
</pre></div>
</div>
<p>고급 사용 사례에 대해서는 <a class="reference external" href="https://tutorials.pytorch.kr/intermediate/torch_compile_tutorial.html">Introduction to torch.compile</a>
를 참조하세요.</p>
</section>
<section id="channels-last">
<h3>컴퓨터 비전 모델에 대해 channels_last 메모리 형식 활성화하는 방법<a class="headerlink" href="#channels-last" title="Link to this heading">#</a></h3>
<p>PyTorch 1.5에서는 합성곱 신경망에 대해 channels_last 메모리 형식을 지원하기 시작했습니다.
이 포맷은 <a class="reference external" href="https://www.nvidia.com/en-us/data-center/tensor-cores/">Tensor Cores</a>
를 사용하여 합성곱 신경망을 더욱 가속화하기 위해
<a class="reference external" href="https://pytorch.org/docs/stable/amp.html">AMP</a>
와 함께 사용할 수 있도록 설계되었습니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">channels_last</span></code> 기능은 아직 실험 단계에 있지만 표준 컴퓨터 비전 모델(예시: ResNet-50,
SSD)에서는 동작할 것으로 예상됩니다. 모델을 <code class="docutils literal notranslate"><span class="pre">channels_last</span></code> 형식으로 변환하는 방법에 대해서는
<a class="reference external" href="https://tutorials.pytorch.kr/intermediate/memory_format_tutorial.html">(베타) PyTorch를 사용한 Channels Last 메모리 형식</a>
을 참조하세요. 튜토리얼에는
<a class="reference external" href="https://tutorials.pytorch.kr/intermediate/memory_format_tutorial.html#converting-existing-models">기존 모델들 변환하기</a>
섹션이 포함되어 있습니다.</p>
</section>
<section id="id10">
<h3>중간 버퍼를 체크포인트로 만드는 방법<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>버퍼 체크포인트 저장은 모델 학습 중 메모리 용량 부담을 완화하기 위한 기법입니다. 역전파에서 앞부분의
변화도를 계산하기 위해 모든 계층의 입력을 저장하는 대신, 일부 계층의 입력만 저장하고 나머지는
역전파 중에 재계산합니다. 메모리 요구 사항이 줄어들어 배치 크기를 증가시킬 수 있으며, 이는 활용
효율을 개선할 수 있습니다.</p>
<p>체크포인트 저장할 대상은 신중하게 선택해야 합니다. 가장 좋은 방법은 재계산 비용이 적은 대규모
레이어의 출력을 저장하지 않는 것입니다. 예를 들어, 활성화 함수(예시: <code class="docutils literal notranslate"><span class="pre">ReLU</span></code> , <code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code>
, <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> ), up/down 샘플링, 작은 누적 깊이(accumulation depth)를 가진 행렬-벡터 연산
등이 체크포인트 저장 대상으로 적합합니다.</p>
<p>PyTorch는 자동으로 체크포인트 저장 및 재계산을 수행하는
<a class="reference external" href="https://pytorch.org/docs/stable/checkpoint.html">torch.utils.checkpoint</a>
API를 지원합니다.</p>
</section>
<section id="api">
<h3>디버깅 API 비활성화<a class="headerlink" href="#api" title="Link to this heading">#</a></h3>
<p>많은 PyTorch API는 디버깅을 위해 설계되었으며 정규 학습 실행 시에는 비활성화해야 합니다.</p>
<ul class="simple">
<li><p>이상탐지 (anomaly detection):
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.detect_anomaly">torch.autograd.detect_anomaly</a>
또는
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.set_detect_anomaly">torch.autograd.set_detect_anomaly(True)</a></p></li>
<li><p>profiler 관련:
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx">torch.autograd.profiler.emit_nvtx</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile">torch.autograd.profiler.profile</a></p></li>
<li><p>autograd <code class="docutils literal notranslate"><span class="pre">gradcheck</span></code>:
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradcheck">torch.autograd.gradcheck</a>
또는
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradgradcheck">torch.autograd.gradgradcheck</a></p></li>
</ul>
</section>
</section>
<section id="cpu">
<h2>CPU 관련 최적화<a class="headerlink" href="#cpu" title="Link to this heading">#</a></h2>
<section id="numa">
<h3>비균일 메모리 접근(NUMA) 제어 활용 방법<a class="headerlink" href="#numa" title="Link to this heading">#</a></h3>
<p>NUMA(비균일 메모리 접근)는 다중 메모리 컨트롤러와 블록이 있는 멀티 소켓 머신에서 메모리의 지역성을
활용하기 위해 데이터 센터 머신에서 사용되는 메모리 레이아웃 디자인입니다. 일반적으로 딥러닝 작업,
학습 또는 추론 모두에서 NUMA 노드 간의 하드웨어 자원 접근 없이 더 나은 성능을 발휘합니다. 따라서
추론은 각 인스턴스가 하나의 소켓에서 실행되도록 여러 인스턴스로 실행할 수 있으며, 이를 통해 처리량을
증가시킬 수 있습니다. 단일 노드에서의 학습 작업에는 분산 학습이 권장되며, 이를 통해 각 학습
프로세스가 하나의 소켓에서 실행되도록 할 수 있습니다.</p>
<p>다음 명령어는 N번째 노드의 코어에서만 PyTorch 스크립트를 실행하며, 소켓 간 메모리
접근을 피하여 메모리 접근 오버헤드를 줄입니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>numactl<span class="w"> </span>--cpunodebind<span class="o">=</span>N<span class="w"> </span>--membind<span class="o">=</span>N<span class="w"> </span>python<span class="w"> </span>&lt;pytorch_script&gt;
</pre></div>
</div>
<p>자세한 설명은
<a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html">여기</a>
서 확인할 수 있습니다.</p>
</section>
<section id="openmp">
<h3>OpenMP 활용하는 방법<a class="headerlink" href="#openmp" title="Link to this heading">#</a></h3>
<p>OpenMP는 병렬 계산 작업의 성능을 향상시키기 위해 사용됩니다.
<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> 는 계산 속도를 높이는 가장 간단한 환경 변수입니다. 이는 OpenMP 계산에
사용되는 스레드 수를 결정합니다.
CPU Affinity 설정은 작업이 여러 코어에 분배되는 방식을 제어합니다. 이는 통신 오버헤드와 캐시 라인
무효화 오버헤드 또는 페이지 스레싱 등에 영향을 미치므로 CPU 친화도를 적절히 설정하면 성능 향상이
가능합니다. <code class="docutils literal notranslate"><span class="pre">GOMP_CPU_AFFINITY</span></code> 와 <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> 는 OpenMP 스레드를 물리적 처리
장치에 바인딩하는 방법을 결정합니다. 자세한 정보는
<a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html">여기</a>
에서 확인할 수 있습니다.</p>
<p>다음 명령어를 사용하면 PyTorch가 N개의 OpenMP 스레드에서 작업을 실행합니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>N
</pre></div>
</div>
<p>일반적으로 GNU OpenMP 구현에서 CPU 친화도를 설정하기 위해 다음 환경 변수를 사용합니다.
<code class="docutils literal notranslate"><span class="pre">OMP_PROC_BIND</span></code> 는 스레드가 프로세서 간에 이동할 수 있는지 여부를 지정합니다. 이를 CLOSE로
설정하면 OpenMP 스레드가 기본 스레드에 가까운 위치 파티션에 유지됩니다. <code class="docutils literal notranslate"><span class="pre">OMP_SCHEDULE</span></code> 는
OpenMP 스레드가 어떻게 스케줄링 되는지를 결정합니다. <code class="docutils literal notranslate"><span class="pre">GOMP_CPU_AFFINITY</span></code> 는 스레드를 특정
CPU에 바인딩합니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_SCHEDULE</span><span class="o">=</span>STATIC
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PROC_BIND</span><span class="o">=</span>CLOSE
<span class="nb">export</span><span class="w"> </span><span class="nv">GOMP_CPU_AFFINITY</span><span class="o">=</span><span class="s2">&quot;N-M&quot;</span>
</pre></div>
</div>
</section>
<section id="intel-openmp-libiomp">
<h3>Intel OpenMP 런타임 라이브러리(<code class="docutils literal notranslate"><span class="pre">libiomp</span></code>)<a class="headerlink" href="#intel-openmp-libiomp" title="Link to this heading">#</a></h3>
<p>기본적으로 PyTorch는 병렬 계산을 위해 GNU OPENMP(GNU <code class="docutils literal notranslate"><span class="pre">libgomp</span></code>)를 사용합니다. Intel
플랫폼에서는 Intel OpenMP 런타임 라이브러리(<code class="docutils literal notranslate"><span class="pre">libiomp</span></code>)가 OpenMP API 사양 지원을
제공합니다. 이는 <code class="docutils literal notranslate"><span class="pre">libgomp</span></code> 보다 더 나은 성능 이점을 제공할 때도 있습니다. 환경 변수
<code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> 를 사용하여 OpenMP 라이브러리를 <code class="docutils literal notranslate"><span class="pre">libiomp</span></code> 로 전환할 수 있습니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span>&lt;path&gt;/libiomp5.so:<span class="nv">$LD_PRELOAD</span>
</pre></div>
</div>
<p>GNU OpenMP의 CPU 친화도 설정과 유사하게, <code class="docutils literal notranslate"><span class="pre">libiomp</span></code> 에서도 CPU 친화도 설정을 제어하는 환경
변수가 제공됩니다. <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> 는 OpenMP 스레드를 물리적 처리 장치에 바인딩합니다.
<code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code> 은 스레드가 병렬 영역의 실행을 완료한 후 대기해야 하는 시간을 ms 단위로
설정합니다. 대부분의 경우 <code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code> 을 1 또는 0으로 설정하면 좋은 성능을 얻을 수
있습니다. 다음 명령어는 Intel OpenMP 런타임 라이브러리에서의 일반적인 설정입니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">KMP_AFFINITY</span><span class="o">=</span><span class="nv">granularity</span><span class="o">=</span>fine,compact,1,0
<span class="nb">export</span><span class="w"> </span><span class="nv">KMP_BLOCKTIME</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
</section>
<section id="id13">
<h3>메모리 할당자 전환<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>딥러닝 작업에서는 기존에 <code class="docutils literal notranslate"><span class="pre">malloc</span></code> 함수보다 메모리를 최대한 재사용할 수 있는 <code class="docutils literal notranslate"><span class="pre">Jemalloc</span></code>
또는 <code class="docutils literal notranslate"><span class="pre">TCMalloc</span></code> 을 사용하면 더 나은 성능을 얻을 수 있습니다.
<a class="reference external" href="https://github.com/jemalloc/jemalloc">Jemalloc</a> 은 일반적인 목적의 <code class="docutils literal notranslate"><span class="pre">malloc</span></code>
을 구현한 것으로, 단편화 방지와 확장 가능한 동시성 지원에 중점을 둡니다.
<a class="reference external" href="https://google.github.io/tcmalloc/overview.html">TCMalloc</a> 은 또한 프로그램
실행 속도를 높이기 위한 몇 가지 최적화를 제공합니다. 그 중 하나는 메모리를 캐시에 보관하여 자주
사용되는 객체에 대한 접근 속도를 높이는 것입니다. 이러한 캐시를 할당 해제 후에도 유지하면 나중에
메모리가 다시 할당될 때 비용이 많이 드는 시스템 호출을 피하는 데 도움이 됩니다. 이들 중 하나를
사용하려면 환경 변수 <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> 를 설정하십시오.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span>&lt;jemalloc.so/tcmalloc.so&gt;:<span class="nv">$LD_PRELOAD</span>
</pre></div>
</div>
</section>
<section id="torchscript-onednn-graph">
<h3>TorchScript로 추론 시 oneDNN Graph 사용하는 방법<a class="headerlink" href="#torchscript-onednn-graph" title="Link to this heading">#</a></h3>
<p>oneDNN Graph는 추론 성능을 크게 향상시킬 수 있습니다. 이는 합성곱, 행렬 곱셈(matmul)과 같은
연산을 주변 연산과 결합하여 처리합니다. PyTorch 2.0에서는 <code class="docutils literal notranslate"><span class="pre">Float32</span></code> 및 <code class="docutils literal notranslate"><span class="pre">BFloat16</span></code>
데이터 유형에 대해 베타 기능으로 지원됩니다. oneDNN Graph는 모델의 그래프를 받아서 예제 입력의
모양을 고려하여 연산자 결합 후보를 식별합니다. 모델은 예제 입력을 사용하여 JIT-traced 되어야
합니다. 동일한 모양의 입력에 대해 몇 번의 워밍업 후 속도가 향상될 수 있습니다. 아래의 예제
코드는 resnet50에 대한 것이지만 사용자가 원하는 모델에서도 oneDNN Graph를 사용할 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># oneDNN Graph를 사용하려면 이 추가 코드 한 줄로 충분합니다.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">enable_onednn_fusion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>oneDNN Graph API를 사용하려면 Float32 추론 시 단 한 줄의 코드를 추가해야 합니다. oneDNN
Graph를 사용 중이라면 <code class="docutils literal notranslate"><span class="pre">torch.jit.optimize_for_inference</span></code> 호출을 피해야 합니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># sample_input은 예상되는 입력과 동일한 모양이어야 합니다.</span>
<span class="n">sample_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)]</span>
<span class="c1"># resnet50 모델을 예시로 사용하지만, 아래 줄은 사용자가 원하는 모델에 맞게 수정할 수 있습니다.</span>
<span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="p">,</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">)()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1"># sample_input으로 모델을 trace하기</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sample_input</span><span class="p">)</span>
<span class="c1"># torch.jit.freeze 호출하기</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
</pre></div>
</div>
<p>모델이 sample_input을 사용해 JIT-traced되면 몇 번의 워밍업 후 추론에 사용할 수 있습니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># 몇 번의 워밍업</span>
    <span class="n">traced_model</span><span class="p">(</span><span class="o">*</span><span class="n">sample_input</span><span class="p">)</span>
    <span class="n">traced_model</span><span class="p">(</span><span class="o">*</span><span class="n">sample_input</span><span class="p">)</span>
    <span class="c1"># 워밍업 실행 후 성능 향상 관찰 가능</span>
    <span class="n">traced_model</span><span class="p">(</span><span class="o">*</span><span class="n">sample_input</span><span class="p">)</span>
</pre></div>
</div>
<p>oneDNN Graph용 JIT fuser는 <code class="docutils literal notranslate"><span class="pre">BFloat16</span></code> 데이터 타입을 사용한 추론도 지원하지만, oneDNN
Graph의 성능 이점은 AVX512_BF16 명령어 세트 아키텍처(ISA)의 머신에서 나타납니다.
다음 코드 예시는 oneDNN Graph를 사용해 <code class="docutils literal notranslate"><span class="pre">BFloat16</span></code> 데이터 타입으로 추론하는 예시입니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># JIT 모드의 AMP는 기본적으로 활성화되어 있으며 eager 모드와 다르게 작동합니다.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_autocast_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">cache_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">):</span>
    <span class="c1"># CNN 기반 비전 모델의 Conv-BatchNorm folding은 AMP를 사용할 때 ``torch.fx.experimental.optimization.fuse``를 통해 동작해야 합니다.</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch.fx.experimental.optimization</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optimization</span>
    <span class="c1"># AMP를 사용하지 않는 경우 optimization.fuse를 호출할 필요는 없습니다.</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">optimization</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">example_input</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="c1"># 몇 번의 워밍업</span>
    <span class="n">model</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>
    <span class="n">model</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>
    <span class="c1"># 워밍업 실행 후 성능 향상 관찰 가능</span>
    <span class="n">model</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="pytorch-distributeddataparallel-ddp-cpu">
<h3>PyTorch <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> (DDP) 기능을 사용해 CPU에서 모델 학습하는 방법<a class="headerlink" href="#pytorch-distributeddataparallel-ddp-cpu" title="Link to this heading">#</a></h3>
<p>DLRM과 같은 소규모 모델 또는 메모리에 바인딩 된 모델의 경우 CPU에서 학습하는 것도 좋은 선택입니다.
여러 소켓을 가진 머신에서는, 분산 학습으로 고효율의 하드웨어 자원을 사용하여 학습 과정을 가속할 수 있습니다.
<a class="reference external" href="https://github.com/intel/torch-ccl">Torch-ccl</a> 은 Intel(R)의 <code class="docutils literal notranslate"><span class="pre">oneCCL</span></code>
(집합 통신 라이브러리)로 최적화되어 효율적인 분산 딥러닝 학습을 위해 <code class="docutils literal notranslate"><span class="pre">allreduce</span></code> ,
<code class="docutils literal notranslate"><span class="pre">allgather</span></code> , <code class="docutils literal notranslate"><span class="pre">alltoall</span></code> 과 같은 집합 연산을 구현합니다. Torch-ccl은 PyTorch C10D
<code class="docutils literal notranslate"><span class="pre">ProcessGroup</span></code> API를 구현하며, 외부 <code class="docutils literal notranslate"><span class="pre">ProcessGroup</span></code> 으로 동적으로 로드할 수 있습니다.
PyTorch DDP 모듈에서 구현된 최적화를 통해 <code class="docutils literal notranslate"><span class="pre">torch-ccl</span></code> 은 통신 연산을 가속화합니다. 통신
커널의 최적화 외에도 <code class="docutils literal notranslate"><span class="pre">torch-ccl</span></code> 은 계산과 통신을 동시에 수행하는 기능을 제공합니다.</p>
</section>
</section>
<section id="gpu">
<h2>GPU 전용 최적화 방법<a class="headerlink" href="#gpu" title="Link to this heading">#</a></h2>
<section id="cudnn-auto-tuner">
<h3>cuDNN auto-tuner 활성화하기<a class="headerlink" href="#cudnn-auto-tuner" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://developer.nvidia.com/cudnn">NVIDIA cuDNN</a> 은 합성곱을 계산하기 위해 여러
알고리즘을 지원합니다. Autotuner는 짧은 벤치마크를 실행하고 주어진 하드웨어와 입력 크기에 대해
최상의 성능을 가진 커널을 선택합니다.</p>
<p>합성곱 신경망 (다른 유형은 현재 지원되지 않음)의 경우 학습하기 전에 cuDNN autotuner를
활성화하려면 다음과 같이 설정하십시오:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<ul class="simple">
<li><p>autotuner의 결정은 비결정적일 수 있습니다. 서로 다른 실행에서 다른 알고리즘이 선택될 수
있습니다. 자세한 내용은
<a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html?highlight=determinism">PyTorch: Reproducibility</a>
를 참조하세요.</p></li>
<li><p>드문 상황에서, 예를 들면 입력 크기가 가변적인 경우, 각 입력 크기에 대해 알고리즘 선택과 관련된
오버헤드를 피하기 위해 autotuner를 비활성화하고 합성곱 신경망을 실행하는 것이 더 나을 수 있습니다.</p></li>
</ul>
</section>
<section id="cpu-gpu">
<h3>불필요한 CPU-GPU 동기화 피하기<a class="headerlink" href="#cpu-gpu" title="Link to this heading">#</a></h3>
<p>CPU가 GPU 같은 가속기보다 최대한 앞서 실행될 수 있도록 불필요한 동기화를 피하여 가속기의 작업 큐에
많은 작업이 포함되도록 하십시오.</p>
<p>가능하면 동기화를 요구하는 작업을 피하십시오. 예시:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">print(cuda_tensor)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuda_tensor.item()</span></code></p></li>
<li><p>메모리 복사 : <code class="docutils literal notranslate"><span class="pre">tensor.cuda()</span></code>,  <code class="docutils literal notranslate"><span class="pre">cuda_tensor.cpu()</span></code> 혹은 이에 상응하는
<code class="docutils literal notranslate"><span class="pre">tensor.to(device)</span></code> 호출</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuda_tensor.nonzero()</span></code></p></li>
<li><p>CUDA tensor에서 수행된 연산 결과에 의존하는 파이썬 제어 흐름
예시: <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">(cuda_tensor</span> <span class="pre">!=</span> <span class="pre">0).all()</span></code></p></li>
</ul>
</section>
<section id="tensor">
<h3>대상 장치에서 직접 tensor 생성하는 방법<a class="headerlink" href="#tensor" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch.rand(size).cuda()</span></code> 를 호출하여 무작위 tensor를 생성하는 대신에 tensor를 직접
장치에서 생성합니다.
<code class="docutils literal notranslate"><span class="pre">torch.rand(size,</span> <span class="pre">device='cuda')</span></code></p>
<p>이는 다음과 같이 <code class="docutils literal notranslate"><span class="pre">device</span></code> 인수를 받아 새로운 tensor를 생성하는 모든 함수에 적용됩니다:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand">torch.rand()</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros">torch.zeros()</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.full.html#torch.full">torch.full()</a>
등.</p>
</section>
<section id="id14">
<h3>혼합 정밀도와 AMP 사용하는 방법<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<p>혼합 정밀도는
<a class="reference external" href="https://www.nvidia.com/en-us/data-center/tensor-cores/">Tensor Cores</a>
를 활용하여 Volta나 최신 GPU 아키텍처에서 최대 3배의 전체 속도 향상을 제공합니다. Tensor
Cores를 사용하려면 AMP를 활성화하고 행렬/텐서 차원이 Tensor Cores를 사용하는 커널 호출
요구 사항을 충족해야 합니다.</p>
<p>Tensor Cores를 사용하려면:</p>
<ul class="simple">
<li><p>size를 8의 배수로 설정 (Tensor Cores의 차원에 맞추기 위해)</p>
<ul>
<li><p><a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance">Deep Learning Performance Documentation</a>
에서 자세한 정보와 레이어 유형에 따른 가이드라인을 참조하세요.</p></li>
<li><p>레이어 크기가 고정되지 않고 다른 매개변수에서 유도되는 경우에도 명시적으로 패딩할 수 있습니다.
(예시: NLP 모델의 어휘 크기 등).</p></li>
</ul>
</li>
<li><p>AMP 활성화하기</p>
<ul>
<li><p>혼합 정밀도 학습과 AMP 소개:
<a class="reference external" href="https://www.youtube.com/watch?v=jF4-_ZK_tyc&amp;feature=youtu.be">video</a>,
<a class="reference external" href="https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/dusan_stosic-training-neural-networks-with-tensor-cores.pdf">slides</a></p></li>
<li><p>PyTorch 1.6부터 사용할 수 있는 PyTorch AMP:
<a class="reference external" href="https://pytorch.org/docs/stable/amp.html">documentation</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples">examples</a>,
<a class="reference external" href="https://tutorials.pytorch.kr/recipes/recipes/amp_recipe.html">tutorial</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="id16">
<h3>가변 입력 길이에 대비하여 메모리 미리 할당하기<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<p>음성 인식 또는 NLP 모델은 종종 가변 시퀀스 길이를 가진 tensor를 입력으로 학습됩니다. 가변 길이는
PyTorch 캐싱 할당기에서 문제를 일으킬 수 있으며, 성능 저하 또는 예기치 않은 메모리 부족 오류를
초래할 수 있습니다. 짧은 시퀀스 길이의 배치가 더 긴 시퀀스 길이의 배치로 이어지면, PyTorch는
이전 반복의 중간 버퍼를 해제하고 새 버퍼를 재할당해야 합니다. 이 과정은 시간이 많이 소요되며 캐싱
할당기에서 조각화(fragmentation)를 일으켜 메모리 부족 오류를 유발할 수 있습니다.</p>
<p>일반적인 해결 방법은 미리 할당(preallocation)을 구현하는 것입니다.
다음 단계로 구성됩니다:</p>
<ol class="arabic simple">
<li><p>최대 시퀀스 길이(훈련 데이터 세트의 최대 길이 또는 사전 정의된 임계값에 해당)를 갖는 (일반적으로
무작위) 입력 배치를 생성합니다.</p></li>
<li><p>생성된 배치로 순방향 및 역방향 과정을 실행합니다. 옵티마이저나 학습률 스케줄러는 실행하지 않으며,
이 단계는 이후 학습에서 재사용할 수 있는 최대 크기의 버퍼를 미리 할당합니다.</p></li>
<li><p>변화도를 0으로 설정합니다.</p></li>
<li><p>정규 학습을 진행합니다.</p></li>
</ol>
</section>
</section>
<section id="id17">
<h2>분산 최적화 방법<a class="headerlink" href="#id17" title="Link to this heading">#</a></h2>
<section id="id18">
<h3>효율적인 데이터 병렬 백엔드 사용하는 방법<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<p>PyTorch에는 데이터 병렬 학습을 구현하는 두 가지 방법이 있습니다:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel">torch.nn.DataParallel</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">torch.nn.parallel.DistributedDataParallel</a></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 은 다중 GPU에 대해 훨씬 더 나은 성능과 확장성을 제공합니다.
자세한 정보는 PyTorch 문서의
<a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel">relevant section of CUDA Best Practices</a>
를 참조하세요.</p>
</section>
<section id="distributeddataparallel-all-reduce">
<h3>학습할 때 <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 이나 변화도 축적 사용 시 불필요한 all-reduce 건너뛰는 방법<a class="headerlink" href="#distributeddataparallel-all-reduce" title="Link to this heading">#</a></h3>
<p>기본적으로
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">torch.nn.parallel.DistributedDataParallel</a>
은 모든 역전파 과정 후에 변화도 all-reduce를 실행하여 학습에 참여하는 모든 워커에서의 평균 변화도를
계산합니다. 학습 시 변화도 축적을 N단계 동안 사용하는 경우, 모든 학습 단계 후에 all-reduce가
요하지 않습니다. 마지막 역전파 호출 직후, 즉 옵티마이저 실행 직전에만 all-reduce를
수행하면 됩니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 은 특정 반복에 대해 변화도 all-reduce를 비활성화하는
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync">no_sync()</a>
컨텍스트 관리자를 제공합니다.
<code class="docutils literal notranslate"><span class="pre">no_sync()</span></code> 는 변화도 축적의 첫 <code class="docutils literal notranslate"><span class="pre">N-1</span></code> 반복에 적용되어야 하며 마지막 반복은 기본 실행을
따르고 필요한 변화도 all-reduce를 수행해야 합니다.</p>
</section>
<section id="distributeddataparallel-find-unused-parameters-true">
<h3><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel(find_unused_parameters=True)</span></code> 를 사용할 때 생성자와 실행 레이어 순서를 일치시키는 방법<a class="headerlink" href="#distributeddataparallel-find-unused-parameters-true" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">torch.nn.parallel.DistributedDataParallel</a>
은 <code class="docutils literal notranslate"><span class="pre">find_unused_parameters=True</span></code> 와 함께 모델 생성자에서의 레이어와 파라미터 순서를
사용하여 <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 변화도 all-reduce를 위한 버킷을 만듭니다.
<code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 은 all-reduce를 역전파와 겹치게 수행합니다. 특정 버킷에 대한
all-reduce는 주어진 버킷의 모든 파라미터에 대한 변화도가 모두 준비되었을 때 비동기적으로 작동됩니다.</p>
<p>최대로 겹치게 하려면 모델 생성자에서의 순서가 실제 실행 중인 순서와 대략적으로 일치해야 합니다.
순서가 맞지 않으면 전체 버킷에 대한 all-reduce는 마지막으로 도착하는 변화도를 기다리게 되며,
이는 역전파와 all-reduce 간의 겹침을 줄일 수 있고, all-reduce가 노출되어 학습 속도가 느려질 수
있습니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">find_unused_parameters=False</span></code> 가 (기본 설정)인 <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 은
역전파 중에 발견된 연산 순서를 기반으로 자동으로 버킷을 형성합니다.
<code class="docutils literal notranslate"><span class="pre">find_unused_parameters=False</span></code> 를 사용할 때는 최적의 성능을 달성하기 위해 레이어나
파라미터의 순서를 재조정할 필요가 없습니다.</p>
</section>
<section id="id21">
<h3>분산 설정에서 작업 부하를 분산하는 방법<a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
<p>작업 부하 불균형은 일반적으로 순차적인 데이터를 처리하는 모델(예시: 음성 인식, 번역, 언어 모델 등)
에서 발생할 수 있습니다. 하나의 장치가 나머지 장치들보다 긴 시퀀스 길이를 가진 데이터 배치를 받으면,
모든 장치가 마지막으로 작업을 끝내는 워커를 기다리게 됩니다. 역전파는
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">DistributedDataParallel</a>
백엔드와 함께 분산 설정에서 암묵적인 동기화 지점으로 작용합니다.</p>
<p>작업 로드 밸런싱 문제를 해결하는 방법은 여러 가지가 있습니다. 핵심은 각 전역 배치 내에서
모든 워커에 걸쳐 작업 부하를 가능한 한 균일하게 분배하는 것입니다. 예를 들어, Transformer는 배치
내에서 대략 일정한 수의 토큰(변동하는 수의 시퀀스)을 형성하여 불균형을 해결하며, 다른 모델은 유사한
시퀀스 길이를 가진 샘플을 버킷화하거나 데이터셋을 시퀀스 길이에 따라 정렬하여 불균형을 해결합니다.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 0.003 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-recipes-recipes-tuning-guide-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/38991cbc7763ed7e0f1b711da737b391/tuning_guide.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tuning_guide.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/8c82db84c10318a94cbe213adb618139/tuning_guide.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tuning_guide.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/810e65eb9306212bd714a1fce48d023b/tuning_guide.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">tuning_guide.zip</span></code></a></p>
</div>
</div>
</section>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="amp_recipe.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">자동 혼합 정밀도(Automatic Mixed Precision) 가이드</p>
      </div>
    </a>
    <a class="right-next"
       href="timer_quick_start.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Timer 빠르게 시작하기</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="amp_recipe.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">자동 혼합 정밀도(Automatic Mixed Precision) 가이드</p>
      </div>
    </a>
    <a class="right-next"
       href="timer_quick_start.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Timer 빠르게 시작하기</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">일반적인 최적화 기법</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">비동기식으로 데이터 가져오기 및 데이터 증강법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">검증 및 추론 시 변화도 계산 비활성화하는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">합성곱 계층 이후에 바로 배치 정규화 계층이 오는 경우에 편향을 비활성화하는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-zero-grad-optimizer-zero-grad-parameter-grad-none">model.zero_grad()나 optimizer.zero_grad() 대신 parameter.grad = None 사용하는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">연산을 결합하여 최적화하는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#channels-last">컴퓨터 비전 모델에 대해 channels_last 메모리 형식 활성화하는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">중간 버퍼를 체크포인트로 만드는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#api">디버깅 API 비활성화</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cpu">CPU 관련 최적화</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numa">비균일 메모리 접근(NUMA) 제어 활용 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#openmp">OpenMP 활용하는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intel-openmp-libiomp">Intel OpenMP 런타임 라이브러리(<code class="docutils literal notranslate"><span class="pre">libiomp</span></code>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">메모리 할당자 전환</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torchscript-onednn-graph">TorchScript로 추론 시 oneDNN Graph 사용하는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-distributeddataparallel-ddp-cpu">PyTorch <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> (DDP) 기능을 사용해 CPU에서 모델 학습하는 방법</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu">GPU 전용 최적화 방법</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cudnn-auto-tuner">cuDNN auto-tuner 활성화하기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cpu-gpu">불필요한 CPU-GPU 동기화 피하기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor">대상 장치에서 직접 tensor 생성하는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">혼합 정밀도와 AMP 사용하는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">가변 입력 길이에 대비하여 메모리 미리 할당하기</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">분산 최적화 방법</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">효율적인 데이터 병렬 백엔드 사용하는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributeddataparallel-all-reduce">학습할 때 <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 이나 변화도 축적 사용 시 불필요한 all-reduce 건너뛰는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributeddataparallel-find-unused-parameters-true"><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel(find_unused_parameters=True)</span></code> 를 사용할 때 생성자와 실행 레이어 순서를 일치시키는 방법</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">분산 설정에서 작업 부하를 분산하는 방법</a></li>
</ul>
</li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "\uc131\ub2a5 \ud29c\ub2dd \uac00\uc774\ub4dc",
       "headline": "\uc131\ub2a5 \ud29c\ub2dd \uac00\uc774\ub4dc",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/recipes/recipes/tuning_guide.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. \uc131\ub2a5 \ud29c\ub2dd \uac00\uc774\ub4dc# \uc800\uc790: Szymon Migacz \uc5ed\uc790: \uc624\uc655\ud0dd \uc131\ub2a5 \ud29c\ub2dd \uac00\uc774\ub4dc\ub294 PyTorch\uc5d0\uc11c \ub525\ub7ec\ub2dd \ubaa8\ub378\uc758 \ud559\uc2b5\uc774\ub098 \ucd94\ub860 \uc18d\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\ub294 \ucd5c\uc801\ud654 \uae30\ubc95\uacfc \uac19\uc740 \uc88b\uc740 \uc608\uc2dc\ub97c \uc18c\uac1c\ud569\ub2c8\ub2e4. \uc81c\uc2dc\ub41c \uae30\ubc95\uc740 \uba87 \uc904\uc758 \ucf54\ub4dc\ub9cc \ubcc0\uacbd\ud574\uc11c \uad6c\ud604 \uac00\ub2a5\ud558\uba70, \ubaa8\ub4e0 \ub3c4\uba54\uc778\uc758 \ub2e4\uc591\ud55c \ub525\ub7ec\ub2dd \ubaa8\ub378\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc778 \ucd5c\uc801\ud654 \uae30\ubc95# \ube44\ub3d9\uae30\uc2dd\uc73c\ub85c \ub370\uc774\ud130 \uac00\uc838\uc624\uae30 \ubc0f \ub370\uc774\ud130 \uc99d\uac15\ubc95# torch.utils.data.DataLoader \ub294 \uac01\uac01 \uc6cc\ucee4\uc758 subprocess\uc5d0\uc11c \ube44\ub3d9\uae30\uc2dd \ub370\uc774\ud130 \ub85c\ub529\uacfc \ub370\uc774\ud130 \uc99d\uac15\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. DataLoader \uc758 num_worker \uae30\ubcf8 \uc124\uc815\uc740 num_worker=0 \uc73c\ub85c, \uc774\ub294 \ub370\uc774\ud130 \ub85c\ub529\uc774 \ub3d9\uae30\uc801\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c0\uba70 \uba54\uc778 \ud504\ub85c\uc138\uc2a4\uc5d0\uc11c \uc2e4\ud589\ub428\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4. \uacb0\uacfc\uc801\uc73c\ub85c \uba54\uc778 \ud559\uc2b5 \ud504\ub85c\uc138\uc2a4\ub294 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc744 \ub54c\uae4c\uc9c0 \uae30\ub2e4\ub824\uc57c \uc2e4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. num_workers \u003e 0 \uc73c\ub85c \uc124\uc815\ud558\uba74 \ube44\ub3d9\uae30\uc2dd \ub370\uc774\ud130 \ub85c\ub529\uacfc \ud559\uc2b5\uacfc \ub370\uc774\ud130 \ub85c\ub529\uc758 \ub3d9\uc2dc \ucc98\ub9ac\uac00 \uac00\ub2a5\ud569\ub2c8\ub2e4. num_workers \uac12\uc740 \uc791\uc5c5\ub7c9, CPU, GPU, \ud559\uc2b5 \ub370\uc774\ud130\uc758 \uc704\uce58\uc5d0 \ub530\ub77c \uc870\uc815\ud574\uc57c \ud569\ub2c8\ub2e4. DataLoader \ub294 pin_memory \uc778\uc790\ub97c \ubc1b\uc73c\uba70 \uae30\ubcf8\uac12\uc740 False \uc785\ub2c8\ub2e4. GPU\ub97c \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0 pin_memory=True \ub85c \uc124\uc815\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4. \uc774\ub294 DataLoader \uac00 \uace0\uc815\ub41c \uba54\ubaa8\ub9ac\ub97c \uc0ac\uc6a9\ud558\uace0, \ud638\uc2a4\ud2b8\uc5d0\uc11c GPU\ub85c \ub354 \ube60\ub974\uace0 \ube44\ub3d9\uae30\uc801\uc778 \uba54\ubaa8\ub9ac \ubcf5\uc0ac\ud569\ub2c8\ub2e4. \uac80\uc99d \ubc0f \ucd94\ub860 \uc2dc \ubcc0\ud654\ub3c4 \uacc4\uc0b0 \ube44\ud65c\uc131\ud654\ud558\ub294 \ubc29\ubc95# PyTorch\ub294 \ubcc0\ud654\ub3c4\uac00 \ud544\uc694\ud55c Tensor\uc640 \uad00\ub828\ub41c \ubaa8\ub4e0 \uc5f0\uc0b0\uc758 \uc911\uac04 \ubc84\ud37c\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc77c\ubc18\uc801\uc73c\ub85c \uac80\uc99d\uc774\ub098 \ucd94\ub860 \ub2e8\uacc4\uc5d0\uc11c\ub294 \ubcc0\ud654\ub3c4\uac00 \ud544\uc694\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. torch.no_grad() \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud2b9\uc815 \ucf54\ub4dc \ube14\ub85d \ub0b4\uc5d0\uc11c \ubcc0\ud654\ub3c4 \uacc4\uc0b0\uc744 \ube44\ud65c\uc131\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uc2e4\ud589 \uc18d\ub3c4\uac00 \ube68\ub77c\uc9c0\uace0 \ud544\uc694\ud55c \uba54\ubaa8\ub9ac \uc591\uc774 \uc904\uc5b4\ub4ed\ub2c8\ub2e4. torch.no_grad() \ub294 \ud568\uc218 \ub370\ucf54\ub808\uc774\ud130\ub85c\ub3c4 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud569\uc131\uacf1 \uacc4\uce35 \uc774\ud6c4\uc5d0 \ubc14\ub85c \ubc30\uce58 \uc815\uaddc\ud654 \uacc4\uce35\uc774 \uc624\ub294 \uacbd\uc6b0\uc5d0 \ud3b8\ud5a5\uc744 \ube44\ud65c\uc131\ud654\ud558\ub294 \ubc29\ubc95# torch.nn.Conv2d() \ud568\uc218\uc5d0\ub294 \uae30\ubcf8\uc801\uc73c\ub85c bias \ub9e4\uac1c\ubcc0\uc218\uac00 True \ub85c \uc124\uc815\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4 (\uc774\ub294 Conv1d \ubc0f Conv3d \uc5d0\uc11c\ub3c4 \ub3d9\uc77c\ud569\ub2c8\ub2e4). nn.Conv2d \uacc4\uce35 \ubc14\ub85c \ub4a4\uc5d0 nn.BatchNorm2d \uacc4\uce35\uc774 \uc774\uc5b4\uc9c4\ub2e4\uba74 \ud569\uc131\uacf1 \uacc4\uce35\uc5d0\uc11c \ud3b8\ud5a5\uc740 \ud544\uc694\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c nn.Conv2d(..., bias=False, ...) \ub85c \uc124\uc815\ud558\uc138\uc694. BatchNorm2d \uc758 \uccab \ub2e8\uacc4\uc5d0\uc11c \ud3c9\uade0\uc744 \ube7c\uc8fc\uae30 \ub54c\ubb38\uc5d0 \ud544\uc694\ud558\uc9c0 \uc54a\uc73c\uba70, \uc774\ub294 \ud3b8\ud5a5\uc758 \ud6a8\uacfc\ub97c \uc0c1\uc1c4\uc2dc\ud0b5\ub2c8\ub2e4. \uc774 \uc6d0\ub9ac\ub294 1\ucc28\uc6d0 \ubc0f 3\ucc28\uc6d0 \ud569\uc131\uacf1\uc5d0\uc11c\ub3c4 \ub3d9\uc77c\ud558\uac8c \uc801\uc6a9\ub418\uba70 BatchNorm (\ub610\ub294 \ub2e4\ub978 \uc815\uaddc\ud654 \uacc4\uce35)\uc774 \ud569\uc131\uacf1\uc758 \ud3b8\ud5a5\uacfc \ub3d9\uc77c\ud55c \ucc28\uc6d0\uc744 \uc815\uaddc\ud654\ud560 \uacbd\uc6b0\uc5d0 \ud574\ub2f9\ub429\ub2c8\ub2e4. torchvision \uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \ubaa8\ub378\uc740 \uc774\ubbf8 \uc774 \ucd5c\uc801\ud654\ub97c \uad6c\ud604\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. model.zero_grad()\ub098 optimizer.zero_grad() \ub300\uc2e0 parameter.grad = None \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95# \ub2e4\uc74c\uacfc \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c \ubcc0\ud654\ub3c4\ub97c \ucd08\uae30\ud654\ud558\ub294 \ub300\uc2e0: model.zero_grad() # \ub610\ub294 optimizer.zero_grad() Traceback (most recent call last): File \"/workspace/tutorials-kr/recipes_source/recipes/tuning_guide.py\", line 67, in \u003cmodule\u003e model.zero_grad() ^^^^^ NameError: name \u0027model\u0027 is not defined \uc544\ub798\uc640 \uac19\uc740 \ubc29\ubc95\uc744 \ub300\uc2e0 \uc0ac\uc6a9\ud558\uc138\uc694: for param in model.parameters(): param.grad = None \ub450 \ubc88\uc9f8 \ucf54\ub4dc\ub294 \uac01 \uac1c\ubcc4 \ub9e4\uac1c\ubcc0\uc218\uc758 \uba54\ubaa8\ub9ac\ub97c 0\uc73c\ub85c \ucd08\uae30\ud654\ud558\uc9c0 \uc54a\uc73c\uba70, \uc774\ud6c4\uc758 \uc5ed\uc804\ud30c \uacfc\uc815\uc5d0\uc11c \ubcc0\ud654\ub3c4\ub97c \uc800\uc7a5\ud560 \ub54c \ub354\ud558\uae30 \ub300\uc2e0 \ub300\uc785 \uc5f0\uc0b0\uc744 \uc0ac\uc6a9\ud558\uc5ec \uba54\ubaa8\ub9ac \uc5f0\uc0b0 \uc218\ub97c \uc904\uc785\ub2c8\ub2e4. \ubcc0\ud654\ub3c4\ub97c 0\uc73c\ub85c \uc124\uc815\ud558\ub294 \uac83\uacfc None \uc73c\ub85c \uc124\uc815\ud558\ub294 \uac83\uc740 \uc57d\uac04\uc758 \uc218\uce58\uc801 \ucc28\uc774\uac00 \uc788\uc73c\ubbc0\ub85c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 torch.optim \ub97c \ucc38\uc870\ud558\uc138\uc694. \ub610\ub294 PyTorch 1.7\ubd80\ud130 model \uc774\ub098 optimizer.zero_grad(set_to_none=True) \ub97c \ud638\ucd9c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5f0\uc0b0\uc744 \uacb0\ud569\ud558\uc5ec \ucd5c\uc801\ud654\ud558\ub294 \ubc29\ubc95# \ud589\ub82c\uc5d0\uc11c element-wise \ub367\uc148, \uacf1\uc148 \uac19\uc740 \uc5f0\uc0b0\uacfc sin() , cos() , sigmoid() \uac19\uc740 \uc218\ud559 \ud568\uc218 \ub4f1\uc758 point-wise \uc5f0\uc0b0\ub4e4\uc740 \ud558\ub098\uc758 \ucee4\ub110\ub85c \uacb0\ud569\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacb0\ud569\uc740 \uba54\ubaa8\ub9ac \uc811\uadfc\uacfc \ucee4\ub110 \uc2e4\ud589 \uc2dc\uac04\uc744 \uc904\uc774\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c point-wise \uc5f0\uc0b0\uc740 \uba54\ubaa8\ub9ac\uc5d0 \ubc14\uc778\ub529\ub429\ub2c8\ub2e4. PyTorch\uc758 eager-mode\uc5d0\uc11c\ub294 \uac01 \uc5f0\uc0b0\ub9c8\ub2e4 \ucee4\ub110\uc744 \uc2e4\ud589\ud558\ubbc0\ub85c \uba54\ubaa8\ub9ac\uc5d0\uc11c \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc640 \uc5f0\uc0b0\uc744 \uc218\ud589\ud558\uace0 (\uc885\uc885 \uac00\uc7a5 \uc2dc\uac04\uc774 \uc801\uac8c \uac78\ub9ac\ub294 \ub2e8\uacc4) \uacb0\uacfc\ub97c \ub2e4\uc2dc \uba54\ubaa8\ub9ac\uc5d0 \uc4f0\ub294 \uacfc\uc815\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \uacb0\ud569\ub41c \uc5f0\uc0b0\uc790\ub97c \uc0ac\uc6a9\ud558\uba74 \uc5ec\ub7ec point-wise \uc5f0\uc0b0\uc744 \uc704\ud574 \ub2e8 \ud558\ub098\uc758 \ucee4\ub110\ub9cc \uc2e4\ud589\ub418\uace0, \ub370\uc774\ud130\ub294 \ud55c \ubc88\ub9cc \ubd88\ub7ec\uc624\uace0 \uc800\uc7a5\ub429\ub2c8\ub2e4. \ud2b9\ud788 \uc774\ub7ec\ud55c \ud6a8\uc728\uc801\uc778 \ubc29\ubc95\uc740 \ud65c\uc131\ud654 \ud568\uc218, \uc635\ud2f0\ub9c8\uc774\uc800, \uc9c1\uc811 \uc218\uc815\ud55c RNN \uc140 \ub4f1\uc5d0\uc11c \uc720\uc6a9\ud569\ub2c8\ub2e4. PyTorch 2\uc5d0\uc11c\ub294 TorchInductor\ub77c\ub294 \ucef4\ud30c\uc77c\ub7ec\ub97c \ud1b5\ud574 \uc790\ub3d9\uc73c\ub85c \ucee4\ub110\uc744 \uacb0\ud569\ud558\ub294 compile-mode\ub97c \ub3c4\uc785\ud588\uc2b5\ub2c8\ub2e4. TorchInductor\ub294 \ub2e8\uc21c\ud55c element-wise \uc5f0\uc0b0\ubfd0\ub9cc \uc544\ub2c8\ub77c \ucd5c\uc801\uc758 \uc131\ub2a5\uc744 \uc704\ud574 point-wise \uc5f0\uc0b0\uacfc \ucd95\uc18c(reduction) \uc5f0\uc0b0\uc744 \uace0\uae09 \uacb0\ud569\ud560 \uc218 \uc788\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\uc5ec \uc131\ub2a5\uc744 \ucd5c\uc801\ud654\ud569\ub2c8\ub2e4. \uac00\uc7a5 \uac04\ub2e8\ud55c \uacbd\uc6b0, \uc5f0\uc0b0 \uacb0\ud569\uc740 \ud568\uc218 \uc815\uc758\uc5d0 torch.compile \ub370\ucf54\ub808\uc774\ud130\ub97c \uc801\uc6a9\ud558\uc5ec \ud65c\uc131\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\uc2dc: @torch.compile def gelu(x): return x * 0.5 * (1.0 + torch.erf(x / 1.41421)) \uace0\uae09 \uc0ac\uc6a9 \uc0ac\ub840\uc5d0 \ub300\ud574\uc11c\ub294 Introduction to torch.compile \ub97c \ucc38\uc870\ud558\uc138\uc694. \ucef4\ud4e8\ud130 \ube44\uc804 \ubaa8\ub378\uc5d0 \ub300\ud574 channels_last \uba54\ubaa8\ub9ac \ud615\uc2dd \ud65c\uc131\ud654\ud558\ub294 \ubc29\ubc95# PyTorch 1.5\uc5d0\uc11c\ub294 \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\uc5d0 \ub300\ud574 channels_last \uba54\ubaa8\ub9ac \ud615\uc2dd\uc744 \uc9c0\uc6d0\ud558\uae30 \uc2dc\uc791\ud588\uc2b5\ub2c8\ub2e4. \uc774 \ud3ec\ub9f7\uc740 Tensor Cores \ub97c \uc0ac\uc6a9\ud558\uc5ec \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\uc744 \ub354\uc6b1 \uac00\uc18d\ud654\ud558\uae30 \uc704\ud574 AMP \uc640 \ud568\uaed8 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4. channels_last \uae30\ub2a5\uc740 \uc544\uc9c1 \uc2e4\ud5d8 \ub2e8\uacc4\uc5d0 \uc788\uc9c0\ub9cc \ud45c\uc900 \ucef4\ud4e8\ud130 \ube44\uc804 \ubaa8\ub378(\uc608\uc2dc: ResNet-50, SSD)\uc5d0\uc11c\ub294 \ub3d9\uc791\ud560 \uac83\uc73c\ub85c \uc608\uc0c1\ub429\ub2c8\ub2e4. \ubaa8\ub378\uc744 channels_last \ud615\uc2dd\uc73c\ub85c \ubcc0\ud658\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574\uc11c\ub294 (\ubca0\ud0c0) PyTorch\ub97c \uc0ac\uc6a9\ud55c Channels Last \uba54\ubaa8\ub9ac \ud615\uc2dd \uc744 \ucc38\uc870\ud558\uc138\uc694. \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\ub294 \uae30\uc874 \ubaa8\ub378\ub4e4 \ubcc0\ud658\ud558\uae30 \uc139\uc158\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc911\uac04 \ubc84\ud37c\ub97c \uccb4\ud06c\ud3ec\uc778\ud2b8\ub85c \ub9cc\ub4dc\ub294 \ubc29\ubc95# \ubc84\ud37c \uccb4\ud06c\ud3ec\uc778\ud2b8 \uc800\uc7a5\uc740 \ubaa8\ub378 \ud559\uc2b5 \uc911 \uba54\ubaa8\ub9ac \uc6a9\ub7c9 \ubd80\ub2f4\uc744 \uc644\ud654\ud558\uae30 \uc704\ud55c \uae30\ubc95\uc785\ub2c8\ub2e4. \uc5ed\uc804\ud30c\uc5d0\uc11c \uc55e\ubd80\ubd84\uc758 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud558\uae30 \uc704\ud574 \ubaa8\ub4e0 \uacc4\uce35\uc758 \uc785\ub825\uc744 \uc800\uc7a5\ud558\ub294 \ub300\uc2e0, \uc77c\ubd80 \uacc4\uce35\uc758 \uc785\ub825\ub9cc \uc800\uc7a5\ud558\uace0 \ub098\uba38\uc9c0\ub294 \uc5ed\uc804\ud30c \uc911\uc5d0 \uc7ac\uacc4\uc0b0\ud569\ub2c8\ub2e4. \uba54\ubaa8\ub9ac \uc694\uad6c \uc0ac\ud56d\uc774 \uc904\uc5b4\ub4e4\uc5b4 \ubc30\uce58 \ud06c\uae30\ub97c \uc99d\uac00\uc2dc\ud0ac \uc218 \uc788\uc73c\uba70, \uc774\ub294 \ud65c\uc6a9 \ud6a8\uc728\uc744 \uac1c\uc120\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uccb4\ud06c\ud3ec\uc778\ud2b8 \uc800\uc7a5\ud560 \ub300\uc0c1\uc740 \uc2e0\uc911\ud558\uac8c \uc120\ud0dd\ud574\uc57c \ud569\ub2c8\ub2e4. \uac00\uc7a5 \uc88b\uc740 \ubc29\ubc95\uc740 \uc7ac\uacc4\uc0b0 \ube44\uc6a9\uc774 \uc801\uc740 \ub300\uaddc\ubaa8 \ub808\uc774\uc5b4\uc758 \ucd9c\ub825\uc744 \uc800\uc7a5\ud558\uc9c0 \uc54a\ub294 \uac83\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \ud65c\uc131\ud654 \ud568\uc218(\uc608\uc2dc: ReLU , Sigmoid , Tanh ), up/down \uc0d8\ud50c\ub9c1, \uc791\uc740 \ub204\uc801 \uae4a\uc774(accumulation depth)\ub97c \uac00\uc9c4 \ud589\ub82c-\ubca1\ud130 \uc5f0\uc0b0 \ub4f1\uc774 \uccb4\ud06c\ud3ec\uc778\ud2b8 \uc800\uc7a5 \ub300\uc0c1\uc73c\ub85c \uc801\ud569\ud569\ub2c8\ub2e4. PyTorch\ub294 \uc790\ub3d9\uc73c\ub85c \uccb4\ud06c\ud3ec\uc778\ud2b8 \uc800\uc7a5 \ubc0f \uc7ac\uacc4\uc0b0\uc744 \uc218\ud589\ud558\ub294 torch.utils.checkpoint API\ub97c \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \ub514\ubc84\uae45 API \ube44\ud65c\uc131\ud654# \ub9ce\uc740 PyTorch API\ub294 \ub514\ubc84\uae45\uc744 \uc704\ud574 \uc124\uacc4\ub418\uc5c8\uc73c\uba70 \uc815\uaddc \ud559\uc2b5 \uc2e4\ud589 \uc2dc\uc5d0\ub294 \ube44\ud65c\uc131\ud654\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\uc0c1\ud0d0\uc9c0 (anomaly detection): torch.autograd.detect_anomaly \ub610\ub294 torch.autograd.set_detect_anomaly(True) profiler \uad00\ub828: torch.autograd.profiler.emit_nvtx, torch.autograd.profiler.profile autograd gradcheck: torch.autograd.gradcheck \ub610\ub294 torch.autograd.gradgradcheck CPU \uad00\ub828 \ucd5c\uc801\ud654# \ube44\uade0\uc77c \uba54\ubaa8\ub9ac \uc811\uadfc(NUMA) \uc81c\uc5b4 \ud65c\uc6a9 \ubc29\ubc95# NUMA(\ube44\uade0\uc77c \uba54\ubaa8\ub9ac \uc811\uadfc)\ub294 \ub2e4\uc911 \uba54\ubaa8\ub9ac \ucee8\ud2b8\ub864\ub7ec\uc640 \ube14\ub85d\uc774 \uc788\ub294 \uba40\ud2f0 \uc18c\ucf13 \uba38\uc2e0\uc5d0\uc11c \uba54\ubaa8\ub9ac\uc758 \uc9c0\uc5ed\uc131\uc744 \ud65c\uc6a9\ud558\uae30 \uc704\ud574 \ub370\uc774\ud130 \uc13c\ud130 \uba38\uc2e0\uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \uba54\ubaa8\ub9ac \ub808\uc774\uc544\uc6c3 \ub514\uc790\uc778\uc785\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \ub525\ub7ec\ub2dd \uc791\uc5c5, \ud559\uc2b5 \ub610\ub294 \ucd94\ub860 \ubaa8\ub450\uc5d0\uc11c NUMA \ub178\ub4dc \uac04\uc758 \ud558\ub4dc\uc6e8\uc5b4 \uc790\uc6d0 \uc811\uadfc \uc5c6\uc774 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubc1c\ud718\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \ucd94\ub860\uc740 \uac01 \uc778\uc2a4\ud134\uc2a4\uac00 \ud558\ub098\uc758 \uc18c\ucf13\uc5d0\uc11c \uc2e4\ud589\ub418\ub3c4\ub85d \uc5ec\ub7ec \uc778\uc2a4\ud134\uc2a4\ub85c \uc2e4\ud589\ud560 \uc218 \uc788\uc73c\uba70, \uc774\ub97c \ud1b5\ud574 \ucc98\ub9ac\ub7c9\uc744 \uc99d\uac00\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e8\uc77c \ub178\ub4dc\uc5d0\uc11c\uc758 \ud559\uc2b5 \uc791\uc5c5\uc5d0\ub294 \ubd84\uc0b0 \ud559\uc2b5\uc774 \uad8c\uc7a5\ub418\uba70, \uc774\ub97c \ud1b5\ud574 \uac01 \ud559\uc2b5 \ud504\ub85c\uc138\uc2a4\uac00 \ud558\ub098\uc758 \uc18c\ucf13\uc5d0\uc11c \uc2e4\ud589\ub418\ub3c4\ub85d \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uba85\ub839\uc5b4\ub294 N\ubc88\uc9f8 \ub178\ub4dc\uc758 \ucf54\uc5b4\uc5d0\uc11c\ub9cc PyTorch \uc2a4\ud06c\ub9bd\ud2b8\ub97c \uc2e4\ud589\ud558\uba70, \uc18c\ucf13 \uac04 \uba54\ubaa8\ub9ac \uc811\uadfc\uc744 \ud53c\ud558\uc5ec \uba54\ubaa8\ub9ac \uc811\uadfc \uc624\ubc84\ud5e4\ub4dc\ub97c \uc904\uc785\ub2c8\ub2e4. numactl --cpunodebind=N --membind=N python \u003cpytorch_script\u003e \uc790\uc138\ud55c \uc124\uba85\uc740 \uc5ec\uae30 \uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. OpenMP \ud65c\uc6a9\ud558\ub294 \ubc29\ubc95# OpenMP\ub294 \ubcd1\ub82c \uacc4\uc0b0 \uc791\uc5c5\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. OMP_NUM_THREADS \ub294 \uacc4\uc0b0 \uc18d\ub3c4\ub97c \ub192\uc774\ub294 \uac00\uc7a5 \uac04\ub2e8\ud55c \ud658\uacbd \ubcc0\uc218\uc785\ub2c8\ub2e4. \uc774\ub294 OpenMP \uacc4\uc0b0\uc5d0 \uc0ac\uc6a9\ub418\ub294 \uc2a4\ub808\ub4dc \uc218\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4. CPU Affinity \uc124\uc815\uc740 \uc791\uc5c5\uc774 \uc5ec\ub7ec \ucf54\uc5b4\uc5d0 \ubd84\ubc30\ub418\ub294 \ubc29\uc2dd\uc744 \uc81c\uc5b4\ud569\ub2c8\ub2e4. \uc774\ub294 \ud1b5\uc2e0 \uc624\ubc84\ud5e4\ub4dc\uc640 \uce90\uc2dc \ub77c\uc778 \ubb34\ud6a8\ud654 \uc624\ubc84\ud5e4\ub4dc \ub610\ub294 \ud398\uc774\uc9c0 \uc2a4\ub808\uc2f1 \ub4f1\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ubbc0\ub85c CPU \uce5c\ud654\ub3c4\ub97c \uc801\uc808\ud788 \uc124\uc815\ud558\uba74 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. GOMP_CPU_AFFINITY \uc640 KMP_AFFINITY \ub294 OpenMP \uc2a4\ub808\ub4dc\ub97c \ubb3c\ub9ac\uc801 \ucc98\ub9ac \uc7a5\uce58\uc5d0 \ubc14\uc778\ub529\ud558\ub294 \ubc29\ubc95\uc744 \uacb0\uc815\ud569\ub2c8\ub2e4. \uc790\uc138\ud55c \uc815\ubcf4\ub294 \uc5ec\uae30 \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uba85\ub839\uc5b4\ub97c \uc0ac\uc6a9\ud558\uba74 PyTorch\uac00 N\uac1c\uc758 OpenMP \uc2a4\ub808\ub4dc\uc5d0\uc11c \uc791\uc5c5\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4. export OMP_NUM_THREADS=N \uc77c\ubc18\uc801\uc73c\ub85c GNU OpenMP \uad6c\ud604\uc5d0\uc11c CPU \uce5c\ud654\ub3c4\ub97c \uc124\uc815\ud558\uae30 \uc704\ud574 \ub2e4\uc74c \ud658\uacbd \ubcc0\uc218\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. OMP_PROC_BIND \ub294 \uc2a4\ub808\ub4dc\uac00 \ud504\ub85c\uc138\uc11c \uac04\uc5d0 \uc774\ub3d9\ud560 \uc218 \uc788\ub294\uc9c0 \uc5ec\ubd80\ub97c \uc9c0\uc815\ud569\ub2c8\ub2e4. \uc774\ub97c CLOSE\ub85c \uc124\uc815\ud558\uba74 OpenMP \uc2a4\ub808\ub4dc\uac00 \uae30\ubcf8 \uc2a4\ub808\ub4dc\uc5d0 \uac00\uae4c\uc6b4 \uc704\uce58 \ud30c\ud2f0\uc158\uc5d0 \uc720\uc9c0\ub429\ub2c8\ub2e4. OMP_SCHEDULE \ub294 OpenMP \uc2a4\ub808\ub4dc\uac00 \uc5b4\ub5bb\uac8c \uc2a4\ucf00\uc904\ub9c1 \ub418\ub294\uc9c0\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4. GOMP_CPU_AFFINITY \ub294 \uc2a4\ub808\ub4dc\ub97c \ud2b9\uc815 CPU\uc5d0 \ubc14\uc778\ub529\ud569\ub2c8\ub2e4. export OMP_SCHEDULE=STATIC export OMP_PROC_BIND=CLOSE export GOMP_CPU_AFFINITY=\"N-M\" Intel OpenMP \ub7f0\ud0c0\uc784 \ub77c\uc774\ube0c\ub7ec\ub9ac(libiomp)# \uae30\ubcf8\uc801\uc73c\ub85c PyTorch\ub294 \ubcd1\ub82c \uacc4\uc0b0\uc744 \uc704\ud574 GNU OPENMP(GNU libgomp)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. Intel \ud50c\ub7ab\ud3fc\uc5d0\uc11c\ub294 Intel OpenMP \ub7f0\ud0c0\uc784 \ub77c\uc774\ube0c\ub7ec\ub9ac(libiomp)\uac00 OpenMP API \uc0ac\uc591 \uc9c0\uc6d0\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774\ub294 libgomp \ubcf4\ub2e4 \ub354 \ub098\uc740 \uc131\ub2a5 \uc774\uc810\uc744 \uc81c\uacf5\ud560 \ub54c\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ud658\uacbd \ubcc0\uc218 LD_PRELOAD \ub97c \uc0ac\uc6a9\ud558\uc5ec OpenMP \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c libiomp \ub85c \uc804\ud658\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. export LD_PRELOAD=\u003cpath\u003e/libiomp5.so:$LD_PRELOAD GNU OpenMP\uc758 CPU \uce5c\ud654\ub3c4 \uc124\uc815\uacfc \uc720\uc0ac\ud558\uac8c, libiomp \uc5d0\uc11c\ub3c4 CPU \uce5c\ud654\ub3c4 \uc124\uc815\uc744 \uc81c\uc5b4\ud558\ub294 \ud658\uacbd \ubcc0\uc218\uac00 \uc81c\uacf5\ub429\ub2c8\ub2e4. KMP_AFFINITY \ub294 OpenMP \uc2a4\ub808\ub4dc\ub97c \ubb3c\ub9ac\uc801 \ucc98\ub9ac \uc7a5\uce58\uc5d0 \ubc14\uc778\ub529\ud569\ub2c8\ub2e4. KMP_BLOCKTIME \uc740 \uc2a4\ub808\ub4dc\uac00 \ubcd1\ub82c \uc601\uc5ed\uc758 \uc2e4\ud589\uc744 \uc644\ub8cc\ud55c \ud6c4 \ub300\uae30\ud574\uc57c \ud558\ub294 \uc2dc\uac04\uc744 ms \ub2e8\uc704\ub85c \uc124\uc815\ud569\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc758 \uacbd\uc6b0 KMP_BLOCKTIME \uc744 1 \ub610\ub294 0\uc73c\ub85c \uc124\uc815\ud558\uba74 \uc88b\uc740 \uc131\ub2a5\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uba85\ub839\uc5b4\ub294 Intel OpenMP \ub7f0\ud0c0\uc784 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c\uc758 \uc77c\ubc18\uc801\uc778 \uc124\uc815\uc785\ub2c8\ub2e4. export KMP_AFFINITY=granularity=fine,compact,1,0 export KMP_BLOCKTIME=1 \uba54\ubaa8\ub9ac \ud560\ub2f9\uc790 \uc804\ud658# \ub525\ub7ec\ub2dd \uc791\uc5c5\uc5d0\uc11c\ub294 \uae30\uc874\uc5d0 malloc \ud568\uc218\ubcf4\ub2e4 \uba54\ubaa8\ub9ac\ub97c \ucd5c\ub300\ud55c \uc7ac\uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 Jemalloc \ub610\ub294 TCMalloc \uc744 \uc0ac\uc6a9\ud558\uba74 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Jemalloc \uc740 \uc77c\ubc18\uc801\uc778 \ubaa9\uc801\uc758 malloc \uc744 \uad6c\ud604\ud55c \uac83\uc73c\ub85c, \ub2e8\ud3b8\ud654 \ubc29\uc9c0\uc640 \ud655\uc7a5 \uac00\ub2a5\ud55c \ub3d9\uc2dc\uc131 \uc9c0\uc6d0\uc5d0 \uc911\uc810\uc744 \ub461\ub2c8\ub2e4. TCMalloc \uc740 \ub610\ud55c \ud504\ub85c\uadf8\ub7a8 \uc2e4\ud589 \uc18d\ub3c4\ub97c \ub192\uc774\uae30 \uc704\ud55c \uba87 \uac00\uc9c0 \ucd5c\uc801\ud654\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uadf8 \uc911 \ud558\ub098\ub294 \uba54\ubaa8\ub9ac\ub97c \uce90\uc2dc\uc5d0 \ubcf4\uad00\ud558\uc5ec \uc790\uc8fc \uc0ac\uc6a9\ub418\ub294 \uac1d\uccb4\uc5d0 \ub300\ud55c \uc811\uadfc \uc18d\ub3c4\ub97c \ub192\uc774\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uce90\uc2dc\ub97c \ud560\ub2f9 \ud574\uc81c \ud6c4\uc5d0\ub3c4 \uc720\uc9c0\ud558\uba74 \ub098\uc911\uc5d0 \uba54\ubaa8\ub9ac\uac00 \ub2e4\uc2dc \ud560\ub2f9\ub420 \ub54c \ube44\uc6a9\uc774 \ub9ce\uc774 \ub4dc\ub294 \uc2dc\uc2a4\ud15c \ud638\ucd9c\uc744 \ud53c\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. \uc774\ub4e4 \uc911 \ud558\ub098\ub97c \uc0ac\uc6a9\ud558\ub824\uba74 \ud658\uacbd \ubcc0\uc218 LD_PRELOAD \ub97c \uc124\uc815\ud558\uc2ed\uc2dc\uc624. export LD_PRELOAD=\u003cjemalloc.so/tcmalloc.so\u003e:$LD_PRELOAD TorchScript\ub85c \ucd94\ub860 \uc2dc oneDNN Graph \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95# oneDNN Graph\ub294 \ucd94\ub860 \uc131\ub2a5\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ud569\uc131\uacf1, \ud589\ub82c \uacf1\uc148(matmul)\uacfc \uac19\uc740 \uc5f0\uc0b0\uc744 \uc8fc\ubcc0 \uc5f0\uc0b0\uacfc \uacb0\ud569\ud558\uc5ec \ucc98\ub9ac\ud569\ub2c8\ub2e4. PyTorch 2.0\uc5d0\uc11c\ub294 Float32 \ubc0f BFloat16 \ub370\uc774\ud130 \uc720\ud615\uc5d0 \ub300\ud574 \ubca0\ud0c0 \uae30\ub2a5\uc73c\ub85c \uc9c0\uc6d0\ub429\ub2c8\ub2e4. oneDNN Graph\ub294 \ubaa8\ub378\uc758 \uadf8\ub798\ud504\ub97c \ubc1b\uc544\uc11c \uc608\uc81c \uc785\ub825\uc758 \ubaa8\uc591\uc744 \uace0\ub824\ud558\uc5ec \uc5f0\uc0b0\uc790 \uacb0\ud569 \ud6c4\ubcf4\ub97c \uc2dd\ubcc4\ud569\ub2c8\ub2e4. \ubaa8\ub378\uc740 \uc608\uc81c \uc785\ub825\uc744 \uc0ac\uc6a9\ud558\uc5ec JIT-traced \ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4. \ub3d9\uc77c\ud55c \ubaa8\uc591\uc758 \uc785\ub825\uc5d0 \ub300\ud574 \uba87 \ubc88\uc758 \uc6cc\ubc0d\uc5c5 \ud6c4 \uc18d\ub3c4\uac00 \ud5a5\uc0c1\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798\uc758 \uc608\uc81c \ucf54\ub4dc\ub294 resnet50\uc5d0 \ub300\ud55c \uac83\uc774\uc9c0\ub9cc \uc0ac\uc6a9\uc790\uac00 \uc6d0\ud558\ub294 \ubaa8\ub378\uc5d0\uc11c\ub3c4 oneDNN Graph\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. # oneDNN Graph\ub97c \uc0ac\uc6a9\ud558\ub824\uba74 \uc774 \ucd94\uac00 \ucf54\ub4dc \ud55c \uc904\ub85c \ucda9\ubd84\ud569\ub2c8\ub2e4. torch.jit.enable_onednn_fusion(True) oneDNN Graph API\ub97c \uc0ac\uc6a9\ud558\ub824\uba74 Float32 \ucd94\ub860 \uc2dc \ub2e8 \ud55c \uc904\uc758 \ucf54\ub4dc\ub97c \ucd94\uac00\ud574\uc57c \ud569\ub2c8\ub2e4. oneDNN Graph\ub97c \uc0ac\uc6a9 \uc911\uc774\ub77c\uba74 torch.jit.optimize_for_inference \ud638\ucd9c\uc744 \ud53c\ud574\uc57c \ud569\ub2c8\ub2e4. # sample_input\uc740 \uc608\uc0c1\ub418\ub294 \uc785\ub825\uacfc \ub3d9\uc77c\ud55c \ubaa8\uc591\uc774\uc5b4\uc57c \ud569\ub2c8\ub2e4. sample_input = [torch.rand(32, 3, 224, 224)] # resnet50 \ubaa8\ub378\uc744 \uc608\uc2dc\ub85c \uc0ac\uc6a9\ud558\uc9c0\ub9cc, \uc544\ub798 \uc904\uc740 \uc0ac\uc6a9\uc790\uac00 \uc6d0\ud558\ub294 \ubaa8\ub378\uc5d0 \ub9de\uac8c \uc218\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. model = getattr(torchvision.models, \"resnet50\")().eval() # sample_input\uc73c\ub85c \ubaa8\ub378\uc744 trace\ud558\uae30 traced_model = torch.jit.trace(model, sample_input) # torch.jit.freeze \ud638\ucd9c\ud558\uae30 traced_model = torch.jit.freeze(traced_model) \ubaa8\ub378\uc774 sample_input\uc744 \uc0ac\uc6a9\ud574 JIT-traced\ub418\uba74 \uba87 \ubc88\uc758 \uc6cc\ubc0d\uc5c5 \ud6c4 \ucd94\ub860\uc5d0 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. with torch.no_grad(): # \uba87 \ubc88\uc758 \uc6cc\ubc0d\uc5c5 traced_model(*sample_input) traced_model(*sample_input) # \uc6cc\ubc0d\uc5c5 \uc2e4\ud589 \ud6c4 \uc131\ub2a5 \ud5a5\uc0c1 \uad00\ucc30 \uac00\ub2a5 traced_model(*sample_input) oneDNN Graph\uc6a9 JIT fuser\ub294 BFloat16 \ub370\uc774\ud130 \ud0c0\uc785\uc744 \uc0ac\uc6a9\ud55c \ucd94\ub860\ub3c4 \uc9c0\uc6d0\ud558\uc9c0\ub9cc, oneDNN Graph\uc758 \uc131\ub2a5 \uc774\uc810\uc740 AVX512_BF16 \uba85\ub839\uc5b4 \uc138\ud2b8 \uc544\ud0a4\ud14d\ucc98(ISA)\uc758 \uba38\uc2e0\uc5d0\uc11c \ub098\ud0c0\ub0a9\ub2c8\ub2e4. \ub2e4\uc74c \ucf54\ub4dc \uc608\uc2dc\ub294 oneDNN Graph\ub97c \uc0ac\uc6a9\ud574 BFloat16 \ub370\uc774\ud130 \ud0c0\uc785\uc73c\ub85c \ucd94\ub860\ud558\ub294 \uc608\uc2dc\uc785\ub2c8\ub2e4. # JIT \ubaa8\ub4dc\uc758 AMP\ub294 \uae30\ubcf8\uc801\uc73c\ub85c \ud65c\uc131\ud654\ub418\uc5b4 \uc788\uc73c\uba70 eager \ubaa8\ub4dc\uc640 \ub2e4\ub974\uac8c \uc791\ub3d9\ud569\ub2c8\ub2e4. torch._C._jit_set_autocast_mode(False) with torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False, dtype=torch.bfloat16): # CNN \uae30\ubc18 \ube44\uc804 \ubaa8\ub378\uc758 Conv-BatchNorm folding\uc740 AMP\ub97c \uc0ac\uc6a9\ud560 \ub54c ``torch.fx.experimental.optimization.fuse``\ub97c \ud1b5\ud574 \ub3d9\uc791\ud574\uc57c \ud569\ub2c8\ub2e4. import torch.fx.experimental.optimization as optimization # AMP\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294 \uacbd\uc6b0 optimization.fuse\ub97c \ud638\ucd9c\ud560 \ud544\uc694\ub294 \uc5c6\uc2b5\ub2c8\ub2e4. model = optimization.fuse(model) model = torch.jit.trace(model, (example_input)) model = torch.jit.freeze(model) # \uba87 \ubc88\uc758 \uc6cc\ubc0d\uc5c5 model(example_input) model(example_input) # \uc6cc\ubc0d\uc5c5 \uc2e4\ud589 \ud6c4 \uc131\ub2a5 \ud5a5\uc0c1 \uad00\ucc30 \uac00\ub2a5 model(example_input) PyTorch DistributedDataParallel (DDP) \uae30\ub2a5\uc744 \uc0ac\uc6a9\ud574 CPU\uc5d0\uc11c \ubaa8\ub378 \ud559\uc2b5\ud558\ub294 \ubc29\ubc95# DLRM\uacfc \uac19\uc740 \uc18c\uaddc\ubaa8 \ubaa8\ub378 \ub610\ub294 \uba54\ubaa8\ub9ac\uc5d0 \ubc14\uc778\ub529 \ub41c \ubaa8\ub378\uc758 \uacbd\uc6b0 CPU\uc5d0\uc11c \ud559\uc2b5\ud558\ub294 \uac83\ub3c4 \uc88b\uc740 \uc120\ud0dd\uc785\ub2c8\ub2e4. \uc5ec\ub7ec \uc18c\ucf13\uc744 \uac00\uc9c4 \uba38\uc2e0\uc5d0\uc11c\ub294, \ubd84\uc0b0 \ud559\uc2b5\uc73c\ub85c \uace0\ud6a8\uc728\uc758 \ud558\ub4dc\uc6e8\uc5b4 \uc790\uc6d0\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5 \uacfc\uc815\uc744 \uac00\uc18d\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Torch-ccl \uc740 Intel(R)\uc758 oneCCL (\uc9d1\ud569 \ud1b5\uc2e0 \ub77c\uc774\ube0c\ub7ec\ub9ac)\ub85c \ucd5c\uc801\ud654\ub418\uc5b4 \ud6a8\uc728\uc801\uc778 \ubd84\uc0b0 \ub525\ub7ec\ub2dd \ud559\uc2b5\uc744 \uc704\ud574 allreduce , allgather , alltoall \uacfc \uac19\uc740 \uc9d1\ud569 \uc5f0\uc0b0\uc744 \uad6c\ud604\ud569\ub2c8\ub2e4. Torch-ccl\uc740 PyTorch C10D ProcessGroup API\ub97c \uad6c\ud604\ud558\uba70, \uc678\ubd80 ProcessGroup \uc73c\ub85c \ub3d9\uc801\uc73c\ub85c \ub85c\ub4dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. PyTorch DDP \ubaa8\ub4c8\uc5d0\uc11c \uad6c\ud604\ub41c \ucd5c\uc801\ud654\ub97c \ud1b5\ud574 torch-ccl \uc740 \ud1b5\uc2e0 \uc5f0\uc0b0\uc744 \uac00\uc18d\ud654\ud569\ub2c8\ub2e4. \ud1b5\uc2e0 \ucee4\ub110\uc758 \ucd5c\uc801\ud654 \uc678\uc5d0\ub3c4 torch-ccl \uc740 \uacc4\uc0b0\uacfc \ud1b5\uc2e0\uc744 \ub3d9\uc2dc\uc5d0 \uc218\ud589\ud558\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. GPU \uc804\uc6a9 \ucd5c\uc801\ud654 \ubc29\ubc95# cuDNN auto-tuner \ud65c\uc131\ud654\ud558\uae30# NVIDIA cuDNN \uc740 \ud569\uc131\uacf1\uc744 \uacc4\uc0b0\ud558\uae30 \uc704\ud574 \uc5ec\ub7ec \uc54c\uace0\ub9ac\uc998\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. Autotuner\ub294 \uc9e7\uc740 \ubca4\uce58\ub9c8\ud06c\ub97c \uc2e4\ud589\ud558\uace0 \uc8fc\uc5b4\uc9c4 \ud558\ub4dc\uc6e8\uc5b4\uc640 \uc785\ub825 \ud06c\uae30\uc5d0 \ub300\ud574 \ucd5c\uc0c1\uc758 \uc131\ub2a5\uc744 \uac00\uc9c4 \ucee4\ub110\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4. \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd (\ub2e4\ub978 \uc720\ud615\uc740 \ud604\uc7ac \uc9c0\uc6d0\ub418\uc9c0 \uc54a\uc74c)\uc758 \uacbd\uc6b0 \ud559\uc2b5\ud558\uae30 \uc804\uc5d0 cuDNN autotuner\ub97c \ud65c\uc131\ud654\ud558\ub824\uba74 \ub2e4\uc74c\uacfc \uac19\uc774 \uc124\uc815\ud558\uc2ed\uc2dc\uc624: torch.backends.cudnn.benchmark = True autotuner\uc758 \uacb0\uc815\uc740 \ube44\uacb0\uc815\uc801\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc11c\ub85c \ub2e4\ub978 \uc2e4\ud589\uc5d0\uc11c \ub2e4\ub978 \uc54c\uace0\ub9ac\uc998\uc774 \uc120\ud0dd\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 PyTorch: Reproducibility \ub97c \ucc38\uc870\ud558\uc138\uc694. \ub4dc\ubb38 \uc0c1\ud669\uc5d0\uc11c, \uc608\ub97c \ub4e4\uba74 \uc785\ub825 \ud06c\uae30\uac00 \uac00\ubcc0\uc801\uc778 \uacbd\uc6b0, \uac01 \uc785\ub825 \ud06c\uae30\uc5d0 \ub300\ud574 \uc54c\uace0\ub9ac\uc998 \uc120\ud0dd\uacfc \uad00\ub828\ub41c \uc624\ubc84\ud5e4\ub4dc\ub97c \ud53c\ud558\uae30 \uc704\ud574 autotuner\ub97c \ube44\ud65c\uc131\ud654\ud558\uace0 \ud569\uc131\uacf1 \uc2e0\uacbd\ub9dd\uc744 \uc2e4\ud589\ud558\ub294 \uac83\uc774 \ub354 \ub098\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubd88\ud544\uc694\ud55c CPU-GPU \ub3d9\uae30\ud654 \ud53c\ud558\uae30# CPU\uac00 GPU \uac19\uc740 \uac00\uc18d\uae30\ubcf4\ub2e4 \ucd5c\ub300\ud55c \uc55e\uc11c \uc2e4\ud589\ub420 \uc218 \uc788\ub3c4\ub85d \ubd88\ud544\uc694\ud55c \ub3d9\uae30\ud654\ub97c \ud53c\ud558\uc5ec \uac00\uc18d\uae30\uc758 \uc791\uc5c5 \ud050\uc5d0 \ub9ce\uc740 \uc791\uc5c5\uc774 \ud3ec\ud568\ub418\ub3c4\ub85d \ud558\uc2ed\uc2dc\uc624. \uac00\ub2a5\ud558\uba74 \ub3d9\uae30\ud654\ub97c \uc694\uad6c\ud558\ub294 \uc791\uc5c5\uc744 \ud53c\ud558\uc2ed\uc2dc\uc624. \uc608\uc2dc: print(cuda_tensor) cuda_tensor.item() \uba54\ubaa8\ub9ac \ubcf5\uc0ac : tensor.cuda(), cuda_tensor.cpu() \ud639\uc740 \uc774\uc5d0 \uc0c1\uc751\ud558\ub294 tensor.to(device) \ud638\ucd9c cuda_tensor.nonzero() CUDA tensor\uc5d0\uc11c \uc218\ud589\ub41c \uc5f0\uc0b0 \uacb0\uacfc\uc5d0 \uc758\uc874\ud558\ub294 \ud30c\uc774\uc36c \uc81c\uc5b4 \ud750\ub984 \uc608\uc2dc: if (cuda_tensor != 0).all() \ub300\uc0c1 \uc7a5\uce58\uc5d0\uc11c \uc9c1\uc811 tensor \uc0dd\uc131\ud558\ub294 \ubc29\ubc95# torch.rand(size).cuda() \ub97c \ud638\ucd9c\ud558\uc5ec \ubb34\uc791\uc704 tensor\ub97c \uc0dd\uc131\ud558\ub294 \ub300\uc2e0\uc5d0 tensor\ub97c \uc9c1\uc811 \uc7a5\uce58\uc5d0\uc11c \uc0dd\uc131\ud569\ub2c8\ub2e4. torch.rand(size, device=\u0027cuda\u0027) \uc774\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 device \uc778\uc218\ub97c \ubc1b\uc544 \uc0c8\ub85c\uc6b4 tensor\ub97c \uc0dd\uc131\ud558\ub294 \ubaa8\ub4e0 \ud568\uc218\uc5d0 \uc801\uc6a9\ub429\ub2c8\ub2e4: torch.rand(), torch.zeros(), torch.full() \ub4f1. \ud63c\ud569 \uc815\ubc00\ub3c4\uc640 AMP \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95# \ud63c\ud569 \uc815\ubc00\ub3c4\ub294 Tensor Cores \ub97c \ud65c\uc6a9\ud558\uc5ec Volta\ub098 \ucd5c\uc2e0 GPU \uc544\ud0a4\ud14d\ucc98\uc5d0\uc11c \ucd5c\ub300 3\ubc30\uc758 \uc804\uccb4 \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. Tensor Cores\ub97c \uc0ac\uc6a9\ud558\ub824\uba74 AMP\ub97c \ud65c\uc131\ud654\ud558\uace0 \ud589\ub82c/\ud150\uc11c \ucc28\uc6d0\uc774 Tensor Cores\ub97c \uc0ac\uc6a9\ud558\ub294 \ucee4\ub110 \ud638\ucd9c \uc694\uad6c \uc0ac\ud56d\uc744 \ucda9\uc871\ud574\uc57c \ud569\ub2c8\ub2e4. Tensor Cores\ub97c \uc0ac\uc6a9\ud558\ub824\uba74: size\ub97c 8\uc758 \ubc30\uc218\ub85c \uc124\uc815 (Tensor Cores\uc758 \ucc28\uc6d0\uc5d0 \ub9de\ucd94\uae30 \uc704\ud574) Deep Learning Performance Documentation \uc5d0\uc11c \uc790\uc138\ud55c \uc815\ubcf4\uc640 \ub808\uc774\uc5b4 \uc720\ud615\uc5d0 \ub530\ub978 \uac00\uc774\ub4dc\ub77c\uc778\uc744 \ucc38\uc870\ud558\uc138\uc694. \ub808\uc774\uc5b4 \ud06c\uae30\uac00 \uace0\uc815\ub418\uc9c0 \uc54a\uace0 \ub2e4\ub978 \ub9e4\uac1c\ubcc0\uc218\uc5d0\uc11c \uc720\ub3c4\ub418\ub294 \uacbd\uc6b0\uc5d0\ub3c4 \uba85\uc2dc\uc801\uc73c\ub85c \ud328\ub529\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uc608\uc2dc: NLP \ubaa8\ub378\uc758 \uc5b4\ud718 \ud06c\uae30 \ub4f1). AMP \ud65c\uc131\ud654\ud558\uae30 \ud63c\ud569 \uc815\ubc00\ub3c4 \ud559\uc2b5\uacfc AMP \uc18c\uac1c: video, slides PyTorch 1.6\ubd80\ud130 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 PyTorch AMP: documentation, examples, tutorial \uac00\ubcc0 \uc785\ub825 \uae38\uc774\uc5d0 \ub300\ube44\ud558\uc5ec \uba54\ubaa8\ub9ac \ubbf8\ub9ac \ud560\ub2f9\ud558\uae30# \uc74c\uc131 \uc778\uc2dd \ub610\ub294 NLP \ubaa8\ub378\uc740 \uc885\uc885 \uac00\ubcc0 \uc2dc\ud000\uc2a4 \uae38\uc774\ub97c \uac00\uc9c4 tensor\ub97c \uc785\ub825\uc73c\ub85c \ud559\uc2b5\ub429\ub2c8\ub2e4. \uac00\ubcc0 \uae38\uc774\ub294 PyTorch \uce90\uc2f1 \ud560\ub2f9\uae30\uc5d0\uc11c \ubb38\uc81c\ub97c \uc77c\uc73c\ud0ac \uc218 \uc788\uc73c\uba70, \uc131\ub2a5 \uc800\ud558 \ub610\ub294 \uc608\uae30\uce58 \uc54a\uc740 \uba54\ubaa8\ub9ac \ubd80\uc871 \uc624\ub958\ub97c \ucd08\ub798\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc9e7\uc740 \uc2dc\ud000\uc2a4 \uae38\uc774\uc758 \ubc30\uce58\uac00 \ub354 \uae34 \uc2dc\ud000\uc2a4 \uae38\uc774\uc758 \ubc30\uce58\ub85c \uc774\uc5b4\uc9c0\uba74, PyTorch\ub294 \uc774\uc804 \ubc18\ubcf5\uc758 \uc911\uac04 \ubc84\ud37c\ub97c \ud574\uc81c\ud558\uace0 \uc0c8 \ubc84\ud37c\ub97c \uc7ac\ud560\ub2f9\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774 \uacfc\uc815\uc740 \uc2dc\uac04\uc774 \ub9ce\uc774 \uc18c\uc694\ub418\uba70 \uce90\uc2f1 \ud560\ub2f9\uae30\uc5d0\uc11c \uc870\uac01\ud654(fragmentation)\ub97c \uc77c\uc73c\ucf1c \uba54\ubaa8\ub9ac \ubd80\uc871 \uc624\ub958\ub97c \uc720\ubc1c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc77c\ubc18\uc801\uc778 \ud574\uacb0 \ubc29\ubc95\uc740 \ubbf8\ub9ac \ud560\ub2f9(preallocation)\uc744 \uad6c\ud604\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \ub2e4\uc74c \ub2e8\uacc4\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4: \ucd5c\ub300 \uc2dc\ud000\uc2a4 \uae38\uc774(\ud6c8\ub828 \ub370\uc774\ud130 \uc138\ud2b8\uc758 \ucd5c\ub300 \uae38\uc774 \ub610\ub294 \uc0ac\uc804 \uc815\uc758\ub41c \uc784\uacc4\uac12\uc5d0 \ud574\ub2f9)\ub97c \uac16\ub294 (\uc77c\ubc18\uc801\uc73c\ub85c \ubb34\uc791\uc704) \uc785\ub825 \ubc30\uce58\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc0dd\uc131\ub41c \ubc30\uce58\ub85c \uc21c\ubc29\ud5a5 \ubc0f \uc5ed\ubc29\ud5a5 \uacfc\uc815\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4. \uc635\ud2f0\ub9c8\uc774\uc800\ub098 \ud559\uc2b5\ub960 \uc2a4\ucf00\uc904\ub7ec\ub294 \uc2e4\ud589\ud558\uc9c0 \uc54a\uc73c\uba70, \uc774 \ub2e8\uacc4\ub294 \uc774\ud6c4 \ud559\uc2b5\uc5d0\uc11c \uc7ac\uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ucd5c\ub300 \ud06c\uae30\uc758 \ubc84\ud37c\ub97c \ubbf8\ub9ac \ud560\ub2f9\ud569\ub2c8\ub2e4. \ubcc0\ud654\ub3c4\ub97c 0\uc73c\ub85c \uc124\uc815\ud569\ub2c8\ub2e4. \uc815\uaddc \ud559\uc2b5\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4. \ubd84\uc0b0 \ucd5c\uc801\ud654 \ubc29\ubc95# \ud6a8\uc728\uc801\uc778 \ub370\uc774\ud130 \ubcd1\ub82c \ubc31\uc5d4\ub4dc \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95# PyTorch\uc5d0\ub294 \ub370\uc774\ud130 \ubcd1\ub82c \ud559\uc2b5\uc744 \uad6c\ud604\ud558\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc774 \uc788\uc2b5\ub2c8\ub2e4: torch.nn.DataParallel torch.nn.parallel.DistributedDataParallel DistributedDataParallel \uc740 \ub2e4\uc911 GPU\uc5d0 \ub300\ud574 \ud6e8\uc52c \ub354 \ub098\uc740 \uc131\ub2a5\uacfc \ud655\uc7a5\uc131\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc790\uc138\ud55c \uc815\ubcf4\ub294 PyTorch \ubb38\uc11c\uc758 relevant section of CUDA Best Practices \ub97c \ucc38\uc870\ud558\uc138\uc694. \ud559\uc2b5\ud560 \ub54c DistributedDataParallel \uc774\ub098 \ubcc0\ud654\ub3c4 \ucd95\uc801 \uc0ac\uc6a9 \uc2dc \ubd88\ud544\uc694\ud55c all-reduce \uac74\ub108\ub6f0\ub294 \ubc29\ubc95# \uae30\ubcf8\uc801\uc73c\ub85c torch.nn.parallel.DistributedDataParallel \uc740 \ubaa8\ub4e0 \uc5ed\uc804\ud30c \uacfc\uc815 \ud6c4\uc5d0 \ubcc0\ud654\ub3c4 all-reduce\ub97c \uc2e4\ud589\ud558\uc5ec \ud559\uc2b5\uc5d0 \ucc38\uc5ec\ud558\ub294 \ubaa8\ub4e0 \uc6cc\ucee4\uc5d0\uc11c\uc758 \ud3c9\uade0 \ubcc0\ud654\ub3c4\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. \ud559\uc2b5 \uc2dc \ubcc0\ud654\ub3c4 \ucd95\uc801\uc744 N\ub2e8\uacc4 \ub3d9\uc548 \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0, \ubaa8\ub4e0 \ud559\uc2b5 \ub2e8\uacc4 \ud6c4\uc5d0 all-reduce\uac00 \uc694\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub9c8\uc9c0\ub9c9 \uc5ed\uc804\ud30c \ud638\ucd9c \uc9c1\ud6c4, \uc989 \uc635\ud2f0\ub9c8\uc774\uc800 \uc2e4\ud589 \uc9c1\uc804\uc5d0\ub9cc all-reduce\ub97c \uc218\ud589\ud558\uba74 \ub429\ub2c8\ub2e4. DistributedDataParallel \uc740 \ud2b9\uc815 \ubc18\ubcf5\uc5d0 \ub300\ud574 \ubcc0\ud654\ub3c4 all-reduce\ub97c \ube44\ud65c\uc131\ud654\ud558\ub294 no_sync() \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. no_sync() \ub294 \ubcc0\ud654\ub3c4 \ucd95\uc801\uc758 \uccab N-1 \ubc18\ubcf5\uc5d0 \uc801\uc6a9\ub418\uc5b4\uc57c \ud558\uba70 \ub9c8\uc9c0\ub9c9 \ubc18\ubcf5\uc740 \uae30\ubcf8 \uc2e4\ud589\uc744 \ub530\ub974\uace0 \ud544\uc694\ud55c \ubcc0\ud654\ub3c4 all-reduce\ub97c \uc218\ud589\ud574\uc57c \ud569\ub2c8\ub2e4. DistributedDataParallel(find_unused_parameters=True) \ub97c \uc0ac\uc6a9\ud560 \ub54c \uc0dd\uc131\uc790\uc640 \uc2e4\ud589 \ub808\uc774\uc5b4 \uc21c\uc11c\ub97c \uc77c\uce58\uc2dc\ud0a4\ub294 \ubc29\ubc95# torch.nn.parallel.DistributedDataParallel \uc740 find_unused_parameters=True \uc640 \ud568\uaed8 \ubaa8\ub378 \uc0dd\uc131\uc790\uc5d0\uc11c\uc758 \ub808\uc774\uc5b4\uc640 \ud30c\ub77c\ubbf8\ud130 \uc21c\uc11c\ub97c \uc0ac\uc6a9\ud558\uc5ec DistributedDataParallel \ubcc0\ud654\ub3c4 all-reduce\ub97c \uc704\ud55c \ubc84\ud0b7\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. DistributedDataParallel \uc740 all-reduce\ub97c \uc5ed\uc804\ud30c\uc640 \uacb9\uce58\uac8c \uc218\ud589\ud569\ub2c8\ub2e4. \ud2b9\uc815 \ubc84\ud0b7\uc5d0 \ub300\ud55c all-reduce\ub294 \uc8fc\uc5b4\uc9c4 \ubc84\ud0b7\uc758 \ubaa8\ub4e0 \ud30c\ub77c\ubbf8\ud130\uc5d0 \ub300\ud55c \ubcc0\ud654\ub3c4\uac00 \ubaa8\ub450 \uc900\ube44\ub418\uc5c8\uc744 \ub54c \ube44\ub3d9\uae30\uc801\uc73c\ub85c \uc791\ub3d9\ub429\ub2c8\ub2e4. \ucd5c\ub300\ub85c \uacb9\uce58\uac8c \ud558\ub824\uba74 \ubaa8\ub378 \uc0dd\uc131\uc790\uc5d0\uc11c\uc758 \uc21c\uc11c\uac00 \uc2e4\uc81c \uc2e4\ud589 \uc911\uc778 \uc21c\uc11c\uc640 \ub300\ub7b5\uc801\uc73c\ub85c \uc77c\uce58\ud574\uc57c \ud569\ub2c8\ub2e4. \uc21c\uc11c\uac00 \ub9de\uc9c0 \uc54a\uc73c\uba74 \uc804\uccb4 \ubc84\ud0b7\uc5d0 \ub300\ud55c all-reduce\ub294 \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ub3c4\ucc29\ud558\ub294 \ubcc0\ud654\ub3c4\ub97c \uae30\ub2e4\ub9ac\uac8c \ub418\uba70, \uc774\ub294 \uc5ed\uc804\ud30c\uc640 all-reduce \uac04\uc758 \uacb9\uce68\uc744 \uc904\uc77c \uc218 \uc788\uace0, all-reduce\uac00 \ub178\ucd9c\ub418\uc5b4 \ud559\uc2b5 \uc18d\ub3c4\uac00 \ub290\ub824\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. find_unused_parameters=False \uac00 (\uae30\ubcf8 \uc124\uc815)\uc778 DistributedDataParallel \uc740 \uc5ed\uc804\ud30c \uc911\uc5d0 \ubc1c\uacac\ub41c \uc5f0\uc0b0 \uc21c\uc11c\ub97c \uae30\ubc18\uc73c\ub85c \uc790\ub3d9\uc73c\ub85c \ubc84\ud0b7\uc744 \ud615\uc131\ud569\ub2c8\ub2e4. find_unused_parameters=False \ub97c \uc0ac\uc6a9\ud560 \ub54c\ub294 \ucd5c\uc801\uc758 \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\uae30 \uc704\ud574 \ub808\uc774\uc5b4\ub098 \ud30c\ub77c\ubbf8\ud130\uc758 \uc21c\uc11c\ub97c \uc7ac\uc870\uc815\ud560 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4. \ubd84\uc0b0 \uc124\uc815\uc5d0\uc11c \uc791\uc5c5 \ubd80\ud558\ub97c \ubd84\uc0b0\ud558\ub294 \ubc29\ubc95# \uc791\uc5c5 \ubd80\ud558 \ubd88\uade0\ud615\uc740 \uc77c\ubc18\uc801\uc73c\ub85c \uc21c\ucc28\uc801\uc778 \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub294 \ubaa8\ub378(\uc608\uc2dc: \uc74c\uc131 \uc778\uc2dd, \ubc88\uc5ed, \uc5b8\uc5b4 \ubaa8\ub378 \ub4f1) \uc5d0\uc11c \ubc1c\uc0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud558\ub098\uc758 \uc7a5\uce58\uac00 \ub098\uba38\uc9c0 \uc7a5\uce58\ub4e4\ubcf4\ub2e4 \uae34 \uc2dc\ud000\uc2a4 \uae38\uc774\ub97c \uac00\uc9c4 \ub370\uc774\ud130 \ubc30\uce58\ub97c \ubc1b\uc73c\uba74, \ubaa8\ub4e0 \uc7a5\uce58\uac00 \ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc791\uc5c5\uc744 \ub05d\ub0b4\ub294 \uc6cc\ucee4\ub97c \uae30\ub2e4\ub9ac\uac8c \ub429\ub2c8\ub2e4. \uc5ed\uc804\ud30c\ub294 DistributedDataParallel \ubc31\uc5d4\ub4dc\uc640 \ud568\uaed8 \ubd84\uc0b0 \uc124\uc815\uc5d0\uc11c \uc554\ubb35\uc801\uc778 \ub3d9\uae30\ud654 \uc9c0\uc810\uc73c\ub85c \uc791\uc6a9\ud569\ub2c8\ub2e4. \uc791\uc5c5 \ub85c\ub4dc \ubc38\ub7f0\uc2f1 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294 \ubc29\ubc95\uc740 \uc5ec\ub7ec \uac00\uc9c0\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ud575\uc2ec\uc740 \uac01 \uc804\uc5ed \ubc30\uce58 \ub0b4\uc5d0\uc11c \ubaa8\ub4e0 \uc6cc\ucee4\uc5d0 \uac78\uccd0 \uc791\uc5c5 \ubd80\ud558\ub97c \uac00\ub2a5\ud55c \ud55c \uade0\uc77c\ud558\uac8c \ubd84\ubc30\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, Transformer\ub294 \ubc30\uce58 \ub0b4\uc5d0\uc11c \ub300\ub7b5 \uc77c\uc815\ud55c \uc218\uc758 \ud1a0\ud070(\ubcc0\ub3d9\ud558\ub294 \uc218\uc758 \uc2dc\ud000\uc2a4)\uc744 \ud615\uc131\ud558\uc5ec \ubd88\uade0\ud615\uc744 \ud574\uacb0\ud558\uba70, \ub2e4\ub978 \ubaa8\ub378\uc740 \uc720\uc0ac\ud55c \uc2dc\ud000\uc2a4 \uae38\uc774\ub97c \uac00\uc9c4 \uc0d8\ud50c\uc744 \ubc84\ud0b7\ud654\ud558\uac70\ub098 \ub370\uc774\ud130\uc14b\uc744 \uc2dc\ud000\uc2a4 \uae38\uc774\uc5d0 \ub530\ub77c \uc815\ub82c\ud558\uc5ec \ubd88\uade0\ud615\uc744 \ud574\uacb0\ud569\ub2c8\ub2e4. Total running time of the script: (0 minutes 0.003 seconds) Download Jupyter notebook: tuning_guide.ipynb Download Python source code: tuning_guide.py Download zipped: tuning_guide.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/recipes/recipes/tuning_guide.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>