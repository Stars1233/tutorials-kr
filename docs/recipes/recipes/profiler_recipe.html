
<!DOCTYPE html>


<html lang="ko" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="PyTorch 프로파일러(Profiler)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/recipes/recipes/profiler_recipe.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="이 레시피에서는 어떻게 PyTorch 프로파일러를 사용하는지, 그리고 모델의 연산자들이 소비하는 메모리와 시간을 측정하는 방법을 살펴보겠습니다. 개요: PyTorch는 사용자가 모델 내의 연산 비용이 큰(expensive) 연산자들이 무엇인지 알고싶을 때 유용하게 사용할 수 있는 간단한 프로파일러 API를 포함하고 있습니다. 이 레시피에서는 모델의 성능(performance)을 분석하려고 할 때 어떻게 프로파일러를 사용해야 하는지를 보여주기 위해 간단한 ResNet 모델을 사용하겠습니다. 설정(Setup): torch 와 to..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="이 레시피에서는 어떻게 PyTorch 프로파일러를 사용하는지, 그리고 모델의 연산자들이 소비하는 메모리와 시간을 측정하는 방법을 살펴보겠습니다. 개요: PyTorch는 사용자가 모델 내의 연산 비용이 큰(expensive) 연산자들이 무엇인지 알고싶을 때 유용하게 사용할 수 있는 간단한 프로파일러 API를 포함하고 있습니다. 이 레시피에서는 모델의 성능(performance)을 분석하려고 할 때 어떻게 프로파일러를 사용해야 하는지를 보여주기 위해 간단한 ResNet 모델을 사용하겠습니다. 설정(Setup): torch 와 to..." />
<meta property="og:ignore_canonical" content="true" />

    <title>PyTorch 프로파일러(Profiler) &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../../_static/doctools.js?v=92e14aea"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/translations.js?v=b5f768d8"></script>
    <script src="../../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'recipes/recipes/profiler_recipe';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/recipes/recipes/profiler_recipe.html" />
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../../genindex.html" />
    <link rel="search" title="검색" href="../../search.html" />
    <link rel="next" title="Captum을 사용하여 모델 해석하기" href="Captum_Recipe.html" />
    <link rel="prev" title="PyTorch에서 변화도를 0으로 만들기" href="zeroing_out_gradients.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../_static/js/theme.js"></script>
<script type="text/javascript" src="../../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="defining_a_neural_network.html">Pytorch를 사용해 신경망 정의하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_logs.html">(beta) torch.compile과 함께 TORCH_LOGS 파이썬 API 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="what_is_state_dict.html">PyTorch에서 state_dict란 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="warmstarting_model_using_parameters_from_a_different_model.html">PyTorch에서 다른 모델의 매개변수를 사용하여 빠르게 모델 시작하기(warmstart)</a></li>
<li class="toctree-l1"><a class="reference internal" href="zeroing_out_gradients.html">PyTorch에서 변화도를 0으로 만들기</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">PyTorch 프로파일러(Profiler)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Captum_Recipe.html">Captum을 사용하여 모델 해석하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_with_pytorch.html">PyTorch로 TensorBoard 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp_recipe.html">자동 혼합 정밀도(Automatic Mixed Precision) 가이드</a></li>
<li class="toctree-l1"><a class="reference internal" href="tuning_guide.html">성능 튜닝 가이드</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiling_optimizer.html">(beta) Compiling the optimizer with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="timer_quick_start.html">Timer 빠르게 시작하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_backend_ipex.html">Intel® CPU에서의 Intel® Extension for PyTorch* 백엔드</a></li>
<li class="toctree-l1"><a class="reference internal" href="../zero_redundancy_optimizer.html">Shard Optimizer States with ZeroRedundancyOptimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cuda_rpc.html">Direct Device-to-Device Communication with TensorPipe CUDA RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_comm_debug_mode.html">Getting Started with <code class="docutils literal notranslate"><span class="pre">CommDebugMode</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">PyTorch Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_load_state_dict_tips.html">Tips for Loading an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> from a Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="reasoning_about_shapes.html">PyTorch의 Shape들에 대한 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="swap_tensors.html">Extension points in <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> for <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> and tensor subclasses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_export_aoti_python.html"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_with_pytorch.html">PyTorch로 TensorBoard 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../inference_tuning_on_aws_graviton.html">(Beta) PyTorch Inference Performance Tuning on AWS Graviton Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../amx.html">Leverage Intel® Advanced Matrix Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_torch_function_modes.html">(beta) Utilizing Torch Function modes with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiling_optimizer_lr_scheduler.html">(beta) Running the compiled optimizer with an LR Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../foreach_map.html">Explicit horizontal fusion with foreach_map and torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_user_defined_triton_kernel_tutorial.html">사용자 정의 Triton 커널을 ``torch.compile``과 함께 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_caching_tutorial.html">Compile Time Caching in <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_caching_configuration_tutorial.html">Compile Time Caching Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regional_compilation.html">Reducing torch.compile cold start compilation time with regional compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regional_aot.html">Reducing AoT cold start compilation time with regional compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intel_neural_compressor_for_pytorch.html">Ease-of-use quantization for PyTorch with Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_device_mesh.html">Getting Started with DeviceMesh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_checkpoint_recipe.html">Getting Started with Distributed Checkpoint (DCP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_async_checkpoint_recipe.html">Asynchronous Saving with Distributed Checkpoint (DCP)</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../recipes_index.html" class="nav-link">Recipes</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">PyTorch...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../recipes_index.html">
        <meta itemprop="name" content="Recipes">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="PyTorch 프로파일러(Profiler)">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">recipes/recipes/profiler_recipe</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-recipes-recipes-profiler-recipe-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="pytorch-profiler">
<span id="sphx-glr-recipes-recipes-profiler-recipe-py"></span><h1>PyTorch 프로파일러(Profiler)<a class="headerlink" href="#pytorch-profiler" title="Link to this heading">#</a></h1>
<p>이 레시피에서는 어떻게 PyTorch 프로파일러를 사용하는지, 그리고 모델의 연산자들이 소비하는 메모리와 시간을 측정하는 방법을 살펴보겠습니다.</p>
<section id="id1">
<h2>개요<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>PyTorch는 사용자가 모델 내의 연산 비용이 큰(expensive) 연산자들이 무엇인지 알고싶을 때 유용하게 사용할 수 있는 간단한 프로파일러 API를 포함하고 있습니다.</p>
<p>이 레시피에서는 모델의 성능(performance)을 분석하려고 할 때 어떻게 프로파일러를 사용해야 하는지를 보여주기 위해 간단한 ResNet 모델을 사용하겠습니다.</p>
</section>
<section id="setup">
<h2>설정(Setup)<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> 와 <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> 을 설치하기 위해서 아래의 커맨드를 입력합니다:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision
</pre></div>
</div>
</section>
<section id="steps">
<h2>단계(Steps)<a class="headerlink" href="#steps" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>필요한 라이브러리들 불러오기</p></li>
<li><p>간단한 ResNet 모델 인스턴스화 하기</p></li>
<li><p>프로파일러를 사용하여 실행시간 분석하기</p></li>
<li><p>프로파일러를 사용하여 메모리 소비 분석하기</p></li>
<li><p>추적기능 사용하기</p></li>
<li><p>Examining stack traces</p></li>
<li><p>Using profiler to analyze long-running jobs</p></li>
</ol>
<section id="id2">
<h3>1. 필요한 라이브러리들 불러오기<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>이 레시피에서는 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 와 <code class="docutils literal notranslate"><span class="pre">torchvision.models</span></code>,
그리고 <code class="docutils literal notranslate"><span class="pre">profiler</span></code> 모듈을 사용합니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">models</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.profiler</span><span class="w"> </span><span class="kn">import</span> <span class="n">profile</span><span class="p">,</span> <span class="n">record_function</span><span class="p">,</span> <span class="n">ProfilerActivity</span>
</pre></div>
</div>
</section>
<section id="resnet">
<h3>2. 간단한 ResNet 모델 인스턴스화 하기<a class="headerlink" href="#resnet" title="Link to this heading">#</a></h3>
<p>ResNet 모델 인스턴스를 만들고 입력값을
준비합니다 :</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3>3. 프로파일러를 사용하여 실행시간 분석하기<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>PyTorch 프로파일러는 컨텍스트 메니저(context manager)를 통해 활성화되고,
여러 매개변수를 받을 수 있습니다. 유용한 몇 가지 매개변수는 다음과 같습니다:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">activities</span></code> - a list of activities to profile:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ProfilerActivity.CPU</span></code> - PyTorch operators, TorchScript functions and
user-defined code labels (see <code class="docutils literal notranslate"><span class="pre">record_function</span></code> below);</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ProfilerActivity.CUDA</span></code> - on-device CUDA kernels;</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">record_shapes</span></code> - 연사자 입력(input)의 shape을 기록할지 여부;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">profile_memory</span></code> - 모델의 텐서(Tensor)들이 소비하는 메모리 양을 보고(report)할지 여부;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_cuda</span></code> - CUDA 커널의 실행시간을 측정할지 여부;</p></li>
</ul>
<p>Note: when using CUDA, profiler also shows the runtime CUDA events
occuring on the host.</p>
<p>프로파일러를 사용하여 어떻게 실행시간을 분석하는지 보겠습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">],</span> <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;model_inference&quot;</span><span class="p">):</span>
        <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">record_function</span></code> 컨텍스트 관리자를 사용하여 임의의 코드 범위에
사용자가 지정한 이름으로 레이블(label)을 표시할 수 있습니다.
(위 예제에서는 <code class="docutils literal notranslate"><span class="pre">model_inference</span></code> 를 레이블로 사용했습니다.)</p>
<p>프로파일러를 사용하면 프로파일러 컨텍스트 관리자로 감싸진(wrap) 코드 범위를
실행하는 동안 어떤 연산자들이 호출되었는지 확인할 수 있습니다.</p>
<p>만약 여러 프로파일러의 범위가 동시에 활성화된 경우(예. PyTorch 쓰레드가 병렬로
실행 중인 경우), 각 프로파일링 컨텍스트 관리자는 각각의 범위 내의 연산자들만
추적(track)합니다.
프로파일러는 또한 <code class="docutils literal notranslate"><span class="pre">torch.jit._fork</span></code> 로 실행된 비동기 작업과
(역전파 단계의 경우) <code class="docutils literal notranslate"><span class="pre">backward()</span></code> 의 호출로 실행된 역전파 연산자들도
자동으로 프로파일링합니다.</p>
<p>위 코드를 실행한 통계를 출력해보겠습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;cpu_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                  model_inference         7.18%       3.482ms       100.00%      48.497ms      48.497ms             1
                     aten::conv2d         0.18%      85.560us        77.19%      37.435ms       1.872ms            20
                aten::convolution         0.46%     222.623us        77.01%      37.349ms       1.867ms            20
               aten::_convolution         0.32%     156.516us        76.55%      37.127ms       1.856ms            20
         aten::mkldnn_convolution        75.92%      36.819ms        76.23%      36.970ms       1.849ms            20
                 aten::batch_norm         0.12%      56.538us         9.91%       4.805ms     240.234us            20
     aten::_batch_norm_impl_index         0.23%     110.516us         9.79%       4.748ms     237.407us            20
          aten::native_batch_norm         9.27%       4.494ms         9.54%       4.628ms     231.384us            20
                      aten::relu_         0.39%     187.916us         1.80%     874.065us      51.416us            17
                 aten::max_pool2d         0.05%      26.233us         1.43%     693.791us     693.791us             1
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 48.497ms
</pre></div>
</div>
<p>(몇몇 열을 제외하고) 출력값이 이렇게 보일 것입니다:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>———————————  ————  ————  ————  ————</dt><dd><p>Name      Self CPU     CPU total  CPU time avg    # of Calls</p>
</dd>
<dt>———————————  ————  ————  ————  ————</dt><dd><blockquote>
<div><blockquote>
<div><blockquote>
<div><blockquote>
<div><dl class="simple">
<dt>model_inference       5.509ms      57.503ms      57.503ms             1</dt><dd><p>aten::conv2d     231.000us      31.931ms       1.597ms            20</p>
</dd>
</dl>
</div></blockquote>
<p>aten::convolution     250.000us      31.700ms       1.585ms            20</p>
</div></blockquote>
<p>aten::_convolution     336.000us      31.450ms       1.573ms            20</p>
</div></blockquote>
<dl class="simple">
<dt>aten::mkldnn_convolution      30.838ms      31.114ms       1.556ms            20</dt><dd><p>aten::batch_norm     211.000us      14.693ms     734.650us            20</p>
</dd>
</dl>
</div></blockquote>
<dl>
<dt>aten::_batch_norm_impl_index     319.000us      14.482ms     724.100us            20</dt><dd><dl>
<dt>aten::native_batch_norm       9.229ms      14.109ms     705.450us            20</dt><dd><blockquote>
<div><p>aten::mean     332.000us       2.631ms     125.286us            21</p>
</div></blockquote>
<p>aten::select       1.668ms       2.292ms       8.988us           255</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p>———————————  ————  ————  ————  ————
Self CPU time total: 57.549ms</p>
<p>예상했던 대로, 대부분의 시간이 합성곱(convolution) 연산(특히 <code class="docutils literal notranslate"><span class="pre">MKL-DNN</span></code> 을 지원하도록
컴파일된 PyTorch의 경우에는 <code class="docutils literal notranslate"><span class="pre">mkldnn_convolution</span></code> )에서 소요되는 것을 확인할 수 있습니다.
(결과 열들 중) Self CPU time과 CPU time의 차이에 유의해야 합니다 -
연산자는 다른 연산자들을 호출할 수 있으며, Self CPU time에는 하위(child) 연산자 호출에서 발생한
시간을 제외해서, Totacl CPU time에는 포함해서 표시합니다.
You can choose to sort by the self cpu time by passing
<code class="docutils literal notranslate"><span class="pre">sort_by=&quot;self_cpu_time_total&quot;</span></code> into the <code class="docutils literal notranslate"><span class="pre">table</span></code> call.</p>
<p>보다 세부적인 결과 정보 및 연산자의 입력 shape을 함께 보려면 <code class="docutils literal notranslate"><span class="pre">group_by_input_shape=True</span></code> 를
인자로 전달하면 됩니다
(note: this requires running the profiler with <code class="docutils literal notranslate"><span class="pre">record_shapes=True</span></code>):</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">(</span><span class="n">group_by_input_shape</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;cpu_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------
                  model_inference         7.18%       3.482ms       100.00%      48.497ms      48.497ms             1                                                                                []
                     aten::conv2d         0.06%      28.275us        27.06%      13.125ms      13.125ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]
                aten::convolution         0.15%      73.125us        27.00%      13.097ms      13.097ms             1                     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], []]
               aten::_convolution         0.08%      39.220us        26.85%      13.024ms      13.024ms             1     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], [], [], [], [], []]
         aten::mkldnn_convolution        26.70%      12.946ms        26.77%      12.984ms      12.984ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]
                     aten::conv2d         0.03%      14.640us        19.12%       9.274ms       2.318ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]
                aten::convolution         0.08%      37.078us        19.09%       9.259ms       2.315ms             4                     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []]
               aten::_convolution         0.07%      32.106us        19.02%       9.222ms       2.306ms             4     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]
         aten::mkldnn_convolution        18.90%       9.165ms        18.95%       9.190ms       2.297ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]
                     aten::conv2d         0.02%       9.454us         5.14%       2.491ms     830.217us             3                          [[5, 128, 28, 28], [128, 128, 3, 3], [], [], [], [], []]
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------
Self CPU time total: 48.497ms
</pre></div>
</div>
<p>(몇몇 열을 제외하고) 출력값이 이렇게 보일 것입니다:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>---------------------------------<span class="w">  </span>------------<span class="w">  </span>-------------------------------------------
<span class="w">                             </span>Name<span class="w">     </span>CPU<span class="w"> </span>total<span class="w">                                 </span>Input<span class="w"> </span>Shapes
---------------------------------<span class="w">  </span>------------<span class="w">  </span>-------------------------------------------
<span class="w">                  </span>model_inference<span class="w">      </span><span class="m">57</span>.503ms<span class="w">                                           </span><span class="o">[]</span>
<span class="w">                     </span>aten::conv2d<span class="w">       </span><span class="m">8</span>.008ms<span class="w">      </span><span class="o">[</span><span class="m">5</span>,64,56,56<span class="o">]</span>,<span class="w"> </span><span class="o">[</span><span class="m">64</span>,64,3,3<span class="o">]</span>,<span class="w"> </span><span class="o">[]</span>,<span class="w"> </span>...,<span class="w"> </span><span class="o">[]]</span>
<span class="w">                </span>aten::convolution<span class="w">       </span><span class="m">7</span>.956ms<span class="w">     </span><span class="o">[[</span><span class="m">5</span>,64,56,56<span class="o">]</span>,<span class="w"> </span><span class="o">[</span><span class="m">64</span>,64,3,3<span class="o">]</span>,<span class="w"> </span><span class="o">[]</span>,<span class="w"> </span>...,<span class="w"> </span><span class="o">[]]</span>
<span class="w">               </span>aten::_convolution<span class="w">       </span><span class="m">7</span>.909ms<span class="w">     </span><span class="o">[[</span><span class="m">5</span>,64,56,56<span class="o">]</span>,<span class="w"> </span><span class="o">[</span><span class="m">64</span>,64,3,3<span class="o">]</span>,<span class="w"> </span><span class="o">[]</span>,<span class="w"> </span>...,<span class="w"> </span><span class="o">[]]</span>
<span class="w">         </span>aten::mkldnn_convolution<span class="w">       </span><span class="m">7</span>.834ms<span class="w">     </span><span class="o">[[</span><span class="m">5</span>,64,56,56<span class="o">]</span>,<span class="w"> </span><span class="o">[</span><span class="m">64</span>,64,3,3<span class="o">]</span>,<span class="w"> </span><span class="o">[]</span>,<span class="w"> </span>...,<span class="w"> </span><span class="o">[]]</span>
<span class="w">                     </span>aten::conv2d<span class="w">       </span><span class="m">6</span>.332ms<span class="w">    </span><span class="o">[[</span><span class="m">5</span>,512,7,7<span class="o">]</span>,<span class="w"> </span><span class="o">[</span><span class="m">512</span>,512,3,3<span class="o">]</span>,<span class="w"> </span><span class="o">[]</span>,<span class="w"> </span>...,<span class="w"> </span><span class="o">[]]</span>
<span class="w">                </span>aten::convolution<span class="w">       </span><span class="m">6</span>.303ms<span class="w">    </span><span class="o">[[</span><span class="m">5</span>,512,7,7<span class="o">]</span>,<span class="w"> </span><span class="o">[</span><span class="m">512</span>,512,3,3<span class="o">]</span>,<span class="w"> </span><span class="o">[]</span>,<span class="w"> </span>...,<span class="w"> </span><span class="o">[]]</span>
<span class="w">               </span>aten::_convolution<span class="w">       </span><span class="m">6</span>.273ms<span class="w">    </span><span class="o">[[</span><span class="m">5</span>,512,7,7<span class="o">]</span>,<span class="w"> </span><span class="o">[</span><span class="m">512</span>,512,3,3<span class="o">]</span>,<span class="w"> </span><span class="o">[]</span>,<span class="w"> </span>...,<span class="w"> </span><span class="o">[]]</span>
<span class="w">         </span>aten::mkldnn_convolution<span class="w">       </span><span class="m">6</span>.233ms<span class="w">    </span><span class="o">[[</span><span class="m">5</span>,512,7,7<span class="o">]</span>,<span class="w"> </span><span class="o">[</span><span class="m">512</span>,512,3,3<span class="o">]</span>,<span class="w"> </span><span class="o">[]</span>,<span class="w"> </span>...,<span class="w"> </span><span class="o">[]]</span>
<span class="w">                     </span>aten::conv2d<span class="w">       </span><span class="m">4</span>.751ms<span class="w">  </span><span class="o">[[</span><span class="m">5</span>,256,14,14<span class="o">]</span>,<span class="w"> </span><span class="o">[</span><span class="m">256</span>,256,3,3<span class="o">]</span>,<span class="w"> </span><span class="o">[]</span>,<span class="w"> </span>...,<span class="w"> </span><span class="o">[]]</span>
---------------------------------<span class="w">  </span>------------<span class="w">  </span>-------------------------------------------
Self<span class="w"> </span>CPU<span class="w"> </span><span class="nb">time</span><span class="w"> </span>total:<span class="w"> </span><span class="m">57</span>.549ms
</pre></div>
</div>
<p>Note the occurence of <code class="docutils literal notranslate"><span class="pre">aten::convolution</span></code> twice with different input shapes.</p>
<p>Profiler can also be used to analyze performance of models executed on GPUs:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">activities</span><span class="o">=</span><span class="p">[</span>
        <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">],</span> <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">record_function</span><span class="p">(</span><span class="s2">&quot;model_inference&quot;</span><span class="p">):</span>
        <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;cuda_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                        model_inference         0.00%       0.000us         0.00%       0.000us       0.000us     175.291ms     26096.35%     175.291ms      87.645ms             2
                                        model_inference         0.61%       1.807ms       100.00%     297.875ms     297.875ms       0.000us         0.00%       2.261ms       2.261ms             1
                                           aten::conv2d         0.02%      62.357us        51.66%     153.899ms       7.695ms       0.000us         0.00%       1.832ms      91.605us            20
                                      aten::convolution         0.06%     183.470us        51.64%     153.837ms       7.692ms       0.000us         0.00%       1.832ms      91.605us            20
                                     aten::_convolution         0.05%     157.311us        51.58%     153.654ms       7.683ms       0.000us         0.00%       1.832ms      91.605us            20
                                aten::cudnn_convolution        38.49%     114.645ms        51.53%     153.496ms       7.675ms     441.244us        65.69%       1.832ms      91.605us            20
                       Runtime Triggered Module Loading        22.26%      66.323ms        22.26%      66.323ms       1.745ms     972.922us       144.84%     972.922us      25.603us            38
                                   cudaFuncSetAttribute         4.97%      14.817ms         5.17%      15.401ms     229.863us       0.000us         0.00%     671.456us      10.022us            67
                                  Lazy Function Loading         0.30%     886.844us         0.30%     886.844us      36.952us     560.986us        83.52%     560.986us      23.374us            24
                                       cudaLaunchKernel         1.70%       5.064ms        19.49%      58.061ms     509.304us       0.000us         0.00%     358.879us       3.148us           114
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 297.884ms
Self CUDA time total: 671.707us
</pre></div>
</div>
<p>(Note: the first use of CUDA profiling may bring an extra overhead.)</p>
<p>The resulting table output:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>-------------------------------------------------------<span class="w">  </span>------------<span class="w">  </span>------------
<span class="w">                                                   </span>Name<span class="w">     </span>Self<span class="w"> </span>CUDA<span class="w">    </span>CUDA<span class="w"> </span>total
-------------------------------------------------------<span class="w">  </span>------------<span class="w">  </span>------------
<span class="w">                                        </span>model_inference<span class="w">       </span><span class="m">0</span>.000us<span class="w">      </span><span class="m">11</span>.666ms
<span class="w">                                           </span>aten::conv2d<span class="w">       </span><span class="m">0</span>.000us<span class="w">      </span><span class="m">10</span>.484ms
<span class="w">                                      </span>aten::convolution<span class="w">       </span><span class="m">0</span>.000us<span class="w">      </span><span class="m">10</span>.484ms
<span class="w">                                     </span>aten::_convolution<span class="w">       </span><span class="m">0</span>.000us<span class="w">      </span><span class="m">10</span>.484ms
<span class="w">                             </span>aten::_convolution_nogroup<span class="w">       </span><span class="m">0</span>.000us<span class="w">      </span><span class="m">10</span>.484ms
<span class="w">                                      </span>aten::thnn_conv2d<span class="w">       </span><span class="m">0</span>.000us<span class="w">      </span><span class="m">10</span>.484ms
<span class="w">                              </span>aten::thnn_conv2d_forward<span class="w">      </span><span class="m">10</span>.484ms<span class="w">      </span><span class="m">10</span>.484ms
void<span class="w"> </span>at::native::im2col_kernel&lt;float&gt;<span class="o">(</span>long,<span class="w"> </span>float<span class="w"> </span>co...<span class="w">       </span><span class="m">3</span>.844ms<span class="w">       </span><span class="m">3</span>.844ms
<span class="w">                                      </span>sgemm_32x32x32_NN<span class="w">       </span><span class="m">3</span>.206ms<span class="w">       </span><span class="m">3</span>.206ms
<span class="w">                                  </span>sgemm_32x32x32_NN_vec<span class="w">       </span><span class="m">3</span>.093ms<span class="w">       </span><span class="m">3</span>.093ms
-------------------------------------------------------<span class="w">  </span>------------<span class="w">  </span>------------
Self<span class="w"> </span>CPU<span class="w"> </span><span class="nb">time</span><span class="w"> </span>total:<span class="w"> </span><span class="m">23</span>.015ms
Self<span class="w"> </span>CUDA<span class="w"> </span><span class="nb">time</span><span class="w"> </span>total:<span class="w"> </span><span class="m">11</span>.666ms
</pre></div>
</div>
<p>Note the occurence of on-device kernels in the output (e.g. <code class="docutils literal notranslate"><span class="pre">sgemm_32x32x32_NN</span></code>).</p>
</section>
<section id="id4">
<h3>4. 프로파일러를 사용하여 메모리 소비 분석하기<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>PyTorch 프로파일러는 모델의 연산자들을 실행하며 (모델의 텐서들이 사용하며) 할당(또는 해제)한
메모리의 양도 표시할 수 있습니다.
아래 출력 결과에서 ‘Self’ memory는 해당 연산자에 의해 호출된 하위(child) 연산자들을 제외한,
연산자 자체에 할당(해제)된 메모리에 해당합니다.
메모리 프로파일링 기능을 활성화하려면 <code class="docutils literal notranslate"><span class="pre">profile_memory=True</span></code> 를 인자로 전달하면 됩니다.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">],</span>
        <span class="n">profile_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;self_cpu_memory_usage&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                      aten::empty         0.72%     197.288us         0.72%     197.288us       0.986us      94.85 MB      94.85 MB           200
    aten::max_pool2d_with_indices         2.12%     582.848us         2.12%     582.848us     582.848us      11.48 MB      11.48 MB             1
                      aten::addmm         0.25%      69.163us         0.28%      78.173us      78.173us      19.53 KB      19.53 KB             1
                       aten::mean         0.06%      15.605us         0.41%     112.912us     112.912us      10.00 KB       9.99 KB             1
                       aten::div_         0.06%      15.916us         0.12%      32.224us      32.224us           8 B           4 B             1
              aten::empty_strided         0.01%       3.461us         0.01%       3.461us       3.461us           4 B           4 B             1
                     aten::conv2d         0.19%      52.158us        80.22%      22.017ms       1.101ms      47.37 MB           0 B            20
                aten::convolution         0.42%     114.002us        80.03%      21.965ms       1.098ms      47.37 MB           0 B            20
               aten::_convolution         0.29%      80.198us        79.62%      21.851ms       1.093ms      47.37 MB           0 B            20
         aten::mkldnn_convolution        78.93%      21.663ms        79.32%      21.770ms       1.089ms      47.37 MB           0 B            20
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 27.445ms
</pre></div>
</div>
<p>(몇몇 열은 제외하였습니다)</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>———————————  ————  ————  ————</dt><dd><p>Name       CPU Mem  Self CPU Mem    # of Calls</p>
</dd>
<dt>———————————  ————  ————  ————</dt><dd><blockquote>
<div><p>aten::empty      94.79 Mb      94.79 Mb           121</p>
</div></blockquote>
<dl>
<dt>aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1</dt><dd><blockquote>
<div><p>aten::addmm      19.53 Kb      19.53 Kb             1</p>
</div></blockquote>
<dl class="simple">
<dt>aten::empty_strided         572 b         572 b            25</dt><dd><dl class="simple">
<dt>aten::<a href="#id7"><span class="problematic" id="id8">resize_</span></a>         240 b         240 b             6</dt><dd><p>aten::abs         480 b         240 b             4
aten::add         160 b         160 b            20</p>
</dd>
</dl>
</dd>
<dt>aten::masked_select         120 b         112 b             1</dt><dd><p>aten::ne         122 b          53 b             6
aten::eq          60 b          30 b             2</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p>———————————  ————  ————  ————
Self CPU time total: 53.064ms</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;cpu_memory_usage&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                      aten::empty         0.72%     197.288us         0.72%     197.288us       0.986us      94.85 MB      94.85 MB           200
                 aten::batch_norm         0.16%      43.403us        12.38%       3.396ms     169.818us      47.41 MB           0 B            20
     aten::_batch_norm_impl_index         0.23%      62.749us        12.22%       3.353ms     167.647us      47.41 MB           0 B            20
          aten::native_batch_norm        11.42%       3.133ms        11.95%       3.281ms     164.045us      47.41 MB     -70.50 KB            20
                     aten::conv2d         0.19%      52.158us        80.22%      22.017ms       1.101ms      47.37 MB           0 B            20
                aten::convolution         0.42%     114.002us        80.03%      21.965ms       1.098ms      47.37 MB           0 B            20
               aten::_convolution         0.29%      80.198us        79.62%      21.851ms       1.093ms      47.37 MB           0 B            20
         aten::mkldnn_convolution        78.93%      21.663ms        79.32%      21.770ms       1.089ms      47.37 MB           0 B            20
                 aten::empty_like         0.10%      28.213us         0.18%      50.287us       2.514us      47.37 MB           0 B            20
                 aten::max_pool2d         0.02%       4.325us         2.14%     587.173us     587.173us      11.48 MB           0 B             1
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 27.445ms
</pre></div>
</div>
<p>(몇몇 열을 제외하고) 출력값이 이렇게 보일 것입니다:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<dl>
<dt>———————————  ————  ————  ————</dt><dd><p>Name       CPU Mem  Self CPU Mem    # of Calls</p>
</dd>
<dt>———————————  ————  ————  ————</dt><dd><blockquote>
<div><blockquote>
<div><blockquote>
<div><p>aten::empty      94.79 Mb      94.79 Mb           121</p>
</div></blockquote>
<p>aten::batch_norm      47.41 Mb           0 b            20</p>
</div></blockquote>
<dl>
<dt>aten::_batch_norm_impl_index      47.41 Mb           0 b            20</dt><dd><blockquote>
<div><dl>
<dt>aten::native_batch_norm      47.41 Mb           0 b            20</dt><dd><blockquote>
<div><blockquote>
<div><p>aten::conv2d      47.37 Mb           0 b            20</p>
</div></blockquote>
<p>aten::convolution      47.37 Mb           0 b            20</p>
</div></blockquote>
<p>aten::_convolution      47.37 Mb           0 b            20</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>aten::mkldnn_convolution      47.37 Mb           0 b            20</dt><dd><p>aten::max_pool2d      11.48 Mb           0 b             1</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1</p>
</dd>
</dl>
<p>———————————  ————  ————  ————
Self CPU time total: 53.064ms</p>
</section>
<section id="id5">
<h3>5. 추적기능 사용하기<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>프로파일링 결과는 <code class="docutils literal notranslate"><span class="pre">.json</span></code> 형태의 추적 파일(trace file)로 출력할 수 있습니다:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="k">with</span> <span class="n">profile</span><span class="p">(</span><span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">])</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">prof</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="s2">&quot;trace.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>사용자는 Chrome 브라우저( <code class="docutils literal notranslate"><span class="pre">chrome://tracing</span></code> )에서 추적 파일을 불러와
프로파일된 일련의 연산자들과 CUDA 커널을 검토해볼 수 있습니다:</p>
<a class="reference internal image-reference" href="../../_images/trace_img.png"><img alt="../../_images/trace_img.png" src="../../_images/trace_img.png" style="width: 754.0px; height: 124.5px;" /></a>
</section>
<section id="examining-stack-traces">
<h3>6. Examining stack traces<a class="headerlink" href="#examining-stack-traces" title="Link to this heading">#</a></h3>
<p>Profiler can be used to analyze Python and TorchScript stack traces:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">profile</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">],</span>
    <span class="n">with_stack</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># Print aggregated stats</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">(</span><span class="n">group_by_stack_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;self_cuda_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                aten::cudnn_convolution        18.64%     468.092us        40.02%       1.005ms      50.263us     439.095us        65.72%     493.717us      24.686us            20
                                 aten::cudnn_batch_norm        13.65%     342.745us        28.41%     713.615us      35.681us     134.335us        20.11%     134.335us       6.717us            20
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 2.512ms
Self CUDA time total: 668.110us
</pre></div>
</div>
<p>The output might look like this (omitting some columns):</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>-------------------------<span class="w">  </span>-----------------------------------------------------------
<span class="w">                     </span>Name<span class="w">  </span>Source<span class="w"> </span>Location
-------------------------<span class="w">  </span>-----------------------------------------------------------
aten::thnn_conv2d_forward<span class="w">  </span>.../torch/nn/modules/conv.py<span class="o">(</span><span class="m">439</span><span class="o">)</span>:<span class="w"> </span>_conv_forward
<span class="w">                           </span>.../torch/nn/modules/conv.py<span class="o">(</span><span class="m">443</span><span class="o">)</span>:<span class="w"> </span>forward
<span class="w">                           </span>.../torch/nn/modules/module.py<span class="o">(</span><span class="m">1051</span><span class="o">)</span>:<span class="w"> </span>_call_impl
<span class="w">                           </span>.../site-packages/torchvision/models/resnet.py<span class="o">(</span><span class="m">63</span><span class="o">)</span>:<span class="w"> </span>forward
<span class="w">                           </span>.../torch/nn/modules/module.py<span class="o">(</span><span class="m">1051</span><span class="o">)</span>:<span class="w"> </span>_call_impl
aten::thnn_conv2d_forward<span class="w">  </span>.../torch/nn/modules/conv.py<span class="o">(</span><span class="m">439</span><span class="o">)</span>:<span class="w"> </span>_conv_forward
<span class="w">                           </span>.../torch/nn/modules/conv.py<span class="o">(</span><span class="m">443</span><span class="o">)</span>:<span class="w"> </span>forward
<span class="w">                           </span>.../torch/nn/modules/module.py<span class="o">(</span><span class="m">1051</span><span class="o">)</span>:<span class="w"> </span>_call_impl
<span class="w">                           </span>.../site-packages/torchvision/models/resnet.py<span class="o">(</span><span class="m">59</span><span class="o">)</span>:<span class="w"> </span>forward
<span class="w">                           </span>.../torch/nn/modules/module.py<span class="o">(</span><span class="m">1051</span><span class="o">)</span>:<span class="w"> </span>_call_impl
-------------------------<span class="w">  </span>-----------------------------------------------------------
Self<span class="w"> </span>CPU<span class="w"> </span><span class="nb">time</span><span class="w"> </span>total:<span class="w"> </span><span class="m">34</span>.016ms
Self<span class="w"> </span>CUDA<span class="w"> </span><span class="nb">time</span><span class="w"> </span>total:<span class="w"> </span><span class="m">11</span>.659ms
</pre></div>
</div>
<p>Note the two convolutions and the two call sites in <code class="docutils literal notranslate"><span class="pre">torchvision/models/resnet.py</span></code> script.</p>
<p>(Warning: stack tracing adds an extra profiling overhead.)</p>
</section>
<section id="using-profiler-to-analyze-long-running-jobs">
<h3>7. Using profiler to analyze long-running jobs<a class="headerlink" href="#using-profiler-to-analyze-long-running-jobs" title="Link to this heading">#</a></h3>
<p>PyTorch profiler offers an additional API to handle long-running jobs
(such as training loops). Tracing all of the execution can be
slow and result in very large trace files. To avoid this, use optional
arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">schedule</span></code> - specifies a function that takes an integer argument (step number)
as an input and returns an action for the profiler, the best way to use this parameter
is to use <code class="docutils literal notranslate"><span class="pre">torch.profiler.schedule</span></code> helper function that can generate a schedule for you;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">on_trace_ready</span></code> - specifies a function that takes a reference to the profiler as
an input and is called by the profiler each time the new trace is ready.</p></li>
</ul>
<p>To illustrate how the API works, let’s first consider the following example with
<code class="docutils literal notranslate"><span class="pre">torch.profiler.schedule</span></code> helper function:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.profiler</span><span class="w"> </span><span class="kn">import</span> <span class="n">schedule</span>

<span class="n">my_schedule</span> <span class="o">=</span> <span class="n">schedule</span><span class="p">(</span>
    <span class="n">skip_first</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">wait</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">warmup</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">active</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">repeat</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Profiler assumes that the long-running job is composed of steps, numbered
starting from zero. The example above defines the following sequence of actions
for the profiler:</p>
<ol class="arabic simple">
<li><p>Parameter <code class="docutils literal notranslate"><span class="pre">skip_first</span></code> tells profiler that it should ignore the first 10 steps
(default value of <code class="docutils literal notranslate"><span class="pre">skip_first</span></code> is zero);</p></li>
<li><p>After the first <code class="docutils literal notranslate"><span class="pre">skip_first</span></code> steps, profiler starts executing profiler cycles;</p></li>
<li><p>Each cycle consists of three phases:</p>
<ul class="simple">
<li><p>idling (<code class="docutils literal notranslate"><span class="pre">wait=5</span></code> steps), during this phase profiler is not active;</p></li>
<li><p>warming up (<code class="docutils literal notranslate"><span class="pre">warmup=1</span></code> steps), during this phase profiler starts tracing, but
the results are discarded; this phase is used to discard the samples obtained by
the profiler at the beginning of the trace since they are usually skewed by an extra
overhead;</p></li>
<li><p>active tracing (<code class="docutils literal notranslate"><span class="pre">active=3</span></code> steps), during this phase profiler traces and records data;</p></li>
</ul>
</li>
<li><p>An optional <code class="docutils literal notranslate"><span class="pre">repeat</span></code> parameter specifies an upper bound on the number of cycles.
By default (zero value), profiler will execute cycles as long as the job runs.</p></li>
</ol>
<p>Thus, in the example above, profiler will skip the first 15 steps, spend the next step on the warm up,
actively record the next 3 steps, skip another 5 steps, spend the next step on the warm up, actively
record another 3 steps. Since the <code class="docutils literal notranslate"><span class="pre">repeat=2</span></code> parameter value is specified, the profiler will stop
the recording after the first two cycles.</p>
<p>At the end of each cycle profiler calls the specified <code class="docutils literal notranslate"><span class="pre">on_trace_ready</span></code> function and passes itself as
an argument. This function is used to process the new trace - either by obtaining the table output or
by saving the output on disk as a trace file.</p>
<p>To send the signal to the profiler that the next step has started, call <code class="docutils literal notranslate"><span class="pre">prof.step()</span></code> function.
The current profiler step is stored in <code class="docutils literal notranslate"><span class="pre">prof.step_num</span></code>.</p>
<p>The following example shows how to use all of the concepts above:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">trace_handler</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;self_cuda_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">export_chrome_trace</span><span class="p">(</span><span class="s2">&quot;/tmp/trace_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">step_num</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">profile</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">],</span>
    <span class="n">schedule</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">schedule</span><span class="p">(</span>
        <span class="n">wait</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">warmup</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">active</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">on_trace_ready</span><span class="o">=</span><span class="n">trace_handler</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">p</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
        <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">p</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us       5.280ms       395.25%       5.280ms       2.640ms             2
                                aten::cudnn_convolution        13.41%     755.975us        24.93%       1.405ms      35.128us     878.128us        65.74%     878.128us      21.953us            40
                                 aten::cudnn_batch_norm        10.72%     604.499us        22.93%       1.293ms      32.316us     269.951us        20.21%     269.951us       6.749us            40
void cudnn::bn_fw_tr_1C11_singleread&lt;float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us     212.735us        15.93%     212.735us       5.598us            38
void cudnn::engines_precompiled::nchwToNhwcKernel&lt;fl...         0.00%       0.000us         0.00%       0.000us       0.000us     194.492us        14.56%     194.492us       4.862us            40
sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     135.548us        10.15%     135.548us      16.944us             8
sm90_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhw...         0.00%       0.000us         0.00%       0.000us       0.000us     115.100us         8.62%     115.100us      11.510us            10
sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     112.512us         8.42%     112.512us      11.251us            10
void cudnn::engines_precompiled::nchwToNhwcKernel&lt;fl...         0.00%       0.000us         0.00%       0.000us       0.000us      88.960us         6.66%      88.960us       2.780us            32
                                             aten::add_         4.31%     242.683us         7.96%     448.469us       8.008us      73.633us         5.51%      73.633us       1.315us            56
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 5.637ms
Self CUDA time total: 1.336ms

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us       5.249ms       392.52%       5.249ms       2.624ms             2
                                aten::cudnn_convolution        13.66%     765.311us        25.29%       1.417ms      35.434us     880.464us        65.85%     880.464us      22.012us            40
                                 aten::cudnn_batch_norm        10.65%     597.080us        22.96%       1.287ms      32.170us     269.596us        20.16%     269.596us       6.740us            40
void cudnn::bn_fw_tr_1C11_singleread&lt;float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us     212.060us        15.86%     212.060us       5.581us            38
void cudnn::engines_precompiled::nchwToNhwcKernel&lt;fl...         0.00%       0.000us         0.00%       0.000us       0.000us     195.386us        14.61%     195.386us       4.885us            40
sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     136.638us        10.22%     136.638us      17.080us             8
sm90_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhw...         0.00%       0.000us         0.00%       0.000us       0.000us     114.590us         8.57%     114.590us      11.459us            10
sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     112.766us         8.43%     112.766us      11.277us            10
void cudnn::engines_precompiled::nchwToNhwcKernel&lt;fl...         0.00%       0.000us         0.00%       0.000us       0.000us      88.672us         6.63%      88.672us       2.771us            32
                                             aten::add_         4.35%     243.530us         8.14%     456.116us       8.145us      73.252us         5.48%      73.252us       1.308us            56
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 5.604ms
Self CUDA time total: 1.337ms
</pre></div>
</div>
</section>
</section>
<section id="id6">
<h2>더 알아보기<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<p>다음 레시피와 튜토리얼을 읽으며 학습을 계속해보세요:</p>
<ul class="simple">
<li><p><a class="reference internal" href="benchmark.html"><span class="doc">PyTorch Benchmark</span></a></p></li>
<li><p><a class="reference internal" href="../../intermediate/tensorboard_profiler_tutorial.html"><span class="doc">텐서보드를 이용한 파이토치 프로파일러</span></a> 튜토리얼</p></li>
<li><p><a class="reference internal" href="../../intermediate/tensorboard_tutorial.html"><span class="doc">TensorBoard로 모델, 데이터, 학습 시각화하기</span></a> 튜토리얼</p></li>
</ul>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 3.909 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-recipes-recipes-profiler-recipe-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/b2c9c15033f17c2bdf31c864f9d39c76/profiler_recipe.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">profiler_recipe.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/74b90fda9cec339519e5ef764956100f/profiler_recipe.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">profiler_recipe.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/5aea67db4405cdadaef190d8442a58a1/profiler_recipe.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">profiler_recipe.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="zeroing_out_gradients.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">PyTorch에서 변화도를 0으로 만들기</p>
      </div>
    </a>
    <a class="right-next"
       href="Captum_Recipe.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Captum을 사용하여 모델 해석하기</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="zeroing_out_gradients.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">PyTorch에서 변화도를 0으로 만들기</p>
      </div>
    </a>
    <a class="right-next"
       href="Captum_Recipe.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Captum을 사용하여 모델 해석하기</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">개요</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">설정(Setup)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">단계(Steps)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1. 필요한 라이브러리들 불러오기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet">2. 간단한 ResNet 모델 인스턴스화 하기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">3. 프로파일러를 사용하여 실행시간 분석하기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">4. 프로파일러를 사용하여 메모리 소비 분석하기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">5. 추적기능 사용하기</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-stack-traces">6. Examining stack traces</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-profiler-to-analyze-long-running-jobs">7. Using profiler to analyze long-running jobs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">더 알아보기</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "PyTorch \ud504\ub85c\ud30c\uc77c\ub7ec(Profiler)",
       "headline": "PyTorch \ud504\ub85c\ud30c\uc77c\ub7ec(Profiler)",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/recipes/recipes/profiler_recipe.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. PyTorch \ud504\ub85c\ud30c\uc77c\ub7ec(Profiler)# \uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 \uc5b4\ub5bb\uac8c PyTorch \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \ubaa8\ub378\uc758 \uc5f0\uc0b0\uc790\ub4e4\uc774 \uc18c\ube44\ud558\ub294 \uba54\ubaa8\ub9ac\uc640 \uc2dc\uac04\uc744 \uce21\uc815\ud558\ub294 \ubc29\ubc95\uc744 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uac1c\uc694# PyTorch\ub294 \uc0ac\uc6a9\uc790\uac00 \ubaa8\ub378 \ub0b4\uc758 \uc5f0\uc0b0 \ube44\uc6a9\uc774 \ud070(expensive) \uc5f0\uc0b0\uc790\ub4e4\uc774 \ubb34\uc5c7\uc778\uc9c0 \uc54c\uace0\uc2f6\uc744 \ub54c \uc720\uc6a9\ud558\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uac04\ub2e8\ud55c \ud504\ub85c\ud30c\uc77c\ub7ec API\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 \ubaa8\ub378\uc758 \uc131\ub2a5(performance)\uc744 \ubd84\uc11d\ud558\ub824\uace0 \ud560 \ub54c \uc5b4\ub5bb\uac8c \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud574\uc57c \ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud574 \uac04\ub2e8\ud55c ResNet \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc124\uc815(Setup)# torch \uc640 torchvision \uc744 \uc124\uce58\ud558\uae30 \uc704\ud574\uc11c \uc544\ub798\uc758 \ucee4\ub9e8\ub4dc\ub97c \uc785\ub825\ud569\ub2c8\ub2e4: pip install torch torchvision \ub2e8\uacc4(Steps)# \ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4 \ubd88\ub7ec\uc624\uae30 \uac04\ub2e8\ud55c ResNet \ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654 \ud558\uae30 \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud589\uc2dc\uac04 \ubd84\uc11d\ud558\uae30 \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uba54\ubaa8\ub9ac \uc18c\ube44 \ubd84\uc11d\ud558\uae30 \ucd94\uc801\uae30\ub2a5 \uc0ac\uc6a9\ud558\uae30 Examining stack traces Using profiler to analyze long-running jobs 1. \ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4 \ubd88\ub7ec\uc624\uae30# \uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 torch \uc640 torchvision.models, \uadf8\ub9ac\uace0 profiler \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4: import torch import torchvision.models as models from torch.profiler import profile, record_function, ProfilerActivity 2. \uac04\ub2e8\ud55c ResNet \ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654 \ud558\uae30# ResNet \ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ub97c \ub9cc\ub4e4\uace0 \uc785\ub825\uac12\uc744 \uc900\ube44\ud569\ub2c8\ub2e4 : model = models.resnet18() inputs = torch.randn(5, 3, 224, 224) 3. \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud589\uc2dc\uac04 \ubd84\uc11d\ud558\uae30# PyTorch \ud504\ub85c\ud30c\uc77c\ub7ec\ub294 \ucee8\ud14d\uc2a4\ud2b8 \uba54\ub2c8\uc800(context manager)\ub97c \ud1b5\ud574 \ud65c\uc131\ud654\ub418\uace0, \uc5ec\ub7ec \ub9e4\uac1c\ubcc0\uc218\ub97c \ubc1b\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc720\uc6a9\ud55c \uba87 \uac00\uc9c0 \ub9e4\uac1c\ubcc0\uc218\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4: activities - a list of activities to profile: ProfilerActivity.CPU - PyTorch operators, TorchScript functions and user-defined code labels (see record_function below); ProfilerActivity.CUDA - on-device CUDA kernels; record_shapes - \uc5f0\uc0ac\uc790 \uc785\ub825(input)\uc758 shape\uc744 \uae30\ub85d\ud560\uc9c0 \uc5ec\ubd80; profile_memory - \ubaa8\ub378\uc758 \ud150\uc11c(Tensor)\ub4e4\uc774 \uc18c\ube44\ud558\ub294 \uba54\ubaa8\ub9ac \uc591\uc744 \ubcf4\uace0(report)\ud560\uc9c0 \uc5ec\ubd80; use_cuda - CUDA \ucee4\ub110\uc758 \uc2e4\ud589\uc2dc\uac04\uc744 \uce21\uc815\ud560\uc9c0 \uc5ec\ubd80; Note: when using CUDA, profiler also shows the runtime CUDA events occuring on the host. \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5b4\ub5bb\uac8c \uc2e4\ud589\uc2dc\uac04\uc744 \ubd84\uc11d\ud558\ub294\uc9c0 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4: with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof: with record_function(\"model_inference\"): model(inputs) record_function \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc784\uc758\uc758 \ucf54\ub4dc \ubc94\uc704\uc5d0 \uc0ac\uc6a9\uc790\uac00 \uc9c0\uc815\ud55c \uc774\ub984\uc73c\ub85c \ub808\uc774\ube14(label)\uc744 \ud45c\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uc704 \uc608\uc81c\uc5d0\uc11c\ub294 model_inference \ub97c \ub808\uc774\ube14\ub85c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.) \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uba74 \ud504\ub85c\ud30c\uc77c\ub7ec \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub85c \uac10\uc2f8\uc9c4(wrap) \ucf54\ub4dc \ubc94\uc704\ub97c \uc2e4\ud589\ud558\ub294 \ub3d9\uc548 \uc5b4\ub5a4 \uc5f0\uc0b0\uc790\ub4e4\uc774 \ud638\ucd9c\ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9cc\uc57d \uc5ec\ub7ec \ud504\ub85c\ud30c\uc77c\ub7ec\uc758 \ubc94\uc704\uac00 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \uacbd\uc6b0(\uc608. PyTorch \uc4f0\ub808\ub4dc\uac00 \ubcd1\ub82c\ub85c \uc2e4\ud589 \uc911\uc778 \uacbd\uc6b0), \uac01 \ud504\ub85c\ud30c\uc77c\ub9c1 \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub294 \uac01\uac01\uc758 \ubc94\uc704 \ub0b4\uc758 \uc5f0\uc0b0\uc790\ub4e4\ub9cc \ucd94\uc801(track)\ud569\ub2c8\ub2e4. \ud504\ub85c\ud30c\uc77c\ub7ec\ub294 \ub610\ud55c torch.jit._fork \ub85c \uc2e4\ud589\ub41c \ube44\ub3d9\uae30 \uc791\uc5c5\uacfc (\uc5ed\uc804\ud30c \ub2e8\uacc4\uc758 \uacbd\uc6b0) backward() \uc758 \ud638\ucd9c\ub85c \uc2e4\ud589\ub41c \uc5ed\uc804\ud30c \uc5f0\uc0b0\uc790\ub4e4\ub3c4 \uc790\ub3d9\uc73c\ub85c \ud504\ub85c\ud30c\uc77c\ub9c1\ud569\ub2c8\ub2e4. \uc704 \ucf54\ub4dc\ub97c \uc2e4\ud589\ud55c \ud1b5\uacc4\ub97c \ucd9c\ub825\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4: print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10)) --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg # of Calls --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ model_inference 7.18% 3.482ms 100.00% 48.497ms 48.497ms 1 aten::conv2d 0.18% 85.560us 77.19% 37.435ms 1.872ms 20 aten::convolution 0.46% 222.623us 77.01% 37.349ms 1.867ms 20 aten::_convolution 0.32% 156.516us 76.55% 37.127ms 1.856ms 20 aten::mkldnn_convolution 75.92% 36.819ms 76.23% 36.970ms 1.849ms 20 aten::batch_norm 0.12% 56.538us 9.91% 4.805ms 240.234us 20 aten::_batch_norm_impl_index 0.23% 110.516us 9.79% 4.748ms 237.407us 20 aten::native_batch_norm 9.27% 4.494ms 9.54% 4.628ms 231.384us 20 aten::relu_ 0.39% 187.916us 1.80% 874.065us 51.416us 17 aten::max_pool2d 0.05% 26.233us 1.43% 693.791us 693.791us 1 --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 48.497ms (\uba87\uba87 \uc5f4\uc744 \uc81c\uc678\ud558\uace0) \ucd9c\ub825\uac12\uc774 \uc774\ub807\uac8c \ubcf4\uc77c \uac83\uc785\ub2c8\ub2e4: \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014Name Self CPU CPU total CPU time avg # of Calls \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 model_inference 5.509ms 57.503ms 57.503ms 1aten::conv2d 231.000us 31.931ms 1.597ms 20 aten::convolution 250.000us 31.700ms 1.585ms 20 aten::_convolution 336.000us 31.450ms 1.573ms 20 aten::mkldnn_convolution 30.838ms 31.114ms 1.556ms 20aten::batch_norm 211.000us 14.693ms 734.650us 20 aten::_batch_norm_impl_index 319.000us 14.482ms 724.100us 20 aten::native_batch_norm 9.229ms 14.109ms 705.450us 20 aten::mean 332.000us 2.631ms 125.286us 21 aten::select 1.668ms 2.292ms 8.988us 255 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 Self CPU time total: 57.549ms \uc608\uc0c1\ud588\ub358 \ub300\ub85c, \ub300\ubd80\ubd84\uc758 \uc2dc\uac04\uc774 \ud569\uc131\uacf1(convolution) \uc5f0\uc0b0(\ud2b9\ud788 MKL-DNN \uc744 \uc9c0\uc6d0\ud558\ub3c4\ub85d \ucef4\ud30c\uc77c\ub41c PyTorch\uc758 \uacbd\uc6b0\uc5d0\ub294 mkldnn_convolution )\uc5d0\uc11c \uc18c\uc694\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. (\uacb0\uacfc \uc5f4\ub4e4 \uc911) Self CPU time\uacfc CPU time\uc758 \ucc28\uc774\uc5d0 \uc720\uc758\ud574\uc57c \ud569\ub2c8\ub2e4 - \uc5f0\uc0b0\uc790\ub294 \ub2e4\ub978 \uc5f0\uc0b0\uc790\ub4e4\uc744 \ud638\ucd9c\ud560 \uc218 \uc788\uc73c\uba70, Self CPU time\uc5d0\ub294 \ud558\uc704(child) \uc5f0\uc0b0\uc790 \ud638\ucd9c\uc5d0\uc11c \ubc1c\uc0dd\ud55c \uc2dc\uac04\uc744 \uc81c\uc678\ud574\uc11c, Totacl CPU time\uc5d0\ub294 \ud3ec\ud568\ud574\uc11c \ud45c\uc2dc\ud569\ub2c8\ub2e4. You can choose to sort by the self cpu time by passing sort_by=\"self_cpu_time_total\" into the table call. \ubcf4\ub2e4 \uc138\ubd80\uc801\uc778 \uacb0\uacfc \uc815\ubcf4 \ubc0f \uc5f0\uc0b0\uc790\uc758 \uc785\ub825 shape\uc744 \ud568\uaed8 \ubcf4\ub824\uba74 group_by_input_shape=True \ub97c \uc778\uc790\ub85c \uc804\ub2ec\ud558\uba74 \ub429\ub2c8\ub2e4 (note: this requires running the profiler with record_shapes=True): print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10)) --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ -------------------------------------------------------------------------------- Name Self CPU % Self CPU CPU total % CPU total CPU time avg # of Calls Input Shapes --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ -------------------------------------------------------------------------------- model_inference 7.18% 3.482ms 100.00% 48.497ms 48.497ms 1 [] aten::conv2d 0.06% 28.275us 27.06% 13.125ms 13.125ms 1 [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []] aten::convolution 0.15% 73.125us 27.00% 13.097ms 13.097ms 1 [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], []] aten::_convolution 0.08% 39.220us 26.85% 13.024ms 13.024ms 1 [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], [], [], [], [], []] aten::mkldnn_convolution 26.70% 12.946ms 26.77% 12.984ms 12.984ms 1 [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []] aten::conv2d 0.03% 14.640us 19.12% 9.274ms 2.318ms 4 [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []] aten::convolution 0.08% 37.078us 19.09% 9.259ms 2.315ms 4 [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []] aten::_convolution 0.07% 32.106us 19.02% 9.222ms 2.306ms 4 [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []] aten::mkldnn_convolution 18.90% 9.165ms 18.95% 9.190ms 2.297ms 4 [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []] aten::conv2d 0.02% 9.454us 5.14% 2.491ms 830.217us 3 [[5, 128, 28, 28], [128, 128, 3, 3], [], [], [], [], []] --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ -------------------------------------------------------------------------------- Self CPU time total: 48.497ms (\uba87\uba87 \uc5f4\uc744 \uc81c\uc678\ud558\uace0) \ucd9c\ub825\uac12\uc774 \uc774\ub807\uac8c \ubcf4\uc77c \uac83\uc785\ub2c8\ub2e4: --------------------------------- ------------ ------------------------------------------- Name CPU total Input Shapes --------------------------------- ------------ ------------------------------------------- model_inference 57.503ms [] aten::conv2d 8.008ms [5,64,56,56], [64,64,3,3], [], ..., []] aten::convolution 7.956ms [[5,64,56,56], [64,64,3,3], [], ..., []] aten::_convolution 7.909ms [[5,64,56,56], [64,64,3,3], [], ..., []] aten::mkldnn_convolution 7.834ms [[5,64,56,56], [64,64,3,3], [], ..., []] aten::conv2d 6.332ms [[5,512,7,7], [512,512,3,3], [], ..., []] aten::convolution 6.303ms [[5,512,7,7], [512,512,3,3], [], ..., []] aten::_convolution 6.273ms [[5,512,7,7], [512,512,3,3], [], ..., []] aten::mkldnn_convolution 6.233ms [[5,512,7,7], [512,512,3,3], [], ..., []] aten::conv2d 4.751ms [[5,256,14,14], [256,256,3,3], [], ..., []] --------------------------------- ------------ ------------------------------------------- Self CPU time total: 57.549ms Note the occurence of aten::convolution twice with different input shapes. Profiler can also be used to analyze performance of models executed on GPUs: model = models.resnet18().cuda() inputs = torch.randn(5, 3, 224, 224).cuda() with profile(activities=[ ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof: with record_function(\"model_inference\"): model(inputs) print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10)) ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ model_inference 0.00% 0.000us 0.00% 0.000us 0.000us 175.291ms 26096.35% 175.291ms 87.645ms 2 model_inference 0.61% 1.807ms 100.00% 297.875ms 297.875ms 0.000us 0.00% 2.261ms 2.261ms 1 aten::conv2d 0.02% 62.357us 51.66% 153.899ms 7.695ms 0.000us 0.00% 1.832ms 91.605us 20 aten::convolution 0.06% 183.470us 51.64% 153.837ms 7.692ms 0.000us 0.00% 1.832ms 91.605us 20 aten::_convolution 0.05% 157.311us 51.58% 153.654ms 7.683ms 0.000us 0.00% 1.832ms 91.605us 20 aten::cudnn_convolution 38.49% 114.645ms 51.53% 153.496ms 7.675ms 441.244us 65.69% 1.832ms 91.605us 20 Runtime Triggered Module Loading 22.26% 66.323ms 22.26% 66.323ms 1.745ms 972.922us 144.84% 972.922us 25.603us 38 cudaFuncSetAttribute 4.97% 14.817ms 5.17% 15.401ms 229.863us 0.000us 0.00% 671.456us 10.022us 67 Lazy Function Loading 0.30% 886.844us 0.30% 886.844us 36.952us 560.986us 83.52% 560.986us 23.374us 24 cudaLaunchKernel 1.70% 5.064ms 19.49% 58.061ms 509.304us 0.000us 0.00% 358.879us 3.148us 114 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 297.884ms Self CUDA time total: 671.707us (Note: the first use of CUDA profiling may bring an extra overhead.) The resulting table output: ------------------------------------------------------- ------------ ------------ Name Self CUDA CUDA total ------------------------------------------------------- ------------ ------------ model_inference 0.000us 11.666ms aten::conv2d 0.000us 10.484ms aten::convolution 0.000us 10.484ms aten::_convolution 0.000us 10.484ms aten::_convolution_nogroup 0.000us 10.484ms aten::thnn_conv2d 0.000us 10.484ms aten::thnn_conv2d_forward 10.484ms 10.484ms void at::native::im2col_kernel\u003cfloat\u003e(long, float co... 3.844ms 3.844ms sgemm_32x32x32_NN 3.206ms 3.206ms sgemm_32x32x32_NN_vec 3.093ms 3.093ms ------------------------------------------------------- ------------ ------------ Self CPU time total: 23.015ms Self CUDA time total: 11.666ms Note the occurence of on-device kernels in the output (e.g. sgemm_32x32x32_NN). 4. \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uba54\ubaa8\ub9ac \uc18c\ube44 \ubd84\uc11d\ud558\uae30# PyTorch \ud504\ub85c\ud30c\uc77c\ub7ec\ub294 \ubaa8\ub378\uc758 \uc5f0\uc0b0\uc790\ub4e4\uc744 \uc2e4\ud589\ud558\uba70 (\ubaa8\ub378\uc758 \ud150\uc11c\ub4e4\uc774 \uc0ac\uc6a9\ud558\uba70) \ud560\ub2f9(\ub610\ub294 \ud574\uc81c)\ud55c \uba54\ubaa8\ub9ac\uc758 \uc591\ub3c4 \ud45c\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798 \ucd9c\ub825 \uacb0\uacfc\uc5d0\uc11c \u2018Self\u2019 memory\ub294 \ud574\ub2f9 \uc5f0\uc0b0\uc790\uc5d0 \uc758\ud574 \ud638\ucd9c\ub41c \ud558\uc704(child) \uc5f0\uc0b0\uc790\ub4e4\uc744 \uc81c\uc678\ud55c, \uc5f0\uc0b0\uc790 \uc790\uccb4\uc5d0 \ud560\ub2f9(\ud574\uc81c)\ub41c \uba54\ubaa8\ub9ac\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4. \uba54\ubaa8\ub9ac \ud504\ub85c\ud30c\uc77c\ub9c1 \uae30\ub2a5\uc744 \ud65c\uc131\ud654\ud558\ub824\uba74 profile_memory=True \ub97c \uc778\uc790\ub85c \uc804\ub2ec\ud558\uba74 \ub429\ub2c8\ub2e4. model = models.resnet18() inputs = torch.randn(5, 3, 224, 224) with profile(activities=[ProfilerActivity.CPU], profile_memory=True, record_shapes=True) as prof: model(inputs) print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10)) --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg CPU Mem Self CPU Mem # of Calls --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ aten::empty 0.72% 197.288us 0.72% 197.288us 0.986us 94.85 MB 94.85 MB 200 aten::max_pool2d_with_indices 2.12% 582.848us 2.12% 582.848us 582.848us 11.48 MB 11.48 MB 1 aten::addmm 0.25% 69.163us 0.28% 78.173us 78.173us 19.53 KB 19.53 KB 1 aten::mean 0.06% 15.605us 0.41% 112.912us 112.912us 10.00 KB 9.99 KB 1 aten::div_ 0.06% 15.916us 0.12% 32.224us 32.224us 8 B 4 B 1 aten::empty_strided 0.01% 3.461us 0.01% 3.461us 3.461us 4 B 4 B 1 aten::conv2d 0.19% 52.158us 80.22% 22.017ms 1.101ms 47.37 MB 0 B 20 aten::convolution 0.42% 114.002us 80.03% 21.965ms 1.098ms 47.37 MB 0 B 20 aten::_convolution 0.29% 80.198us 79.62% 21.851ms 1.093ms 47.37 MB 0 B 20 aten::mkldnn_convolution 78.93% 21.663ms 79.32% 21.770ms 1.089ms 47.37 MB 0 B 20 --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 27.445ms (\uba87\uba87 \uc5f4\uc740 \uc81c\uc678\ud558\uc600\uc2b5\ub2c8\ub2e4) \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014Name CPU Mem Self CPU Mem # of Calls \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 aten::empty 94.79 Mb 94.79 Mb 121 aten::max_pool2d_with_indices 11.48 Mb 11.48 Mb 1 aten::addmm 19.53 Kb 19.53 Kb 1 aten::empty_strided 572 b 572 b 25 aten::resize_ 240 b 240 b 6aten::abs 480 b 240 b 4 aten::add 160 b 160 b 20 aten::masked_select 120 b 112 b 1aten::ne 122 b 53 b 6 aten::eq 60 b 30 b 2 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 Self CPU time total: 53.064ms print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10)) --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg CPU Mem Self CPU Mem # of Calls --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ aten::empty 0.72% 197.288us 0.72% 197.288us 0.986us 94.85 MB 94.85 MB 200 aten::batch_norm 0.16% 43.403us 12.38% 3.396ms 169.818us 47.41 MB 0 B 20 aten::_batch_norm_impl_index 0.23% 62.749us 12.22% 3.353ms 167.647us 47.41 MB 0 B 20 aten::native_batch_norm 11.42% 3.133ms 11.95% 3.281ms 164.045us 47.41 MB -70.50 KB 20 aten::conv2d 0.19% 52.158us 80.22% 22.017ms 1.101ms 47.37 MB 0 B 20 aten::convolution 0.42% 114.002us 80.03% 21.965ms 1.098ms 47.37 MB 0 B 20 aten::_convolution 0.29% 80.198us 79.62% 21.851ms 1.093ms 47.37 MB 0 B 20 aten::mkldnn_convolution 78.93% 21.663ms 79.32% 21.770ms 1.089ms 47.37 MB 0 B 20 aten::empty_like 0.10% 28.213us 0.18% 50.287us 2.514us 47.37 MB 0 B 20 aten::max_pool2d 0.02% 4.325us 2.14% 587.173us 587.173us 11.48 MB 0 B 1 --------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 27.445ms (\uba87\uba87 \uc5f4\uc744 \uc81c\uc678\ud558\uace0) \ucd9c\ub825\uac12\uc774 \uc774\ub807\uac8c \ubcf4\uc77c \uac83\uc785\ub2c8\ub2e4: \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014Name CPU Mem Self CPU Mem # of Calls \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 aten::empty 94.79 Mb 94.79 Mb 121 aten::batch_norm 47.41 Mb 0 b 20 aten::_batch_norm_impl_index 47.41 Mb 0 b 20 aten::native_batch_norm 47.41 Mb 0 b 20 aten::conv2d 47.37 Mb 0 b 20 aten::convolution 47.37 Mb 0 b 20 aten::_convolution 47.37 Mb 0 b 20 aten::mkldnn_convolution 47.37 Mb 0 b 20aten::max_pool2d 11.48 Mb 0 b 1 aten::max_pool2d_with_indices 11.48 Mb 11.48 Mb 1 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 \u2014\u2014\u2014\u2014 Self CPU time total: 53.064ms 5. \ucd94\uc801\uae30\ub2a5 \uc0ac\uc6a9\ud558\uae30# \ud504\ub85c\ud30c\uc77c\ub9c1 \uacb0\uacfc\ub294 .json \ud615\ud0dc\uc758 \ucd94\uc801 \ud30c\uc77c(trace file)\ub85c \ucd9c\ub825\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4: model = models.resnet18().cuda() inputs = torch.randn(5, 3, 224, 224).cuda() with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof: model(inputs) prof.export_chrome_trace(\"trace.json\") \uc0ac\uc6a9\uc790\ub294 Chrome \ube0c\ub77c\uc6b0\uc800( chrome://tracing )\uc5d0\uc11c \ucd94\uc801 \ud30c\uc77c\uc744 \ubd88\ub7ec\uc640 \ud504\ub85c\ud30c\uc77c\ub41c \uc77c\ub828\uc758 \uc5f0\uc0b0\uc790\ub4e4\uacfc CUDA \ucee4\ub110\uc744 \uac80\ud1a0\ud574\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4: 6. Examining stack traces# Profiler can be used to analyze Python and TorchScript stack traces: with profile( activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_stack=True, ) as prof: model(inputs) # Print aggregated stats print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2)) ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ aten::cudnn_convolution 18.64% 468.092us 40.02% 1.005ms 50.263us 439.095us 65.72% 493.717us 24.686us 20 aten::cudnn_batch_norm 13.65% 342.745us 28.41% 713.615us 35.681us 134.335us 20.11% 134.335us 6.717us 20 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 2.512ms Self CUDA time total: 668.110us The output might look like this (omitting some columns): ------------------------- ----------------------------------------------------------- Name Source Location ------------------------- ----------------------------------------------------------- aten::thnn_conv2d_forward .../torch/nn/modules/conv.py(439): _conv_forward .../torch/nn/modules/conv.py(443): forward .../torch/nn/modules/module.py(1051): _call_impl .../site-packages/torchvision/models/resnet.py(63): forward .../torch/nn/modules/module.py(1051): _call_impl aten::thnn_conv2d_forward .../torch/nn/modules/conv.py(439): _conv_forward .../torch/nn/modules/conv.py(443): forward .../torch/nn/modules/module.py(1051): _call_impl .../site-packages/torchvision/models/resnet.py(59): forward .../torch/nn/modules/module.py(1051): _call_impl ------------------------- ----------------------------------------------------------- Self CPU time total: 34.016ms Self CUDA time total: 11.659ms Note the two convolutions and the two call sites in torchvision/models/resnet.py script. (Warning: stack tracing adds an extra profiling overhead.) 7. Using profiler to analyze long-running jobs# PyTorch profiler offers an additional API to handle long-running jobs (such as training loops). Tracing all of the execution can be slow and result in very large trace files. To avoid this, use optional arguments: schedule - specifies a function that takes an integer argument (step number) as an input and returns an action for the profiler, the best way to use this parameter is to use torch.profiler.schedule helper function that can generate a schedule for you; on_trace_ready - specifies a function that takes a reference to the profiler as an input and is called by the profiler each time the new trace is ready. To illustrate how the API works, let\u2019s first consider the following example with torch.profiler.schedule helper function: from torch.profiler import schedule my_schedule = schedule( skip_first=10, wait=5, warmup=1, active=3, repeat=2) Profiler assumes that the long-running job is composed of steps, numbered starting from zero. The example above defines the following sequence of actions for the profiler: Parameter skip_first tells profiler that it should ignore the first 10 steps (default value of skip_first is zero); After the first skip_first steps, profiler starts executing profiler cycles; Each cycle consists of three phases: idling (wait=5 steps), during this phase profiler is not active; warming up (warmup=1 steps), during this phase profiler starts tracing, but the results are discarded; this phase is used to discard the samples obtained by the profiler at the beginning of the trace since they are usually skewed by an extra overhead; active tracing (active=3 steps), during this phase profiler traces and records data; An optional repeat parameter specifies an upper bound on the number of cycles. By default (zero value), profiler will execute cycles as long as the job runs. Thus, in the example above, profiler will skip the first 15 steps, spend the next step on the warm up, actively record the next 3 steps, skip another 5 steps, spend the next step on the warm up, actively record another 3 steps. Since the repeat=2 parameter value is specified, the profiler will stop the recording after the first two cycles. At the end of each cycle profiler calls the specified on_trace_ready function and passes itself as an argument. This function is used to process the new trace - either by obtaining the table output or by saving the output on disk as a trace file. To send the signal to the profiler that the next step has started, call prof.step() function. The current profiler step is stored in prof.step_num. The following example shows how to use all of the concepts above: def trace_handler(p): output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10) print(output) p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\") with profile( activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], schedule=torch.profiler.schedule( wait=1, warmup=1, active=2), on_trace_ready=trace_handler ) as p: for idx in range(8): model(inputs) p.step() ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ProfilerStep* 0.00% 0.000us 0.00% 0.000us 0.000us 5.280ms 395.25% 5.280ms 2.640ms 2 aten::cudnn_convolution 13.41% 755.975us 24.93% 1.405ms 35.128us 878.128us 65.74% 878.128us 21.953us 40 aten::cudnn_batch_norm 10.72% 604.499us 22.93% 1.293ms 32.316us 269.951us 20.21% 269.951us 6.749us 40 void cudnn::bn_fw_tr_1C11_singleread\u003cfloat, 512, tru... 0.00% 0.000us 0.00% 0.000us 0.000us 212.735us 15.93% 212.735us 5.598us 38 void cudnn::engines_precompiled::nchwToNhwcKernel\u003cfl... 0.00% 0.000us 0.00% 0.000us 0.000us 194.492us 14.56% 194.492us 4.862us 40 sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh... 0.00% 0.000us 0.00% 0.000us 0.000us 135.548us 10.15% 135.548us 16.944us 8 sm90_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhw... 0.00% 0.000us 0.00% 0.000us 0.000us 115.100us 8.62% 115.100us 11.510us 10 sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh... 0.00% 0.000us 0.00% 0.000us 0.000us 112.512us 8.42% 112.512us 11.251us 10 void cudnn::engines_precompiled::nchwToNhwcKernel\u003cfl... 0.00% 0.000us 0.00% 0.000us 0.000us 88.960us 6.66% 88.960us 2.780us 32 aten::add_ 4.31% 242.683us 7.96% 448.469us 8.008us 73.633us 5.51% 73.633us 1.315us 56 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 5.637ms Self CUDA time total: 1.336ms ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ProfilerStep* 0.00% 0.000us 0.00% 0.000us 0.000us 5.249ms 392.52% 5.249ms 2.624ms 2 aten::cudnn_convolution 13.66% 765.311us 25.29% 1.417ms 35.434us 880.464us 65.85% 880.464us 22.012us 40 aten::cudnn_batch_norm 10.65% 597.080us 22.96% 1.287ms 32.170us 269.596us 20.16% 269.596us 6.740us 40 void cudnn::bn_fw_tr_1C11_singleread\u003cfloat, 512, tru... 0.00% 0.000us 0.00% 0.000us 0.000us 212.060us 15.86% 212.060us 5.581us 38 void cudnn::engines_precompiled::nchwToNhwcKernel\u003cfl... 0.00% 0.000us 0.00% 0.000us 0.000us 195.386us 14.61% 195.386us 4.885us 40 sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh... 0.00% 0.000us 0.00% 0.000us 0.000us 136.638us 10.22% 136.638us 17.080us 8 sm90_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhw... 0.00% 0.000us 0.00% 0.000us 0.000us 114.590us 8.57% 114.590us 11.459us 10 sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh... 0.00% 0.000us 0.00% 0.000us 0.000us 112.766us 8.43% 112.766us 11.277us 10 void cudnn::engines_precompiled::nchwToNhwcKernel\u003cfl... 0.00% 0.000us 0.00% 0.000us 0.000us 88.672us 6.63% 88.672us 2.771us 32 aten::add_ 4.35% 243.530us 8.14% 456.116us 8.145us 73.252us 5.48% 73.252us 1.308us 56 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 5.604ms Self CUDA time total: 1.337ms \ub354 \uc54c\uc544\ubcf4\uae30# \ub2e4\uc74c \ub808\uc2dc\ud53c\uc640 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc77d\uc73c\uba70 \ud559\uc2b5\uc744 \uacc4\uc18d\ud574\ubcf4\uc138\uc694: PyTorch Benchmark \ud150\uc11c\ubcf4\ub4dc\ub97c \uc774\uc6a9\ud55c \ud30c\uc774\ud1a0\uce58 \ud504\ub85c\ud30c\uc77c\ub7ec \ud29c\ud1a0\ub9ac\uc5bc TensorBoard\ub85c \ubaa8\ub378, \ub370\uc774\ud130, \ud559\uc2b5 \uc2dc\uac01\ud654\ud558\uae30 \ud29c\ud1a0\ub9ac\uc5bc Total running time of the script: (0 minutes 3.909 seconds) Download Jupyter notebook: profiler_recipe.ipynb Download Python source code: profiler_recipe.py Download zipped: profiler_recipe.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/recipes/recipes/profiler_recipe.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>