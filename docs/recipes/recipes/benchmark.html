
<!DOCTYPE html>


<html lang="ko" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2022-11-30T07:09:41+00:00" /><meta property="og:title" content="PyTorch Benchmark" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/recipes/recipes/benchmark.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="This recipe provides a quick-start guide to using PyTorch benchmark module to measure and compare code performance. Introduction: Benchmarking is an important step in writing code. It helps us validate that our code meets performance expectations, compare different approaches to solving the same ..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="This recipe provides a quick-start guide to using PyTorch benchmark module to measure and compare code performance. Introduction: Benchmarking is an important step in writing code. It helps us validate that our code meets performance expectations, compare different approaches to solving the same ..." />
<meta property="og:ignore_canonical" content="true" />

    <title>PyTorch Benchmark &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../../_static/doctools.js?v=92e14aea"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/translations.js?v=b5f768d8"></script>
    <script src="../../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'recipes/recipes/benchmark';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/recipes/recipes/benchmark.html" />
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../../genindex.html" />
    <link rel="search" title="검색" href="../../search.html" />
    <link rel="next" title="Tips for Loading an nn.Module from a Checkpoint" href="module_load_state_dict_tips.html" />
    <link rel="prev" title="Getting Started with CommDebugMode" href="../distributed_comm_debug_mode.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../../_static/js/theme.js"></script>
<script type="text/javascript" src="../../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2022년 11월 30일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="defining_a_neural_network.html">Pytorch를 사용해 신경망 정의하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_logs.html">(beta) torch.compile과 함께 TORCH_LOGS 파이썬 API 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="what_is_state_dict.html">PyTorch에서 state_dict란 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="warmstarting_model_using_parameters_from_a_different_model.html">PyTorch에서 다른 모델의 매개변수를 사용하여 빠르게 모델 시작하기(warmstart)</a></li>
<li class="toctree-l1"><a class="reference internal" href="zeroing_out_gradients.html">PyTorch에서 변화도를 0으로 만들기</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiler_recipe.html">PyTorch 프로파일러(Profiler)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Captum_Recipe.html">Captum을 사용하여 모델 해석하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_with_pytorch.html">PyTorch로 TensorBoard 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp_recipe.html">자동 혼합 정밀도(Automatic Mixed Precision) 가이드</a></li>
<li class="toctree-l1"><a class="reference internal" href="tuning_guide.html">성능 튜닝 가이드</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiling_optimizer.html">(beta) Compiling the optimizer with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="timer_quick_start.html">Timer 빠르게 시작하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_backend_ipex.html">Intel® CPU에서의 Intel® Extension for PyTorch* 백엔드</a></li>
<li class="toctree-l1"><a class="reference internal" href="../zero_redundancy_optimizer.html">Shard Optimizer States with ZeroRedundancyOptimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cuda_rpc.html">Direct Device-to-Device Communication with TensorPipe CUDA RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_comm_debug_mode.html">Getting Started with <code class="docutils literal notranslate"><span class="pre">CommDebugMode</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_export_challenges_solutions.html">Demonstration of torch.export flow, common challenges and the solutions to address them</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">PyTorch Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="module_load_state_dict_tips.html">Tips for Loading an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> from a Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="reasoning_about_shapes.html">PyTorch의 Shape들에 대한 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="swap_tensors.html">Extension points in <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> for <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> and tensor subclasses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_export_aoti_python.html"><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> AOTInductor Tutorial for Python runtime (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_with_pytorch.html">PyTorch로 TensorBoard 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../inference_tuning_on_aws_graviton.html">(Beta) PyTorch Inference Performance Tuning on AWS Graviton Processors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../amx.html">Leverage Intel® Advanced Matrix Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_torch_function_modes.html">(beta) Utilizing Torch Function modes with torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiling_optimizer_lr_scheduler.html">(beta) Running the compiled optimizer with an LR Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../foreach_map.html">Explicit horizontal fusion with foreach_map and torch.compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_user_defined_triton_kernel_tutorial.html">사용자 정의 Triton 커널을 ``torch.compile``과 함께 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_caching_tutorial.html">Compile Time Caching in <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch_compile_caching_configuration_tutorial.html">Compile Time Caching Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regional_compilation.html">Reducing torch.compile cold start compilation time with regional compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regional_aot.html">Reducing AoT cold start compilation time with regional compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intel_neural_compressor_for_pytorch.html">Ease-of-use quantization for PyTorch with Intel® Neural Compressor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_device_mesh.html">Getting Started with DeviceMesh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_checkpoint_recipe.html">Getting Started with Distributed Checkpoint (DCP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_async_checkpoint_recipe.html">Asynchronous Saving with Distributed Checkpoint (DCP)</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../recipes_index.html" class="nav-link">Recipes</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">PyTorch Benchmark</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="../../recipes_index.html">
        <meta itemprop="name" content="Recipes">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="PyTorch Benchmark">
        <meta itemprop="position" content="2">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">recipes/recipes/benchmark</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-recipes-recipes-benchmark-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="pytorch-benchmark">
<span id="sphx-glr-recipes-recipes-benchmark-py"></span><h1>PyTorch Benchmark<a class="headerlink" href="#pytorch-benchmark" title="Link to this heading">#</a></h1>
<p>This recipe provides a quick-start guide to using PyTorch
<code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module to measure and compare code performance.</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Benchmarking is an important step in writing code. It helps
us validate that our code meets performance expectations,
compare different approaches to solving the same problem and
prevent performance regressions.</p>
<p>There are many options when it comes to benchmarking PyTorch code
including the Python builtin <code class="docutils literal notranslate"><span class="pre">timeit</span></code> module. However, benchmarking
PyTorch code has many caveats that can be easily overlooked such as
managing the number of threads and synchronizing CUDA devices. Moreover,
generating Tensor inputs for benchmarking can be quite tedious.</p>
<p>This recipe demonstrates how to use PyTorch <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module to avoid
common mistakes while making it easier to compare performance of
different code, generate input for benchmarking and more.</p>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<p>Before we begin, install <code class="docutils literal notranslate"><span class="pre">torch</span></code> if it isn’t already available.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span>
</pre></div>
</div>
</section>
<section id="steps">
<h2>Steps<a class="headerlink" href="#steps" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Defining functions to benchmark</p></li>
<li><p>Benchmarking with <code class="docutils literal notranslate"><span class="pre">timeit.Timer</span></code></p></li>
<li><p>Benchmarking with <code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark.Timer</span></code></p></li>
<li><p>Benchmarking with <code class="docutils literal notranslate"><span class="pre">Blocked</span> <span class="pre">Autorange</span></code></p></li>
<li><p>Comparing benchmark results</p></li>
<li><p>Saving/Loading benchmark results</p></li>
<li><p>Generating inputs with <code class="docutils literal notranslate"><span class="pre">Fuzzed</span> <span class="pre">Parameters</span></code></p></li>
<li><p>Collecting instruction counts with <code class="docutils literal notranslate"><span class="pre">Callgrind</span></code></p></li>
</ol>
<section id="defining-functions-to-benchmark">
<h3>1. Defining functions to benchmark<a class="headerlink" href="#defining-functions-to-benchmark" title="Link to this heading">#</a></h3>
<p>As of the time of this writing, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.dot.html?highlight=dot#torch.dot">torch.dot</a>
does not support batched mode, so we will compare two approaches to
implementing it using existing <code class="docutils literal notranslate"><span class="pre">torch</span></code> operators: one approach uses a
combination of <code class="docutils literal notranslate"><span class="pre">mul</span></code> and <code class="docutils literal notranslate"><span class="pre">sum</span></code> while the other reduces the problem to <code class="docutils literal notranslate"><span class="pre">bmm</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="k">def</span><span class="w"> </span><span class="nf">batched_dot_mul_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;Computes batched dot by multiplying and summing&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">batched_dot_bmm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;Computes batched dot by reducing to ``bmm``&#39;&#39;&#39;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>


<span class="c1"># Input for benchmarking</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

<span class="c1"># Ensure that both functions compute the same output</span>
<span class="k">assert</span> <span class="n">batched_dot_mul_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">batched_dot_bmm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="benchmarking-with-timeit-timer">
<h3>2. Benchmarking with <code class="docutils literal notranslate"><span class="pre">timeit.Timer</span></code><a class="headerlink" href="#benchmarking-with-timeit-timer" title="Link to this heading">#</a></h3>
<p>First, let’s benchmark the code using Python’s builtin <code class="docutils literal notranslate"><span class="pre">timeit</span></code> module.
We keep the benchmark code simple here so we can compare the defaults
of <code class="docutils literal notranslate"><span class="pre">timeit</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">timeit</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;mul_sum(x, x):  </span><span class="si">{</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bmm(x, x):      </span><span class="si">{</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>mul_sum(x, x):  148.9 us
bmm(x, x):       72.6 us
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id1">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id1" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> mul_sum(x, x):  111.6 us
 bmm(x, x):       70.0 us
</pre></div>
</div>
</div>
</section>
<section id="benchmarking-with-torch-utils-benchmark-timer">
<h3>3. Benchmarking with <code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark.Timer</span></code><a class="headerlink" href="#benchmarking-with-torch-utils-benchmark-timer" title="Link to this heading">#</a></h3>
<p>PyTorch <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module was designed to be familiar to those who
have used the <code class="docutils literal notranslate"><span class="pre">timeit</span></code> module before. However, its defaults make it
easier and safer to use for benchmarking PyTorch code. Let’s first
compare the same basic API as above.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.benchmark</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">benchmark</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fe4880638d0&gt;
batched_dot_mul_sum(x, x)
setup: from __main__ import batched_dot_mul_sum
  287.58 us
  1 measurement, 100 runs , 1 thread
&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fe4867b6090&gt;
batched_dot_bmm(x, x)
setup: from __main__ import batched_dot_bmm
  497.98 us
  1 measurement, 100 runs , 1 thread
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id2" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;
 batched_dot_mul_sum(x, x)
 setup: from __main__ import batched_dot_mul_sum
   379.29 us
   1 measurement, 100 runs , 1 thread
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb103d67048&gt;
 batched_dot_bmm(x, x)
 setup: from __main__ import batched_dot_bmm
   716.42 us
   1 measurement, 100 runs , 1 thread
</pre></div>
</div>
</div>
<p>Even though the APIs are the same for the basic functionality, there
are some important differences. <code class="docutils literal notranslate"><span class="pre">benchmark.Timer.timeit()</span></code> returns the
time per run as opposed to the total runtime like <code class="docutils literal notranslate"><span class="pre">timeit.Timer.timeit()</span></code>
does. PyTorch <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module also provides formatted string
representations for printing the results.</p>
<p>Another important difference, and the reason why the results diverge
is that PyTorch benchmark module runs in a single thread by default.
We can change the number of threads with the <code class="docutils literal notranslate"><span class="pre">num_threads</span></code> argument.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark.Timer</span></code> takes several additional arguments
including: <code class="docutils literal notranslate"><span class="pre">label</span></code>, <code class="docutils literal notranslate"><span class="pre">sub_label</span></code>, <code class="docutils literal notranslate"><span class="pre">description</span></code> and <code class="docutils literal notranslate"><span class="pre">env</span></code> which change
the __repr__ of the measurement object returned and are used for
grouping the results (more on this later).</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">num_threads</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_num_threads</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Benchmarking on </span><span class="si">{</span><span class="n">num_threads</span><span class="si">}</span><span class="s1"> threads&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
    <span class="n">num_threads</span><span class="o">=</span><span class="n">num_threads</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Multithreaded batch dot&#39;</span><span class="p">,</span>
    <span class="n">sub_label</span><span class="o">=</span><span class="s1">&#39;Implemented using mul and sum&#39;</span><span class="p">)</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
    <span class="n">num_threads</span><span class="o">=</span><span class="n">num_threads</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Multithreaded batch dot&#39;</span><span class="p">,</span>
    <span class="n">sub_label</span><span class="o">=</span><span class="s1">&#39;Implemented using bmm&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Benchmarking on 128 threads
&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fe487c77250&gt;
Multithreaded batch dot: Implemented using mul and sum
setup: from __main__ import batched_dot_mul_sum
  105.23 us
  1 measurement, 100 runs , 128 threads
&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fe487b791d0&gt;
Multithreaded batch dot: Implemented using bmm
setup: from __main__ import batched_dot_bmm
  59.98 us
  1 measurement, 100 runs , 128 threads
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id3">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id3" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Benchmarking on 40 threads
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb103d54080&gt;
 Multithreaded batch dot: Implemented using mul and sum
 setup: from __main__ import batched_dot_mul_sum
   118.47 us
   1 measurement, 100 runs , 40 threads
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;
 Multithreaded batch dot: Implemented using bmm
 setup: from __main__ import batched_dot_bmm
   68.21 us
   1 measurement, 100 runs , 40 threads
</pre></div>
</div>
</div>
<p>Running <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> with all threads available gives similar results
as the <code class="docutils literal notranslate"><span class="pre">timeit</span></code> module. More importantly, which version is faster
depends on how many threads we run the code with. This is why it’s
important to benchmark the code with thread settings that are
representative of real use cases. Another important thing to remember
is to synchronize CPU and CUDA when benchmarking on the GPU. Let’s run
the above benchmarks again on a CUDA tensor and see what happens.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="c1"># Ran each twice to show difference before/after warm-up</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;mul_sum(x, x):  </span><span class="si">{</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;mul_sum(x, x):  </span><span class="si">{</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bmm(x, x):      </span><span class="si">{</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bmm(x, x):      </span><span class="si">{</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>mul_sum(x, x):  206.7 us
mul_sum(x, x):   12.7 us
bmm(x, x):      980.4 us
bmm(x, x):       13.8 us
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id4" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> mul_sum(x, x):   27.6 us
 mul_sum(x, x):   25.3 us
 bmm(x, x):      2775.5 us
 bmm(x, x):       22.4 us
</pre></div>
</div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="c1"># Run only once since benchmark module does warm-up for us</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fe487c262d0&gt;
batched_dot_mul_sum(x, x)
setup: from __main__ import batched_dot_mul_sum
  34.80 us
  1 measurement, 100 runs , 1 thread
&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fe4867be450&gt;
batched_dot_bmm(x, x)
setup: from __main__ import batched_dot_bmm
  23.34 us
  1 measurement, 100 runs , 1 thread
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id5">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id5" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080&gt;
 batched_dot_mul_sum(x, x)
 setup: from __main__ import batched_dot_mul_sum
   232.93 us
   1 measurement, 100 runs , 1 thread
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;
 batched_dot_bmm(x, x)
 setup: from __main__ import batched_dot_bmm
   181.04 us
   1 measurement, 100 runs , 1 thread
</pre></div>
</div>
</div>
<p>The results reveal something interesting. The first run of the <code class="docutils literal notranslate"><span class="pre">bmm</span></code>
version using the <code class="docutils literal notranslate"><span class="pre">timeit</span></code> module takes much longer than the second
run. This is because <code class="docutils literal notranslate"><span class="pre">bmm</span></code> calls into <cite>cuBLAS</cite> which needs to be
loaded the first time it’s called which takes some time. This is why
it’s important to do a warm-up run before benchmarking, luckily for
us, PyTorch’s <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module takes care of that.</p>
<p>The difference in the results between <code class="docutils literal notranslate"><span class="pre">timeit</span></code> and <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> modules
is because the <cite>timeit</cite> module is not synchronizing CUDA and is thus only
timing the time to launch the kernel. PyTorch’s <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module does
the synchronization for us.</p>
</section>
<section id="benchmarking-with-blocked-autorange">
<h3>4. Benchmarking with <cite>Blocked Autorange</cite><a class="headerlink" href="#benchmarking-with-blocked-autorange" title="Link to this heading">#</a></h3>
<p>While <code class="docutils literal notranslate"><span class="pre">timeit.Timer.autorange</span></code> takes a single continuous measurement
of at least 0.2 seconds, <cite>torch.utils.benchmark.blocked_autorange</cite>
takes many measurements whose times total at least 0.2 seconds (which
can be changed by the <cite>min_run_time</cite> parameter) subject to the constraint
that timing overhead is a small fraction of the overall measurement.
This is accomplished by first running with an increasing number of runs
per loop until the runtime is much larger than measurement overhead
(which also serves as a warm up), and then taking measurements until
the target time is reached. This has the useful properties that it wastes
less data and allows us to compute statistics to estimate the reliability
of the measurements.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">m0</span> <span class="o">=</span> <span class="n">t0</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">()</span>
<span class="n">m1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">m0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m1</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fe4867be450&gt;
batched_dot_mul_sum(x, x)
setup: from __main__ import batched_dot_mul_sum
  34.70 us
  1 measurement, 10000 runs , 1 thread
&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fe487bbd250&gt;
batched_dot_bmm(x, x)
setup: from __main__ import batched_dot_bmm
  23.29 us
  1 measurement, 10000 runs , 1 thread
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id6">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id6" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;
 batched_dot_mul_sum(x, x)
 setup: from __main__ import batched_dot_mul_sum
   231.79 us
   1 measurement, 1000 runs , 1 thread
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080&gt;
 batched_dot_bmm(x, x)
 setup: from __main__ import batched_dot_bmm
   Median: 162.08 us
   2 measurements, 1000 runs per measurement, 1 thread
</pre></div>
</div>
</div>
<p>We can also inspect the individual statistics from the returned
measurements object.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean:   </span><span class="si">{</span><span class="n">m0</span><span class="o">.</span><span class="n">mean</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s2">6.2f</span><span class="si">}</span><span class="s2"> us&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Median: </span><span class="si">{</span><span class="n">m0</span><span class="o">.</span><span class="n">median</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s2">6.2f</span><span class="si">}</span><span class="s2"> us&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Mean:    34.70 us
Median:  34.70 us
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id7">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id7" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Mean:   231.79 us
 Median: 231.79 us
</pre></div>
</div>
</div>
</section>
<section id="comparing-benchmark-results">
<h3>5. Comparing benchmark results<a class="headerlink" href="#comparing-benchmark-results" title="Link to this heading">#</a></h3>
<p>So far we’ve been comparing our two versions of batched dot against a
single input. In practice, we want to try a combination of inputs as
well as different number of threads. The <code class="docutils literal notranslate"><span class="pre">Compare</span></code> class helps display
the results of many measurements in a formatted table. It uses the
annotations described above (<cite>label</cite>, <cite>sub_label</cite>, <cite>num_threads</cite>, etc.) as
well as <cite>description</cite> to group and organize the table. Let’s use
<code class="docutils literal notranslate"><span class="pre">Compare</span></code> to see how our functions perform for different input sizes
and number of threads.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">itertools</span><span class="w"> </span><span class="kn">import</span> <span class="n">product</span>

<span class="c1"># Compare takes a list of measurements which we&#39;ll save in results.</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">10000</span><span class="p">]</span>
<span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
    <span class="c1"># label and sub_label are the rows</span>
    <span class="c1"># description is the column</span>
    <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Batched dot&#39;</span>
    <span class="n">sub_label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;[</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">]&#39;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">num_threads</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">]:</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
            <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
            <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
            <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
            <span class="n">num_threads</span><span class="o">=</span><span class="n">num_threads</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span>
            <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s1">&#39;mul/sum&#39;</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
            <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
            <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
            <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
            <span class="n">num_threads</span><span class="o">=</span><span class="n">num_threads</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span>
            <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s1">&#39;bmm&#39;</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">compare</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">compare</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[--------------- Batched dot ----------------]
                      |  mul/sum   |    bmm
1 threads: -----------------------------------
      [1, 1]          |       3.3  |       5.5
      [1, 64]         |       3.4  |       5.7
      [1, 1024]       |       3.6  |       5.9
      [1, 10000]      |       5.1  |       6.9
      [64, 1]         |       3.7  |       6.1
      [64, 64]        |       4.7  |       9.0
      [64, 1024]      |      16.3  |     178.8
      [64, 10000]     |     277.9  |    1684.8
      [1024, 1]       |       4.4  |      11.1
      [1024, 64]      |      18.0  |      55.4
      [1024, 1024]    |     468.8  |    2769.2
      [1024, 10000]   |   14630.1  |   26818.0
      [10000, 1]      |      10.2  |      57.6
      [10000, 64]     |     296.7  |     495.0
      [10000, 1024]   |   14513.2  |   27164.9
      [10000, 10000]  |  185983.9  |  264568.8
4 threads: -----------------------------------
      [1, 1]          |       3.4  |       5.5
      [1, 64]         |       3.5  |       5.8
      [1, 1024]       |       3.6  |       5.9
      [1, 10000]      |       5.0  |       6.9
      [64, 1]         |       3.7  |       6.1
      [64, 64]        |       4.9  |       9.1
      [64, 1024]      |      45.8  |     223.9
      [64, 10000]     |      37.6  |    2033.7
      [1024, 1]       |       4.5  |      11.2
      [1024, 64]      |      67.7  |      34.5
      [1024, 1024]    |      74.5  |     715.4
      [1024, 10000]   |    7078.1  |    6833.2
      [10000, 1]      |      10.3  |      57.6
      [10000, 64]     |      60.1  |     151.0
      [10000, 1024]   |    7173.4  |    6783.7
      [10000, 10000]  |   77143.6  |   65605.9
16 threads: ----------------------------------
      [1, 1]          |       3.5  |       5.6
      [1, 64]         |       3.5  |       5.7
      [1, 1024]       |       3.6  |       5.9
      [1, 10000]      |       5.0  |       6.9
      [64, 1]         |       3.8  |       6.1
      [64, 64]        |       4.8  |       9.0
      [64, 1024]      |      37.7  |     303.5
      [64, 10000]     |      22.5  |    3053.1
      [1024, 1]       |       4.6  |      11.2
      [1024, 64]      |      38.0  |      36.0
      [1024, 1024]    |      27.6  |     209.2
      [1024, 10000]   |    3499.9  |    1867.4
      [10000, 1]      |      10.3  |      57.5
      [10000, 64]     |      31.5  |      50.0
      [10000, 1024]   |    3745.5  |    1752.8
      [10000, 10000]  |   37417.4  |   16803.8
32 threads: ----------------------------------
      [1, 1]          |       3.5  |       5.5
      [1, 64]         |       3.4  |       5.7
      [1, 1024]       |       3.7  |       5.9
      [1, 10000]      |       5.0  |       6.9
      [64, 1]         |       3.8  |       6.1
      [64, 64]        |       4.9  |       9.0
      [64, 1024]      |      49.9  |     306.9
      [64, 10000]     |      68.8  |    2841.5
      [1024, 1]       |       4.6  |      11.2
      [1024, 64]      |      40.6  |      37.5
      [1024, 1024]    |      28.8  |     129.1
      [1024, 10000]   |    3668.5  |    1312.2
      [10000, 1]      |      10.3  |      57.6
      [10000, 64]     |      84.4  |      46.3
      [10000, 1024]   |    3548.7  |     907.7
      [10000, 10000]  |   36430.9  |    8427.9

Times are in microseconds (us).
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id8">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id8" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> [--------------- Batched dot ----------------]
                       |  mul/sum   |    bmm
 1 threads: -----------------------------------
       [1, 1]          |       5.9  |      11.2
       [1, 64]         |       6.4  |      11.4
       [1, 1024]       |       6.7  |      14.2
       [1, 10000]      |      10.2  |      23.7
       [64, 1]         |       6.3  |      11.5
       [64, 64]        |       8.6  |      15.4
       [64, 1024]      |      39.4  |     204.4
       [64, 10000]     |     274.9  |     748.5
       [1024, 1]       |       7.7  |      17.8
       [1024, 64]      |      40.3  |      76.4
       [1024, 1024]    |     432.4  |    2795.9
       [1024, 10000]   |   22657.3  |   11899.5
       [10000, 1]      |      16.9  |      74.8
       [10000, 64]     |     300.3  |     609.4
       [10000, 1024]   |   23098.6  |   27246.1
       [10000, 10000]  |  267073.7  |  118823.7
 4 threads: -----------------------------------
       [1, 1]          |       6.0  |      11.5
       [1, 64]         |       6.2  |      11.2
       [1, 1024]       |       6.8  |      14.3
       [1, 10000]      |      10.2  |      23.7
       [64, 1]         |       6.3  |      16.2
       [64, 64]        |       8.8  |      18.2
       [64, 1024]      |      41.5  |     189.1
       [64, 10000]     |      91.7  |     849.1
       [1024, 1]       |       7.6  |      17.4
       [1024, 64]      |      43.5  |      33.5
       [1024, 1024]    |     135.4  |    2782.3
       [1024, 10000]   |    7471.1  |   11874.0
       [10000, 1]      |      16.8  |      33.9
       [10000, 64]     |     118.7  |     173.2
       [10000, 1024]   |    7264.6  |   27824.7
       [10000, 10000]  |  100060.9  |  121499.0
 16 threads: ----------------------------------
       [1, 1]          |       6.0  |      11.3
       [1, 64]         |       6.2  |      11.2
       [1, 1024]       |       6.9  |      14.2
       [1, 10000]      |      10.3  |      23.8
       [64, 1]         |       6.4  |      24.1
       [64, 64]        |       9.0  |      23.8
       [64, 1024]      |      54.1  |     188.5
       [64, 10000]     |      49.9  |     748.0
       [1024, 1]       |       7.6  |      23.4
       [1024, 64]      |      55.5  |      28.2
       [1024, 1024]    |      66.9  |    2773.9
       [1024, 10000]   |    6111.5  |   12833.7
       [10000, 1]      |      16.9  |      27.5
       [10000, 64]     |      59.5  |      73.7
       [10000, 1024]   |    6295.9  |   27062.0
       [10000, 10000]  |   71804.5  |  120365.8
 32 threads: ----------------------------------
       [1, 1]          |       5.9  |      11.3
       [1, 64]         |       6.2  |      11.3
       [1, 1024]       |       6.7  |      14.2
       [1, 10000]      |      10.5  |      23.8
       [64, 1]         |       6.3  |      31.7
       [64, 64]        |       9.1  |      30.4
       [64, 1024]      |      72.0  |     190.4
       [64, 10000]     |     103.1  |     746.9
       [1024, 1]       |       7.6  |      28.4
       [1024, 64]      |      70.5  |      31.9
       [1024, 1024]    |      65.6  |    2804.6
       [1024, 10000]   |    6764.0  |   11871.4
       [10000, 1]      |      17.8  |      31.8
       [10000, 64]     |     110.3  |      56.0
       [10000, 1024]   |    6640.2  |   27592.2
       [10000, 10000]  |   73003.4  |  120083.2

 Times are in microseconds (us).
</pre></div>
</div>
</div>
<p>The results above indicate that the version which reduces to <code class="docutils literal notranslate"><span class="pre">bmm</span></code>
is better for larger tensors running on multiple threads, while for
smaller and/or single thread code, the other version is better.</p>
<p><code class="docutils literal notranslate"><span class="pre">Compare</span></code> also provides functions for changing the table format</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">compare</span><span class="o">.</span><span class="n">trim_significant_figures</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">colorize</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[-------------- Batched dot --------------]
                      |  mul/sum  |   bmm
1 threads: --------------------------------
      [1, 1]          |        3  |       6
      [1, 64]         |        3  |       6
      [1, 1024]       |        4  |       6
      [1, 10000]      |        5  |       7
      [64, 1]         |        4  |       6
      [64, 64]        |        5  |       9
      [64, 1024]      |       16  |     179
      [64, 10000]     |      278  |    1680
      [1024, 1]       |        4  |      11
      [1024, 64]      |       18  |      55
      [1024, 1024]    |      469  |    2769
      [1024, 10000]   |    14600  |   26820
      [10000, 1]      |       10  |      58
      [10000, 64]     |      297  |     490
      [10000, 1024]   |    14500  |   27160
      [10000, 10000]  |   190000  |  264600
4 threads: --------------------------------
      [1, 1]          |        3  |       6
      [1, 64]         |        4  |       6
      [1, 1024]       |        4  |       6
      [1, 10000]      |        5  |       7
      [64, 1]         |        4  |       6
      [64, 64]        |        5  |       9
      [64, 1024]      |       46  |     220
      [64, 10000]     |       38  |    2000
      [1024, 1]       |        4  |      11
      [1024, 64]      |       68  |      35
      [1024, 1024]    |       74  |     715
      [1024, 10000]   |     7100  |    6833
      [10000, 1]      |       10  |      58
      [10000, 64]     |       60  |     200
      [10000, 1024]   |     7200  |    6784
      [10000, 10000]  |    77000  |   65600
16 threads: -------------------------------
      [1, 1]          |        4  |       6
      [1, 64]         |        4  |       6
      [1, 1024]       |        4  |       6
      [1, 10000]      |        5  |       7
      [64, 1]         |        4  |       6
      [64, 64]        |        5  |       9
      [64, 1024]      |       40  |     303
      [64, 10000]     |       22  |    3100
      [1024, 1]       |        5  |      11
      [1024, 64]      |       38  |      36
      [1024, 1024]    |       28  |     209
      [1024, 10000]   |     3000  |    1900
      [10000, 1]      |       10  |      58
      [10000, 64]     |       32  |      50
      [10000, 1024]   |     4000  |    1800
      [10000, 10000]  |    40000  |   20000
32 threads: -------------------------------
      [1, 1]          |        4  |       6
      [1, 64]         |        3  |       6
      [1, 1024]       |        4  |       6
      [1, 10000]      |        5  |       7
      [64, 1]         |        4  |       6
      [64, 64]        |        5  |       9
      [64, 1024]      |       50  |     307
      [64, 10000]     |       69  |    2840
      [1024, 1]       |        5  |      11
      [1024, 64]      |       41  |      38
      [1024, 1024]    |       29  |     129
      [1024, 10000]   |     3700  |    1310
      [10000, 1]      |       10  |      58
      [10000, 64]     |       84  |      50
      [10000, 1024]   |     4000  |     910
      [10000, 10000]  |    36000  |    8430

Times are in microseconds (us).
</pre></div>
</div>
</section>
<section id="saving-loading-benchmark-results">
<h3>6. Saving/Loading benchmark results<a class="headerlink" href="#saving-loading-benchmark-results" title="Link to this heading">#</a></h3>
<p><cite>Measurements</cite> (and <code class="docutils literal notranslate"><span class="pre">CallgrindStats</span></code> which are described in section 8)
can be serialized by the <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module. This makes A/B testing easy, as you can collect
measurements from two separate environments, pickle them, and then
load both in a single environment. Timer even takes an <cite>env</cite>
constructor argument so that such A/B testing works seamlessly.</p>
<p>Let’s imagine that rather than two Python functions, the add/sum
and <code class="docutils literal notranslate"><span class="pre">bmm</span></code> approaches were in two different builds of PyTorch.
The example below demonstrates how one might A/B test them. For
simplicity, we only use a subset of shapes, and simply round trip
results through pickle rather than actually using multiple environments
and writing results to disk.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>

<span class="n">ab_test_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">env</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;environment A: mul/sum&#39;</span><span class="p">,</span> <span class="s1">&#39;environment B: bmm&#39;</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">10000</span><span class="p">),</span> <span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
        <span class="n">dot_fn</span> <span class="o">=</span> <span class="p">(</span><span class="n">batched_dot_mul_sum</span> <span class="k">if</span> <span class="n">env</span> <span class="o">==</span> <span class="s1">&#39;environment A: mul/sum&#39;</span> <span class="k">else</span> <span class="n">batched_dot_bmm</span><span class="p">)</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
            <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot(x, x)&#39;</span><span class="p">,</span>
            <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;batched_dot&#39;</span><span class="p">:</span> <span class="n">dot_fn</span><span class="p">},</span>
            <span class="n">num_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batched dot&#39;</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;[</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">,</span>
            <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ab_test_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>

<span class="n">ab_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ab_test_results</span><span class="p">]</span>
<span class="n">compare</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">ab_results</span><span class="p">)</span>
<span class="n">compare</span><span class="o">.</span><span class="n">trim_significant_figures</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">colorize</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[------------------------------------- Batched dot -------------------------------------]
                                               |  [1, 1]  |  [1024, 10000]  |  [10000, 1]
1 threads: ------------------------------------------------------------------------------
  (environment A: mul/sum)  batched_dot(x, x)  |   3.4    |      14100      |      10
  (environment B: bmm)      batched_dot(x, x)  |   5.4    |      27100      |      58

Times are in microseconds (us).
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id9">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id9" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> [------------------------------------- Batched dot -------------------------------------]
                                                |  [1, 1]  |  [1024, 10000]  |  [10000, 1]
 1 threads: ------------------------------------------------------------------------------
   (environment A: mul/sum)  batched_dot(x, x)  |     7    |      36000      |      21
   (environment B: bmm)      batched_dot(x, x)  |    14    |      40000      |      85

 Times are in microseconds (us).
</pre></div>
</div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># And just to show that we can round trip all of the results from earlier:</span>
<span class="n">round_tripped_results</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">results</span><span class="p">))</span>
<span class="k">assert</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">results</span><span class="p">))</span> <span class="o">==</span> <span class="nb">str</span><span class="p">(</span><span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">round_tripped_results</span><span class="p">)))</span>
</pre></div>
</div>
</section>
<section id="generating-inputs-with-fuzzed-parameters">
<h3>7. Generating inputs with <cite>Fuzzed Parameters</cite><a class="headerlink" href="#generating-inputs-with-fuzzed-parameters" title="Link to this heading">#</a></h3>
<p>As we’ve seen in the previous section, there can be some stark
performance differences depending on the input tensors. Hence, it
is a good idea to run benchmarks on a number of different inputs.
However, creating all these input tensors can be tedious which is
where <code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark.Fuzzer</span></code> and related classes come in.
Let’s take a look at how we can use the <code class="docutils literal notranslate"><span class="pre">Fuzzer</span></code> to create some test
cases for the benchmark.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.benchmark</span><span class="w"> </span><span class="kn">import</span> <span class="n">Fuzzer</span><span class="p">,</span> <span class="n">FuzzedParameter</span><span class="p">,</span> <span class="n">FuzzedTensor</span><span class="p">,</span> <span class="n">ParameterAlias</span>

<span class="c1"># Generates random tensors with 128 to 10000000 elements and sizes k0 and k1 chosen from a</span>
<span class="c1"># ``loguniform`` distribution in [1, 10000], 40% of which will be discontiguous on average.</span>
<span class="n">example_fuzzer</span> <span class="o">=</span> <span class="n">Fuzzer</span><span class="p">(</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">FuzzedParameter</span><span class="p">(</span><span class="s1">&#39;k0&#39;</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;loguniform&#39;</span><span class="p">),</span>
        <span class="n">FuzzedParameter</span><span class="p">(</span><span class="s1">&#39;k1&#39;</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;loguniform&#39;</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">FuzzedTensor</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;k0&#39;</span><span class="p">,</span> <span class="s1">&#39;k1&#39;</span><span class="p">),</span> <span class="n">min_elements</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">max_elements</span><span class="o">=</span><span class="mi">10000000</span><span class="p">,</span> <span class="n">probability_contiguous</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">tensor_params</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">example_fuzzer</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># description is the column label</span>
    <span class="n">sub_label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k0&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;6</span><span class="si">}</span><span class="s2"> x </span><span class="si">{</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k1&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;4</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">tensor_params</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][</span><span class="s1">&#39;is_contiguous&#39;</span><span class="p">]</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;(discontiguous)&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
        <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
        <span class="nb">globals</span><span class="o">=</span><span class="n">tensors</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batched dot&#39;</span><span class="p">,</span>
        <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s1">&#39;mul/sum&#39;</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
        <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
        <span class="nb">globals</span><span class="o">=</span><span class="n">tensors</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batched dot&#39;</span><span class="p">,</span>
        <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s1">&#39;bmm&#39;</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">compare</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">compare</span><span class="o">.</span><span class="n">trim_significant_figures</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[--------------------- Batched dot ---------------------]
                                     |  mul/sum  |   bmm
1 threads: ----------------------------------------------
      725    x 257                   |      39   |    148
      49     x 383                   |       7   |     20
      34     x 1468                  |      12   |    138
      187    x 5039                  |     433   |   2490
      2140   x 1296 (discontiguous)  |    1240   |  20100
      78     x 1598                  |      25   |    337
      519    x 763                   |     144   |   1058
      141    x 1082                  |      29   |    410
      78     x 5    (discontiguous)  |       4   |      8
      187    x 1                     |       4   |      7

Times are in microseconds (us).
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id10">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id10" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> [--------------------- Batched dot ---------------------]
                                      |  mul/sum  |   bmm
 1 threads: ----------------------------------------------
       725    x 257                   |      87   |    180
       49     x 383                   |      15   |     30
       34     x 1468                  |      30   |    118
       187    x 5039                  |     400   |   1200
       2140   x 1296 (discontiguous)  |    2000   |  41000
       78     x 1598                  |      74   |    310
       519    x 763                   |     190   |   1500
       141    x 1082                  |      87   |    500
       78     x 5    (discontiguous)  |       9   |     20
       187    x 1                     |      12   |     10

 Times are in microseconds (us).
</pre></div>
</div>
</div>
<p>There is a lot of flexibility for defining your own <code class="docutils literal notranslate"><span class="pre">fuzzers</span></code> which
is great for creating a powerful set of inputs to benchmark. But to
make things even simpler, PyTorch benchmark module comes with some
built-in <code class="docutils literal notranslate"><span class="pre">fuzzers</span></code> for common benchmarking needs. Let’s take a look at
how we can use one of these built-in <code class="docutils literal notranslate"><span class="pre">fuzzers</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.benchmark.op_fuzzers</span><span class="w"> </span><span class="kn">import</span> <span class="n">binary</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">tensor_params</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">binary</span><span class="o">.</span><span class="n">BinaryOpFuzzer</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">sub_label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k0&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;6</span><span class="si">}</span><span class="s2"> x </span><span class="si">{</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k1&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;4</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">tensor_params</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][</span><span class="s1">&#39;is_contiguous&#39;</span><span class="p">]</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;(discontiguous)&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
        <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
        <span class="nb">globals</span><span class="o">=</span><span class="n">tensors</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batched dot&#39;</span><span class="p">,</span>
        <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s1">&#39;mul/sum&#39;</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
        <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
        <span class="nb">globals</span><span class="o">=</span><span class="n">tensors</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batched dot&#39;</span><span class="p">,</span>
        <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s1">&#39;bmm&#39;</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">compare</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">compare</span><span class="o">.</span><span class="n">trim_significant_figures</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">colorize</span><span class="p">(</span><span class="n">rowwise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">compare</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[----------------------- Batched dot ------------------------]
                                         |  mul/sum  |   bmm
1 threads: ---------------------------------------------------
      64     x 473  (discontiguous)      |   20880   |   43000
      16384  x 12642115 (discontiguous)  |      15   |      65
      8192   x 892                       |    3280   |   19340
      512    x 64   (discontiguous)      |   64000   |  190000
      493    x 27   (discontiguous)      |    1328   |    2808
      118    x 32   (discontiguous)      |     783   |    2200
      16     x 495  (discontiguous)      |    5790   |   28000
      488    x 62374                     |   50000   |   80000
      240372 x 69                        |   23200   |   13000
      40156  x 32   (discontiguous)      |     743   |    2050

Times are in microseconds (us).
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id11">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id11" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> [----------------------- Batched dot ------------------------]
                                          |  mul/sum  |   bmm
 1 threads: ---------------------------------------------------
       64     x 473  (discontiguous)      |    10000  |   40000
       16384  x 12642115 (discontiguous)  |       31  |      78
       8192   x 892                       |     4800  |   20400
       512    x 64   (discontiguous)      |   110000  |  400000
       493    x 27   (discontiguous)      |     1100  |    2440
       118    x 32   (discontiguous)      |      870  |    2030
       16     x 495  (discontiguous)      |    23600  |   24000
       488    x 62374                     |    90000  |  100000
       240372 x 69                        |    40000  |   16000
       40156  x 32   (discontiguous)      |     2670  |    5000

 Times are in microseconds (us).
</pre></div>
</div>
</div>
</section>
<section id="collecting-instruction-counts-with-callgrind">
<h3>8. Collecting instruction counts with <code class="docutils literal notranslate"><span class="pre">Callgrind</span></code><a class="headerlink" href="#collecting-instruction-counts-with-callgrind" title="Link to this heading">#</a></h3>
<p>One of the challenges of optimizing code is the variation and opacity of
wall time. There are many sources of non-determinism, from adaptive clock
speeds to resource contention with other processes. Furthermore, end-to-end
time gives no insight into where time is being spent, which is really what
we’re interested in when optimizing code.</p>
<p>A complementary approach is to also collect instruction counts. These counts
are a proxy metric and do not capture all aspects of performance
(e.g. memory or I/O bound tasks), however they do have several useful
properties. Instruction counts are reproducible, insensitive to environmental
variation, and offer fine grained insight into where a program is spending
cycles.</p>
<p>To see the utility of instruction counts, let us look at how we might
reduce the overhead of <cite>batched_dot_mul_sum</cite>. The obvious solution is to
move it to C++, so we avoid going between Python and C++ multiple times.</p>
<p>Fortunately, the source is nearly identical. One question that we have to ask
in C++ is whether we should take arguments by value or reference.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">batched_dot_src</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">/* ---- Python ---- */</span>
<span class="s2">// def batched_dot_mul_sum(a, b):</span>
<span class="s2">//     return a.mul(b).sum(-1)</span>

<span class="s2">torch::Tensor batched_dot_mul_sum_v0(</span>
<span class="s2">    const torch::Tensor a,</span>
<span class="s2">    const torch::Tensor b) {</span>
<span class="s2">  return a.mul(b).sum(-1);</span>
<span class="s2">}</span>

<span class="s2">torch::Tensor batched_dot_mul_sum_v1(</span>
<span class="s2">    const torch::Tensor&amp; a,</span>
<span class="s2">    const torch::Tensor&amp; b) {</span>
<span class="s2">  return a.mul(b).sum(-1);</span>
<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span>


<span class="c1"># PyTorch makes it easy to test our C++ implementations by providing a utility</span>
<span class="c1"># to JIT compile C++ source into Python extensions:</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">cpp_extension</span>
<span class="n">cpp_lib</span> <span class="o">=</span> <span class="n">cpp_extension</span><span class="o">.</span><span class="n">load_inline</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;cpp_lib&#39;</span><span class="p">,</span>
    <span class="n">cpp_sources</span><span class="o">=</span><span class="n">batched_dot_src</span><span class="p">,</span>
    <span class="n">extra_cflags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;-O3&#39;</span><span class="p">],</span>
    <span class="n">extra_include_paths</span><span class="o">=</span><span class="p">[</span>
        <span class="c1"># `load_inline` needs to know where to find ``pybind11`` headers.</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;CONDA_PREFIX&#39;</span><span class="p">),</span> <span class="s1">&#39;include&#39;</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">functions</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;batched_dot_mul_sum_v0&#39;</span><span class="p">,</span> <span class="s1">&#39;batched_dot_mul_sum_v1&#39;</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># `load_inline` will create a shared object that is loaded into Python. When we collect</span>
<span class="c1"># instruction counts Timer will create a subprocess, so we need to re-import it. The</span>
<span class="c1"># import process is slightly more complicated for C extensions, but that&#39;s all we&#39;re</span>
<span class="c1"># doing here.</span>
<span class="n">module_import_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2"># https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path</span>
<span class="s2">import importlib.util</span>
<span class="s2">spec = importlib.util.spec_from_file_location(&quot;cpp_lib&quot;, </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">cpp_lib</span><span class="o">.</span><span class="vm">__file__</span><span class="p">)</span><span class="si">}</span><span class="s2">)</span>
<span class="s2">cpp_lib = importlib.util.module_from_spec(spec)</span>
<span class="s2">spec.loader.exec_module(cpp_lib)&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">textwrap</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pretty_print</span><span class="p">(</span><span class="n">result</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Import machinery for ``cpp_lib.so`` can get repetitive to look at.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="n">module_import_str</span><span class="p">,</span> <span class="s2">&quot;  &quot;</span><span class="p">),</span> <span class="s2">&quot;  import cpp_lib&quot;</span><span class="p">))</span>


<span class="n">t_baseline</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\</span>
<span class="s1">from __main__ import batched_dot_mul_sum</span>
<span class="s1">x = torch.randn(2, 2)&#39;&#39;&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;cpp_lib.batched_dot_mul_sum_v0(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\</span>
<span class="si">{</span><span class="n">module_import_str</span><span class="si">}</span>
<span class="s1">x = torch.randn(2, 2)&#39;&#39;&#39;</span><span class="p">)</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;cpp_lib.batched_dot_mul_sum_v1(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\</span>
<span class="si">{</span><span class="n">module_import_str</span><span class="si">}</span>
<span class="s1">x = torch.randn(2, 2)&#39;&#39;&#39;</span><span class="p">)</span>

<span class="c1"># Moving to C++ did indeed reduce overhead, but it&#39;s hard to tell which</span>
<span class="c1"># calling convention is more efficient. v1 (call with references) seems to</span>
<span class="c1"># be a bit faster, but it&#39;s within measurement error.</span>
<span class="n">pretty_print</span><span class="p">(</span><span class="n">t_baseline</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">())</span>
<span class="n">pretty_print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">())</span>
<span class="n">pretty_print</span><span class="p">(</span><span class="n">t1</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-pytb notranslate"><div class="highlight"><pre><span></span><span class="gt">Traceback (most recent call last):</span>
  File <span class="nb">&quot;/workspace/tutorials-kr/recipes_source/recipes/benchmark.py&quot;</span>, line <span class="m">727</span>, in <span class="n">&lt;module&gt;</span>
<span class="w">    </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;CONDA_PREFIX&#39;</span><span class="p">),</span> <span class="s1">&#39;include&#39;</span><span class="p">)</span>
  File <span class="nb">&quot;&lt;frozen posixpath&gt;&quot;</span>, line <span class="m">76</span>, in <span class="n">join</span>
<span class="gr">TypeError</span>: <span class="n">expected str, bytes or os.PathLike object, not NoneType</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id12">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id12" title="Link to this code">#</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;
 batched_dot_mul_sum(x, x)
 setup:
   from __main__ import batched_dot_mul_sum
   x = torch.randn(2, 2)

   6.92 us
   1 measurement, 100000 runs , 1 thread
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;
 cpp_lib.batched_dot_mul_sum_v0(x, x)
 setup:
   import cpp_lib
   x = torch.randn(2, 2)

   5.29 us
   1 measurement, 100000 runs , 1 thread
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;
 cpp_lib.batched_dot_mul_sum_v1(x, x)
 setup:
   import cpp_lib
   x = torch.randn(2, 2)

   5.22 us
   1 measurement, 100000 runs , 1 thread
</pre></div>
</div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s use ``Callgrind`` to determine which is better.</span>
<span class="n">stats_v0</span> <span class="o">=</span> <span class="n">t0</span><span class="o">.</span><span class="n">collect_callgrind</span><span class="p">()</span>
<span class="n">stats_v1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">collect_callgrind</span><span class="p">()</span>

<span class="n">pretty_print</span><span class="p">(</span><span class="n">stats_v0</span><span class="p">)</span>
<span class="n">pretty_print</span><span class="p">(</span><span class="n">stats_v1</span><span class="p">)</span>

<span class="c1"># `.as_standardized` removes file names and some path prefixes, and makes</span>
<span class="c1"># it easier to read the function symbols.</span>
<span class="n">stats_v0</span> <span class="o">=</span> <span class="n">stats_v0</span><span class="o">.</span><span class="n">as_standardized</span><span class="p">()</span>
<span class="n">stats_v1</span> <span class="o">=</span> <span class="n">stats_v1</span><span class="o">.</span><span class="n">as_standardized</span><span class="p">()</span>

<span class="c1"># `.delta` diffs the instruction counts, and `.denoise` removes several</span>
<span class="c1"># functions in the Python interpreter that are known to have significant</span>
<span class="c1"># jitter.</span>
<span class="n">delta</span> <span class="o">=</span> <span class="n">stats_v1</span><span class="o">.</span><span class="n">delta</span><span class="p">(</span><span class="n">stats_v0</span><span class="p">)</span><span class="o">.</span><span class="n">denoise</span><span class="p">()</span>

<span class="c1"># `.transform` is a convenience API for transforming function names. It is</span>
<span class="c1"># useful for increasing cancelation when ``diff-ing`` instructions, as well as</span>
<span class="c1"># just generally improving readability.</span>
<span class="n">replacements</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">(</span><span class="s2">&quot;???:void pybind11&quot;</span><span class="p">,</span> <span class="s2">&quot;pybind11&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;batched_dot_mul_sum_v0&quot;</span><span class="p">,</span> <span class="s2">&quot;batched_dot_mul_sum_v1&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;at::Tensor, at::Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;...&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;at::Tensor const&amp;, at::Tensor const&amp;&quot;</span><span class="p">,</span> <span class="s2">&quot;...&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;auto torch::detail::wrap_pybind_function_impl_&quot;</span><span class="p">,</span> <span class="s2">&quot;wrap_pybind_function_impl_&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">before</span><span class="p">,</span> <span class="n">after</span> <span class="ow">in</span> <span class="n">replacements</span><span class="p">:</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">delta</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">before</span><span class="p">,</span> <span class="n">after</span><span class="p">))</span>

<span class="c1"># We can use print options to control how much of the function to display.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">160</span><span class="p">)</span>

<span class="c1"># Once parsed, the instruction counts make clear that passing `a` and `b`</span>
<span class="c1"># by reference is more efficient as it skips some ``c10::TensorImpl`` bookkeeping</span>
<span class="c1"># for the intermediate Tensors, and is also works better with ``pybind11``. This</span>
<span class="c1"># is consistent with our noisy wall time observations.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb0f06e7630&gt;
cpp_lib.batched_dot_mul_sum_v0(x, x)
setup:
  import cpp_lib
  x = torch.randn(2, 2)
                           All          Noisy symbols removed
    Instructions:      2392671                    2392671
    Baseline:             4367                       4367
100 runs per measurement, 1 thread
Warning: PyTorch was not built with debug symbols.
         Source information may be limited. Rebuild with
         REL_WITH_DEB_INFO=1 for more detailed results.
&lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb10400d208&gt;
cpp_lib.batched_dot_mul_sum_v1(x, x)
setup:
  import cpp_lib
  x = torch.randn(2, 2)
                           All          Noisy symbols removed
    Instructions:      2378978                    2378978
    Baseline:             4367                       4367
    100 runs per measurement, 1 thread
    Warning: PyTorch was not built with debug symbols.
             Source information may be limited. Rebuild with
             REL_WITH_DEB_INFO=1 for more detailed results.
    &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7fb1000ab358&gt;
          86  ???:0x000000000020d9e0
      56  ???:0x000000000020db10
   -1100  pybind11::cpp_function::initialize&lt;wrap_pybind_function_impl_&lt;at::Tensor ... r (&amp;)(...), std::integer_sequence&lt;unsigned long, 0ul, 1ul&gt;)::{lambda(...)
   -1600  ???:wrap_pybind_function_impl_&lt;at::Tensor (&amp;)(...), 0ul, 1ul&gt;(at::Tensor (&amp;)(...), std::integer_sequence&lt;unsigned long, 0ul, 1ul&gt;)::{lambda(...)
   -5200  ???:c10::intrusive_ptr&lt;c10::TensorImpl, c10::UndefinedTensorImpl&gt;::reset_()
   -5935  ???:0x000000000022c0e0
Total: -13693
</pre></div>
</div>
</section>
</section>
<section id="learn-more">
<h2>Learn More<a class="headerlink" href="#learn-more" title="Link to this heading">#</a></h2>
<p>Take a look at these other recipes to continue your learning:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://tutorials.pytorch.kr/recipes/recipes/profiler.html">PyTorch Profiler</a></p></li>
</ul>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (4 minutes 18.013 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-recipes-recipes-benchmark-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/54db51700fabe094cbf7f11f5195d2bd/benchmark.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">benchmark.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/72c2f17ac50228049705f9a4d76c7815/benchmark.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">benchmark.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/f4dfac73cd6ebb51378b6a0307ed73b6/benchmark.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">benchmark.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="../distributed_comm_debug_mode.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Getting Started with <code class="docutils literal notranslate"><span class="pre">CommDebugMode</span></code></p>
      </div>
    </a>
    <a class="right-next"
       href="module_load_state_dict_tips.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Tips for Loading an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> from a Checkpoint</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../distributed_comm_debug_mode.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">이전</p>
        <p class="prev-next-title">Getting Started with <code class="docutils literal notranslate"><span class="pre">CommDebugMode</span></code></p>
      </div>
    </a>
    <a class="right-next"
       href="module_load_state_dict_tips.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">다음</p>
        <p class="prev-next-title">Tips for Loading an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> from a Checkpoint</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-functions-to-benchmark">1. Defining functions to benchmark</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking-with-timeit-timer">2. Benchmarking with <code class="docutils literal notranslate"><span class="pre">timeit.Timer</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking-with-torch-utils-benchmark-timer">3. Benchmarking with <code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark.Timer</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking-with-blocked-autorange">4. Benchmarking with <cite>Blocked Autorange</cite></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-benchmark-results">5. Comparing benchmark results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-loading-benchmark-results">6. Saving/Loading benchmark results</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-inputs-with-fuzzed-parameters">7. Generating inputs with <cite>Fuzzed Parameters</cite></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collecting-instruction-counts-with-callgrind">8. Collecting instruction counts with <code class="docutils literal notranslate"><span class="pre">Callgrind</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learn-more">Learn More</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "PyTorch Benchmark",
       "headline": "PyTorch Benchmark",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/recipes/recipes/benchmark.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. PyTorch Benchmark# This recipe provides a quick-start guide to using PyTorch benchmark module to measure and compare code performance. Introduction# Benchmarking is an important step in writing code. It helps us validate that our code meets performance expectations, compare different approaches to solving the same problem and prevent performance regressions. There are many options when it comes to benchmarking PyTorch code including the Python builtin timeit module. However, benchmarking PyTorch code has many caveats that can be easily overlooked such as managing the number of threads and synchronizing CUDA devices. Moreover, generating Tensor inputs for benchmarking can be quite tedious. This recipe demonstrates how to use PyTorch benchmark module to avoid common mistakes while making it easier to compare performance of different code, generate input for benchmarking and more. Setup# Before we begin, install torch if it isn\u2019t already available. pip install torch Steps# Defining functions to benchmark Benchmarking with timeit.Timer Benchmarking with torch.utils.benchmark.Timer Benchmarking with Blocked Autorange Comparing benchmark results Saving/Loading benchmark results Generating inputs with Fuzzed Parameters Collecting instruction counts with Callgrind 1. Defining functions to benchmark# As of the time of this writing, torch.dot does not support batched mode, so we will compare two approaches to implementing it using existing torch operators: one approach uses a combination of mul and sum while the other reduces the problem to bmm. import torch def batched_dot_mul_sum(a, b): \u0027\u0027\u0027Computes batched dot by multiplying and summing\u0027\u0027\u0027 return a.mul(b).sum(-1) def batched_dot_bmm(a, b): \u0027\u0027\u0027Computes batched dot by reducing to ``bmm``\u0027\u0027\u0027 a = a.reshape(-1, 1, a.shape[-1]) b = b.reshape(-1, b.shape[-1], 1) return torch.bmm(a, b).flatten(-3) # Input for benchmarking x = torch.randn(10000, 64) # Ensure that both functions compute the same output assert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x)) 2. Benchmarking with timeit.Timer# First, let\u2019s benchmark the code using Python\u2019s builtin timeit module. We keep the benchmark code simple here so we can compare the defaults of timeit and torch.utils.benchmark. import timeit t0 = timeit.Timer( stmt=\u0027batched_dot_mul_sum(x, x)\u0027, setup=\u0027from __main__ import batched_dot_mul_sum\u0027, globals={\u0027x\u0027: x}) t1 = timeit.Timer( stmt=\u0027batched_dot_bmm(x, x)\u0027, setup=\u0027from __main__ import batched_dot_bmm\u0027, globals={\u0027x\u0027: x}) print(f\u0027mul_sum(x, x): {t0.timeit(100) / 100 * 1e6:\u003e5.1f} us\u0027) print(f\u0027bmm(x, x): {t1.timeit(100) / 100 * 1e6:\u003e5.1f} us\u0027) mul_sum(x, x): 148.9 us bmm(x, x): 72.6 us Output# mul_sum(x, x): 111.6 us bmm(x, x): 70.0 us 3. Benchmarking with torch.utils.benchmark.Timer# PyTorch benchmark module was designed to be familiar to those who have used the timeit module before. However, its defaults make it easier and safer to use for benchmarking PyTorch code. Let\u2019s first compare the same basic API as above. import torch.utils.benchmark as benchmark t0 = benchmark.Timer( stmt=\u0027batched_dot_mul_sum(x, x)\u0027, setup=\u0027from __main__ import batched_dot_mul_sum\u0027, globals={\u0027x\u0027: x}) t1 = benchmark.Timer( stmt=\u0027batched_dot_bmm(x, x)\u0027, setup=\u0027from __main__ import batched_dot_bmm\u0027, globals={\u0027x\u0027: x}) print(t0.timeit(100)) print(t1.timeit(100)) \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fe4880638d0\u003e batched_dot_mul_sum(x, x) setup: from __main__ import batched_dot_mul_sum 287.58 us 1 measurement, 100 runs , 1 thread \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fe4867b6090\u003e batched_dot_bmm(x, x) setup: from __main__ import batched_dot_bmm 497.98 us 1 measurement, 100 runs , 1 thread Output# \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0\u003e batched_dot_mul_sum(x, x) setup: from __main__ import batched_dot_mul_sum 379.29 us 1 measurement, 100 runs , 1 thread \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb103d67048\u003e batched_dot_bmm(x, x) setup: from __main__ import batched_dot_bmm 716.42 us 1 measurement, 100 runs , 1 thread Even though the APIs are the same for the basic functionality, there are some important differences. benchmark.Timer.timeit() returns the time per run as opposed to the total runtime like timeit.Timer.timeit() does. PyTorch benchmark module also provides formatted string representations for printing the results. Another important difference, and the reason why the results diverge is that PyTorch benchmark module runs in a single thread by default. We can change the number of threads with the num_threads argument. torch.utils.benchmark.Timer takes several additional arguments including: label, sub_label, description and env which change the __repr__ of the measurement object returned and are used for grouping the results (more on this later). num_threads = torch.get_num_threads() print(f\u0027Benchmarking on {num_threads} threads\u0027) t0 = benchmark.Timer( stmt=\u0027batched_dot_mul_sum(x, x)\u0027, setup=\u0027from __main__ import batched_dot_mul_sum\u0027, globals={\u0027x\u0027: x}, num_threads=num_threads, label=\u0027Multithreaded batch dot\u0027, sub_label=\u0027Implemented using mul and sum\u0027) t1 = benchmark.Timer( stmt=\u0027batched_dot_bmm(x, x)\u0027, setup=\u0027from __main__ import batched_dot_bmm\u0027, globals={\u0027x\u0027: x}, num_threads=num_threads, label=\u0027Multithreaded batch dot\u0027, sub_label=\u0027Implemented using bmm\u0027) print(t0.timeit(100)) print(t1.timeit(100)) Benchmarking on 128 threads \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fe487c77250\u003e Multithreaded batch dot: Implemented using mul and sum setup: from __main__ import batched_dot_mul_sum 105.23 us 1 measurement, 100 runs , 128 threads \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fe487b791d0\u003e Multithreaded batch dot: Implemented using bmm setup: from __main__ import batched_dot_bmm 59.98 us 1 measurement, 100 runs , 128 threads Output# Benchmarking on 40 threads \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb103d54080\u003e Multithreaded batch dot: Implemented using mul and sum setup: from __main__ import batched_dot_mul_sum 118.47 us 1 measurement, 100 runs , 40 threads \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8\u003e Multithreaded batch dot: Implemented using bmm setup: from __main__ import batched_dot_bmm 68.21 us 1 measurement, 100 runs , 40 threads Running benchmark with all threads available gives similar results as the timeit module. More importantly, which version is faster depends on how many threads we run the code with. This is why it\u2019s important to benchmark the code with thread settings that are representative of real use cases. Another important thing to remember is to synchronize CPU and CUDA when benchmarking on the GPU. Let\u2019s run the above benchmarks again on a CUDA tensor and see what happens. x = torch.randn(10000, 1024, device=\u0027cuda\u0027) t0 = timeit.Timer( stmt=\u0027batched_dot_mul_sum(x, x)\u0027, setup=\u0027from __main__ import batched_dot_mul_sum\u0027, globals={\u0027x\u0027: x}) t1 = timeit.Timer( stmt=\u0027batched_dot_bmm(x, x)\u0027, setup=\u0027from __main__ import batched_dot_bmm\u0027, globals={\u0027x\u0027: x}) # Ran each twice to show difference before/after warm-up print(f\u0027mul_sum(x, x): {t0.timeit(100) / 100 * 1e6:\u003e5.1f} us\u0027) print(f\u0027mul_sum(x, x): {t0.timeit(100) / 100 * 1e6:\u003e5.1f} us\u0027) print(f\u0027bmm(x, x): {t1.timeit(100) / 100 * 1e6:\u003e5.1f} us\u0027) print(f\u0027bmm(x, x): {t1.timeit(100) / 100 * 1e6:\u003e5.1f} us\u0027) mul_sum(x, x): 206.7 us mul_sum(x, x): 12.7 us bmm(x, x): 980.4 us bmm(x, x): 13.8 us Output# mul_sum(x, x): 27.6 us mul_sum(x, x): 25.3 us bmm(x, x): 2775.5 us bmm(x, x): 22.4 us t0 = benchmark.Timer( stmt=\u0027batched_dot_mul_sum(x, x)\u0027, setup=\u0027from __main__ import batched_dot_mul_sum\u0027, globals={\u0027x\u0027: x}) t1 = benchmark.Timer( stmt=\u0027batched_dot_bmm(x, x)\u0027, setup=\u0027from __main__ import batched_dot_bmm\u0027, globals={\u0027x\u0027: x}) # Run only once since benchmark module does warm-up for us print(t0.timeit(100)) print(t1.timeit(100)) \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fe487c262d0\u003e batched_dot_mul_sum(x, x) setup: from __main__ import batched_dot_mul_sum 34.80 us 1 measurement, 100 runs , 1 thread \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fe4867be450\u003e batched_dot_bmm(x, x) setup: from __main__ import batched_dot_bmm 23.34 us 1 measurement, 100 runs , 1 thread Output# \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080\u003e batched_dot_mul_sum(x, x) setup: from __main__ import batched_dot_mul_sum 232.93 us 1 measurement, 100 runs , 1 thread \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0\u003e batched_dot_bmm(x, x) setup: from __main__ import batched_dot_bmm 181.04 us 1 measurement, 100 runs , 1 thread The results reveal something interesting. The first run of the bmm version using the timeit module takes much longer than the second run. This is because bmm calls into cuBLAS which needs to be loaded the first time it\u2019s called which takes some time. This is why it\u2019s important to do a warm-up run before benchmarking, luckily for us, PyTorch\u2019s benchmark module takes care of that. The difference in the results between timeit and benchmark modules is because the timeit module is not synchronizing CUDA and is thus only timing the time to launch the kernel. PyTorch\u2019s benchmark module does the synchronization for us. 4. Benchmarking with Blocked Autorange# While timeit.Timer.autorange takes a single continuous measurement of at least 0.2 seconds, torch.utils.benchmark.blocked_autorange takes many measurements whose times total at least 0.2 seconds (which can be changed by the min_run_time parameter) subject to the constraint that timing overhead is a small fraction of the overall measurement. This is accomplished by first running with an increasing number of runs per loop until the runtime is much larger than measurement overhead (which also serves as a warm up), and then taking measurements until the target time is reached. This has the useful properties that it wastes less data and allows us to compute statistics to estimate the reliability of the measurements. m0 = t0.blocked_autorange() m1 = t1.blocked_autorange() print(m0) print(m1) \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fe4867be450\u003e batched_dot_mul_sum(x, x) setup: from __main__ import batched_dot_mul_sum 34.70 us 1 measurement, 10000 runs , 1 thread \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fe487bbd250\u003e batched_dot_bmm(x, x) setup: from __main__ import batched_dot_bmm 23.29 us 1 measurement, 10000 runs , 1 thread Output# \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0\u003e batched_dot_mul_sum(x, x) setup: from __main__ import batched_dot_mul_sum 231.79 us 1 measurement, 1000 runs , 1 thread \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080\u003e batched_dot_bmm(x, x) setup: from __main__ import batched_dot_bmm Median: 162.08 us 2 measurements, 1000 runs per measurement, 1 thread We can also inspect the individual statistics from the returned measurements object. print(f\"Mean: {m0.mean * 1e6:6.2f} us\") print(f\"Median: {m0.median * 1e6:6.2f} us\") Mean: 34.70 us Median: 34.70 us Output# Mean: 231.79 us Median: 231.79 us 5. Comparing benchmark results# So far we\u2019ve been comparing our two versions of batched dot against a single input. In practice, we want to try a combination of inputs as well as different number of threads. The Compare class helps display the results of many measurements in a formatted table. It uses the annotations described above (label, sub_label, num_threads, etc.) as well as description to group and organize the table. Let\u2019s use Compare to see how our functions perform for different input sizes and number of threads. from itertools import product # Compare takes a list of measurements which we\u0027ll save in results. results = [] sizes = [1, 64, 1024, 10000] for b, n in product(sizes, sizes): # label and sub_label are the rows # description is the column label = \u0027Batched dot\u0027 sub_label = f\u0027[{b}, {n}]\u0027 x = torch.ones((b, n)) for num_threads in [1, 4, 16, 32]: results.append(benchmark.Timer( stmt=\u0027batched_dot_mul_sum(x, x)\u0027, setup=\u0027from __main__ import batched_dot_mul_sum\u0027, globals={\u0027x\u0027: x}, num_threads=num_threads, label=label, sub_label=sub_label, description=\u0027mul/sum\u0027, ).blocked_autorange(min_run_time=1)) results.append(benchmark.Timer( stmt=\u0027batched_dot_bmm(x, x)\u0027, setup=\u0027from __main__ import batched_dot_bmm\u0027, globals={\u0027x\u0027: x}, num_threads=num_threads, label=label, sub_label=sub_label, description=\u0027bmm\u0027, ).blocked_autorange(min_run_time=1)) compare = benchmark.Compare(results) compare.print() [--------------- Batched dot ----------------] | mul/sum | bmm 1 threads: ----------------------------------- [1, 1] | 3.3 | 5.5 [1, 64] | 3.4 | 5.7 [1, 1024] | 3.6 | 5.9 [1, 10000] | 5.1 | 6.9 [64, 1] | 3.7 | 6.1 [64, 64] | 4.7 | 9.0 [64, 1024] | 16.3 | 178.8 [64, 10000] | 277.9 | 1684.8 [1024, 1] | 4.4 | 11.1 [1024, 64] | 18.0 | 55.4 [1024, 1024] | 468.8 | 2769.2 [1024, 10000] | 14630.1 | 26818.0 [10000, 1] | 10.2 | 57.6 [10000, 64] | 296.7 | 495.0 [10000, 1024] | 14513.2 | 27164.9 [10000, 10000] | 185983.9 | 264568.8 4 threads: ----------------------------------- [1, 1] | 3.4 | 5.5 [1, 64] | 3.5 | 5.8 [1, 1024] | 3.6 | 5.9 [1, 10000] | 5.0 | 6.9 [64, 1] | 3.7 | 6.1 [64, 64] | 4.9 | 9.1 [64, 1024] | 45.8 | 223.9 [64, 10000] | 37.6 | 2033.7 [1024, 1] | 4.5 | 11.2 [1024, 64] | 67.7 | 34.5 [1024, 1024] | 74.5 | 715.4 [1024, 10000] | 7078.1 | 6833.2 [10000, 1] | 10.3 | 57.6 [10000, 64] | 60.1 | 151.0 [10000, 1024] | 7173.4 | 6783.7 [10000, 10000] | 77143.6 | 65605.9 16 threads: ---------------------------------- [1, 1] | 3.5 | 5.6 [1, 64] | 3.5 | 5.7 [1, 1024] | 3.6 | 5.9 [1, 10000] | 5.0 | 6.9 [64, 1] | 3.8 | 6.1 [64, 64] | 4.8 | 9.0 [64, 1024] | 37.7 | 303.5 [64, 10000] | 22.5 | 3053.1 [1024, 1] | 4.6 | 11.2 [1024, 64] | 38.0 | 36.0 [1024, 1024] | 27.6 | 209.2 [1024, 10000] | 3499.9 | 1867.4 [10000, 1] | 10.3 | 57.5 [10000, 64] | 31.5 | 50.0 [10000, 1024] | 3745.5 | 1752.8 [10000, 10000] | 37417.4 | 16803.8 32 threads: ---------------------------------- [1, 1] | 3.5 | 5.5 [1, 64] | 3.4 | 5.7 [1, 1024] | 3.7 | 5.9 [1, 10000] | 5.0 | 6.9 [64, 1] | 3.8 | 6.1 [64, 64] | 4.9 | 9.0 [64, 1024] | 49.9 | 306.9 [64, 10000] | 68.8 | 2841.5 [1024, 1] | 4.6 | 11.2 [1024, 64] | 40.6 | 37.5 [1024, 1024] | 28.8 | 129.1 [1024, 10000] | 3668.5 | 1312.2 [10000, 1] | 10.3 | 57.6 [10000, 64] | 84.4 | 46.3 [10000, 1024] | 3548.7 | 907.7 [10000, 10000] | 36430.9 | 8427.9 Times are in microseconds (us). Output# [--------------- Batched dot ----------------] | mul/sum | bmm 1 threads: ----------------------------------- [1, 1] | 5.9 | 11.2 [1, 64] | 6.4 | 11.4 [1, 1024] | 6.7 | 14.2 [1, 10000] | 10.2 | 23.7 [64, 1] | 6.3 | 11.5 [64, 64] | 8.6 | 15.4 [64, 1024] | 39.4 | 204.4 [64, 10000] | 274.9 | 748.5 [1024, 1] | 7.7 | 17.8 [1024, 64] | 40.3 | 76.4 [1024, 1024] | 432.4 | 2795.9 [1024, 10000] | 22657.3 | 11899.5 [10000, 1] | 16.9 | 74.8 [10000, 64] | 300.3 | 609.4 [10000, 1024] | 23098.6 | 27246.1 [10000, 10000] | 267073.7 | 118823.7 4 threads: ----------------------------------- [1, 1] | 6.0 | 11.5 [1, 64] | 6.2 | 11.2 [1, 1024] | 6.8 | 14.3 [1, 10000] | 10.2 | 23.7 [64, 1] | 6.3 | 16.2 [64, 64] | 8.8 | 18.2 [64, 1024] | 41.5 | 189.1 [64, 10000] | 91.7 | 849.1 [1024, 1] | 7.6 | 17.4 [1024, 64] | 43.5 | 33.5 [1024, 1024] | 135.4 | 2782.3 [1024, 10000] | 7471.1 | 11874.0 [10000, 1] | 16.8 | 33.9 [10000, 64] | 118.7 | 173.2 [10000, 1024] | 7264.6 | 27824.7 [10000, 10000] | 100060.9 | 121499.0 16 threads: ---------------------------------- [1, 1] | 6.0 | 11.3 [1, 64] | 6.2 | 11.2 [1, 1024] | 6.9 | 14.2 [1, 10000] | 10.3 | 23.8 [64, 1] | 6.4 | 24.1 [64, 64] | 9.0 | 23.8 [64, 1024] | 54.1 | 188.5 [64, 10000] | 49.9 | 748.0 [1024, 1] | 7.6 | 23.4 [1024, 64] | 55.5 | 28.2 [1024, 1024] | 66.9 | 2773.9 [1024, 10000] | 6111.5 | 12833.7 [10000, 1] | 16.9 | 27.5 [10000, 64] | 59.5 | 73.7 [10000, 1024] | 6295.9 | 27062.0 [10000, 10000] | 71804.5 | 120365.8 32 threads: ---------------------------------- [1, 1] | 5.9 | 11.3 [1, 64] | 6.2 | 11.3 [1, 1024] | 6.7 | 14.2 [1, 10000] | 10.5 | 23.8 [64, 1] | 6.3 | 31.7 [64, 64] | 9.1 | 30.4 [64, 1024] | 72.0 | 190.4 [64, 10000] | 103.1 | 746.9 [1024, 1] | 7.6 | 28.4 [1024, 64] | 70.5 | 31.9 [1024, 1024] | 65.6 | 2804.6 [1024, 10000] | 6764.0 | 11871.4 [10000, 1] | 17.8 | 31.8 [10000, 64] | 110.3 | 56.0 [10000, 1024] | 6640.2 | 27592.2 [10000, 10000] | 73003.4 | 120083.2 Times are in microseconds (us). The results above indicate that the version which reduces to bmm is better for larger tensors running on multiple threads, while for smaller and/or single thread code, the other version is better. Compare also provides functions for changing the table format compare.trim_significant_figures() compare.colorize() compare.print() [-------------- Batched dot --------------] | mul/sum | bmm 1 threads: -------------------------------- [1, 1] | 3 | 6 [1, 64] | 3 | 6 [1, 1024] | 4 | 6 [1, 10000] | 5 | 7 [64, 1] | 4 | 6 [64, 64] | 5 | 9 [64, 1024] | 16 | 179 [64, 10000] | 278 | 1680 [1024, 1] | 4 | 11 [1024, 64] | 18 | 55 [1024, 1024] | 469 | 2769 [1024, 10000] | 14600 | 26820 [10000, 1] | 10 | 58 [10000, 64] | 297 | 490 [10000, 1024] | 14500 | 27160 [10000, 10000] | 190000 | 264600 4 threads: -------------------------------- [1, 1] | 3 | 6 [1, 64] | 4 | 6 [1, 1024] | 4 | 6 [1, 10000] | 5 | 7 [64, 1] | 4 | 6 [64, 64] | 5 | 9 [64, 1024] | 46 | 220 [64, 10000] | 38 | 2000 [1024, 1] | 4 | 11 [1024, 64] | 68 | 35 [1024, 1024] | 74 | 715 [1024, 10000] | 7100 | 6833 [10000, 1] | 10 | 58 [10000, 64] | 60 | 200 [10000, 1024] | 7200 | 6784 [10000, 10000] | 77000 | 65600 16 threads: ------------------------------- [1, 1] | 4 | 6 [1, 64] | 4 | 6 [1, 1024] | 4 | 6 [1, 10000] | 5 | 7 [64, 1] | 4 | 6 [64, 64] | 5 | 9 [64, 1024] | 40 | 303 [64, 10000] | 22 | 3100 [1024, 1] | 5 | 11 [1024, 64] | 38 | 36 [1024, 1024] | 28 | 209 [1024, 10000] | 3000 | 1900 [10000, 1] | 10 | 58 [10000, 64] | 32 | 50 [10000, 1024] | 4000 | 1800 [10000, 10000] | 40000 | 20000 32 threads: ------------------------------- [1, 1] | 4 | 6 [1, 64] | 3 | 6 [1, 1024] | 4 | 6 [1, 10000] | 5 | 7 [64, 1] | 4 | 6 [64, 64] | 5 | 9 [64, 1024] | 50 | 307 [64, 10000] | 69 | 2840 [1024, 1] | 5 | 11 [1024, 64] | 41 | 38 [1024, 1024] | 29 | 129 [1024, 10000] | 3700 | 1310 [10000, 1] | 10 | 58 [10000, 64] | 84 | 50 [10000, 1024] | 4000 | 910 [10000, 10000] | 36000 | 8430 Times are in microseconds (us). 6. Saving/Loading benchmark results# Measurements (and CallgrindStats which are described in section 8) can be serialized by the pickle module. This makes A/B testing easy, as you can collect measurements from two separate environments, pickle them, and then load both in a single environment. Timer even takes an env constructor argument so that such A/B testing works seamlessly. Let\u2019s imagine that rather than two Python functions, the add/sum and bmm approaches were in two different builds of PyTorch. The example below demonstrates how one might A/B test them. For simplicity, we only use a subset of shapes, and simply round trip results through pickle rather than actually using multiple environments and writing results to disk. import pickle ab_test_results = [] for env in (\u0027environment A: mul/sum\u0027, \u0027environment B: bmm\u0027): for b, n in ((1, 1), (1024, 10000), (10000, 1)): x = torch.ones((b, n)) dot_fn = (batched_dot_mul_sum if env == \u0027environment A: mul/sum\u0027 else batched_dot_bmm) m = benchmark.Timer( stmt=\u0027batched_dot(x, x)\u0027, globals={\u0027x\u0027: x, \u0027batched_dot\u0027: dot_fn}, num_threads=1, label=\u0027Batched dot\u0027, description=f\u0027[{b}, {n}]\u0027, env=env, ).blocked_autorange(min_run_time=1) ab_test_results.append(pickle.dumps(m)) ab_results = [pickle.loads(i) for i in ab_test_results] compare = benchmark.Compare(ab_results) compare.trim_significant_figures() compare.colorize() compare.print() [------------------------------------- Batched dot -------------------------------------] | [1, 1] | [1024, 10000] | [10000, 1] 1 threads: ------------------------------------------------------------------------------ (environment A: mul/sum) batched_dot(x, x) | 3.4 | 14100 | 10 (environment B: bmm) batched_dot(x, x) | 5.4 | 27100 | 58 Times are in microseconds (us). Output# [------------------------------------- Batched dot -------------------------------------] | [1, 1] | [1024, 10000] | [10000, 1] 1 threads: ------------------------------------------------------------------------------ (environment A: mul/sum) batched_dot(x, x) | 7 | 36000 | 21 (environment B: bmm) batched_dot(x, x) | 14 | 40000 | 85 Times are in microseconds (us). # And just to show that we can round trip all of the results from earlier: round_tripped_results = pickle.loads(pickle.dumps(results)) assert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results))) 7. Generating inputs with Fuzzed Parameters# As we\u2019ve seen in the previous section, there can be some stark performance differences depending on the input tensors. Hence, it is a good idea to run benchmarks on a number of different inputs. However, creating all these input tensors can be tedious which is where torch.utils.benchmark.Fuzzer and related classes come in. Let\u2019s take a look at how we can use the Fuzzer to create some test cases for the benchmark. from torch.utils.benchmark import Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias # Generates random tensors with 128 to 10000000 elements and sizes k0 and k1 chosen from a # ``loguniform`` distribution in [1, 10000], 40% of which will be discontiguous on average. example_fuzzer = Fuzzer( parameters = [ FuzzedParameter(\u0027k0\u0027, minval=1, maxval=10000, distribution=\u0027loguniform\u0027), FuzzedParameter(\u0027k1\u0027, minval=1, maxval=10000, distribution=\u0027loguniform\u0027), ], tensors = [ FuzzedTensor(\u0027x\u0027, size=(\u0027k0\u0027, \u0027k1\u0027), min_elements=128, max_elements=10000000, probability_contiguous=0.6) ], seed=0, ) results = [] for tensors, tensor_params, params in example_fuzzer.take(10): # description is the column label sub_label=f\"{params[\u0027k0\u0027]:\u003c6} x {params[\u0027k1\u0027]:\u003c4} {\u0027\u0027 if tensor_params[\u0027x\u0027][\u0027is_contiguous\u0027] else \u0027(discontiguous)\u0027}\" results.append(benchmark.Timer( stmt=\u0027batched_dot_mul_sum(x, x)\u0027, setup=\u0027from __main__ import batched_dot_mul_sum\u0027, globals=tensors, label=\u0027Batched dot\u0027, sub_label=sub_label, description=\u0027mul/sum\u0027, ).blocked_autorange(min_run_time=1)) results.append(benchmark.Timer( stmt=\u0027batched_dot_bmm(x, x)\u0027, setup=\u0027from __main__ import batched_dot_bmm\u0027, globals=tensors, label=\u0027Batched dot\u0027, sub_label=sub_label, description=\u0027bmm\u0027, ).blocked_autorange(min_run_time=1)) compare = benchmark.Compare(results) compare.trim_significant_figures() compare.print() [--------------------- Batched dot ---------------------] | mul/sum | bmm 1 threads: ---------------------------------------------- 725 x 257 | 39 | 148 49 x 383 | 7 | 20 34 x 1468 | 12 | 138 187 x 5039 | 433 | 2490 2140 x 1296 (discontiguous) | 1240 | 20100 78 x 1598 | 25 | 337 519 x 763 | 144 | 1058 141 x 1082 | 29 | 410 78 x 5 (discontiguous) | 4 | 8 187 x 1 | 4 | 7 Times are in microseconds (us). Output# [--------------------- Batched dot ---------------------] | mul/sum | bmm 1 threads: ---------------------------------------------- 725 x 257 | 87 | 180 49 x 383 | 15 | 30 34 x 1468 | 30 | 118 187 x 5039 | 400 | 1200 2140 x 1296 (discontiguous) | 2000 | 41000 78 x 1598 | 74 | 310 519 x 763 | 190 | 1500 141 x 1082 | 87 | 500 78 x 5 (discontiguous) | 9 | 20 187 x 1 | 12 | 10 Times are in microseconds (us). There is a lot of flexibility for defining your own fuzzers which is great for creating a powerful set of inputs to benchmark. But to make things even simpler, PyTorch benchmark module comes with some built-in fuzzers for common benchmarking needs. Let\u2019s take a look at how we can use one of these built-in fuzzers. from torch.utils.benchmark.op_fuzzers import binary results = [] for tensors, tensor_params, params in binary.BinaryOpFuzzer(seed=0).take(10): sub_label=f\"{params[\u0027k0\u0027]:\u003c6} x {params[\u0027k1\u0027]:\u003c4} {\u0027\u0027 if tensor_params[\u0027x\u0027][\u0027is_contiguous\u0027] else \u0027(discontiguous)\u0027}\" results.append(benchmark.Timer( stmt=\u0027batched_dot_mul_sum(x, x)\u0027, setup=\u0027from __main__ import batched_dot_mul_sum\u0027, globals=tensors, label=\u0027Batched dot\u0027, sub_label=sub_label, description=\u0027mul/sum\u0027, ).blocked_autorange(min_run_time=1)) results.append(benchmark.Timer( stmt=\u0027batched_dot_bmm(x, x)\u0027, setup=\u0027from __main__ import batched_dot_bmm\u0027, globals=tensors, label=\u0027Batched dot\u0027, sub_label=sub_label, description=\u0027bmm\u0027, ).blocked_autorange(min_run_time=1)) compare = benchmark.Compare(results) compare.trim_significant_figures() compare.colorize(rowwise=True) compare.print() [----------------------- Batched dot ------------------------] | mul/sum | bmm 1 threads: --------------------------------------------------- 64 x 473 (discontiguous) | 20880 | 43000 16384 x 12642115 (discontiguous) | 15 | 65 8192 x 892 | 3280 | 19340 512 x 64 (discontiguous) | 64000 | 190000 493 x 27 (discontiguous) | 1328 | 2808 118 x 32 (discontiguous) | 783 | 2200 16 x 495 (discontiguous) | 5790 | 28000 488 x 62374 | 50000 | 80000 240372 x 69 | 23200 | 13000 40156 x 32 (discontiguous) | 743 | 2050 Times are in microseconds (us). Output# [----------------------- Batched dot ------------------------] | mul/sum | bmm 1 threads: --------------------------------------------------- 64 x 473 (discontiguous) | 10000 | 40000 16384 x 12642115 (discontiguous) | 31 | 78 8192 x 892 | 4800 | 20400 512 x 64 (discontiguous) | 110000 | 400000 493 x 27 (discontiguous) | 1100 | 2440 118 x 32 (discontiguous) | 870 | 2030 16 x 495 (discontiguous) | 23600 | 24000 488 x 62374 | 90000 | 100000 240372 x 69 | 40000 | 16000 40156 x 32 (discontiguous) | 2670 | 5000 Times are in microseconds (us). 8. Collecting instruction counts with Callgrind# One of the challenges of optimizing code is the variation and opacity of wall time. There are many sources of non-determinism, from adaptive clock speeds to resource contention with other processes. Furthermore, end-to-end time gives no insight into where time is being spent, which is really what we\u2019re interested in when optimizing code. A complementary approach is to also collect instruction counts. These counts are a proxy metric and do not capture all aspects of performance (e.g. memory or I/O bound tasks), however they do have several useful properties. Instruction counts are reproducible, insensitive to environmental variation, and offer fine grained insight into where a program is spending cycles. To see the utility of instruction counts, let us look at how we might reduce the overhead of batched_dot_mul_sum. The obvious solution is to move it to C++, so we avoid going between Python and C++ multiple times. Fortunately, the source is nearly identical. One question that we have to ask in C++ is whether we should take arguments by value or reference. batched_dot_src = \"\"\"\\ /* ---- Python ---- */ // def batched_dot_mul_sum(a, b): // return a.mul(b).sum(-1) torch::Tensor batched_dot_mul_sum_v0( const torch::Tensor a, const torch::Tensor b) { return a.mul(b).sum(-1); } torch::Tensor batched_dot_mul_sum_v1( const torch::Tensor\u0026 a, const torch::Tensor\u0026 b) { return a.mul(b).sum(-1); } \"\"\" # PyTorch makes it easy to test our C++ implementations by providing a utility # to JIT compile C++ source into Python extensions: import os from torch.utils import cpp_extension cpp_lib = cpp_extension.load_inline( name=\u0027cpp_lib\u0027, cpp_sources=batched_dot_src, extra_cflags=[\u0027-O3\u0027], extra_include_paths=[ # `load_inline` needs to know where to find ``pybind11`` headers. os.path.join(os.getenv(\u0027CONDA_PREFIX\u0027), \u0027include\u0027) ], functions=[\u0027batched_dot_mul_sum_v0\u0027, \u0027batched_dot_mul_sum_v1\u0027] ) # `load_inline` will create a shared object that is loaded into Python. When we collect # instruction counts Timer will create a subprocess, so we need to re-import it. The # import process is slightly more complicated for C extensions, but that\u0027s all we\u0027re # doing here. module_import_str = f\"\"\"\\ # https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path import importlib.util spec = importlib.util.spec_from_file_location(\"cpp_lib\", {repr(cpp_lib.__file__)}) cpp_lib = importlib.util.module_from_spec(spec) spec.loader.exec_module(cpp_lib)\"\"\" import textwrap def pretty_print(result): \"\"\"Import machinery for ``cpp_lib.so`` can get repetitive to look at.\"\"\" print(repr(result).replace(textwrap.indent(module_import_str, \" \"), \" import cpp_lib\")) t_baseline = benchmark.Timer( stmt=\u0027batched_dot_mul_sum(x, x)\u0027, setup=\u0027\u0027\u0027\\ from __main__ import batched_dot_mul_sum x = torch.randn(2, 2)\u0027\u0027\u0027) t0 = benchmark.Timer( stmt=\u0027cpp_lib.batched_dot_mul_sum_v0(x, x)\u0027, setup=f\u0027\u0027\u0027\\ {module_import_str} x = torch.randn(2, 2)\u0027\u0027\u0027) t1 = benchmark.Timer( stmt=\u0027cpp_lib.batched_dot_mul_sum_v1(x, x)\u0027, setup=f\u0027\u0027\u0027\\ {module_import_str} x = torch.randn(2, 2)\u0027\u0027\u0027) # Moving to C++ did indeed reduce overhead, but it\u0027s hard to tell which # calling convention is more efficient. v1 (call with references) seems to # be a bit faster, but it\u0027s within measurement error. pretty_print(t_baseline.blocked_autorange()) pretty_print(t0.blocked_autorange()) pretty_print(t1.blocked_autorange()) Traceback (most recent call last): File \"/workspace/tutorials-kr/recipes_source/recipes/benchmark.py\", line 727, in \u003cmodule\u003e os.path.join(os.getenv(\u0027CONDA_PREFIX\u0027), \u0027include\u0027) File \"\u003cfrozen posixpath\u003e\", line 76, in join TypeError: expected str, bytes or os.PathLike object, not NoneType Output# \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8\u003e batched_dot_mul_sum(x, x) setup: from __main__ import batched_dot_mul_sum x = torch.randn(2, 2) 6.92 us 1 measurement, 100000 runs , 1 thread \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8\u003e cpp_lib.batched_dot_mul_sum_v0(x, x) setup: import cpp_lib x = torch.randn(2, 2) 5.29 us 1 measurement, 100000 runs , 1 thread \u003ctorch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8\u003e cpp_lib.batched_dot_mul_sum_v1(x, x) setup: import cpp_lib x = torch.randn(2, 2) 5.22 us 1 measurement, 100000 runs , 1 thread # Let\u0027s use ``Callgrind`` to determine which is better. stats_v0 = t0.collect_callgrind() stats_v1 = t1.collect_callgrind() pretty_print(stats_v0) pretty_print(stats_v1) # `.as_standardized` removes file names and some path prefixes, and makes # it easier to read the function symbols. stats_v0 = stats_v0.as_standardized() stats_v1 = stats_v1.as_standardized() # `.delta` diffs the instruction counts, and `.denoise` removes several # functions in the Python interpreter that are known to have significant # jitter. delta = stats_v1.delta(stats_v0).denoise() # `.transform` is a convenience API for transforming function names. It is # useful for increasing cancelation when ``diff-ing`` instructions, as well as # just generally improving readability. replacements = ( (\"???:void pybind11\", \"pybind11\"), (\"batched_dot_mul_sum_v0\", \"batched_dot_mul_sum_v1\"), (\"at::Tensor, at::Tensor\", \"...\"), (\"at::Tensor const\u0026, at::Tensor const\u0026\", \"...\"), (\"auto torch::detail::wrap_pybind_function_impl_\", \"wrap_pybind_function_impl_\"), ) for before, after in replacements: delta = delta.transform(lambda l: l.replace(before, after)) # We can use print options to control how much of the function to display. torch.set_printoptions(linewidth=160) # Once parsed, the instruction counts make clear that passing `a` and `b` # by reference is more efficient as it skips some ``c10::TensorImpl`` bookkeeping # for the intermediate Tensors, and is also works better with ``pybind11``. This # is consistent with our noisy wall time observations. print(delta) \u003ctorch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb0f06e7630\u003e cpp_lib.batched_dot_mul_sum_v0(x, x) setup: import cpp_lib x = torch.randn(2, 2) All Noisy symbols removed Instructions: 2392671 2392671 Baseline: 4367 4367 100 runs per measurement, 1 thread Warning: PyTorch was not built with debug symbols. Source information may be limited. Rebuild with REL_WITH_DEB_INFO=1 for more detailed results. \u003ctorch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb10400d208\u003e cpp_lib.batched_dot_mul_sum_v1(x, x) setup: import cpp_lib x = torch.randn(2, 2) All Noisy symbols removed Instructions: 2378978 2378978 Baseline: 4367 4367 100 runs per measurement, 1 thread Warning: PyTorch was not built with debug symbols. Source information may be limited. Rebuild with REL_WITH_DEB_INFO=1 for more detailed results. \u003ctorch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7fb1000ab358\u003e 86 ???:0x000000000020d9e0 56 ???:0x000000000020db10 -1100 pybind11::cpp_function::initialize\u003cwrap_pybind_function_impl_\u003cat::Tensor ... r (\u0026)(...), std::integer_sequence\u003cunsigned long, 0ul, 1ul\u003e)::{lambda(...) -1600 ???:wrap_pybind_function_impl_\u003cat::Tensor (\u0026)(...), 0ul, 1ul\u003e(at::Tensor (\u0026)(...), std::integer_sequence\u003cunsigned long, 0ul, 1ul\u003e)::{lambda(...) -5200 ???:c10::intrusive_ptr\u003cc10::TensorImpl, c10::UndefinedTensorImpl\u003e::reset_() -5935 ???:0x000000000022c0e0 Total: -13693 Learn More# Take a look at these other recipes to continue your learning: PyTorch Profiler Total running time of the script: (4 minutes 18.013 seconds) Download Jupyter notebook: benchmark.ipynb Download Python source code: benchmark.py Download zipped: benchmark.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/recipes/recipes/benchmark.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>