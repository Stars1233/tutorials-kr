
<!DOCTYPE html>


<html lang="ko" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="article:modified_time" content="2024-06-09T15:35:40+00:00" /><meta property="og:title" content="Recurrent DQN: Training recurrent policies" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/intermediate/dqn_with_rnn_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Author: Vincent Moens What you will learn How to incorporating an RNN in an actor in TorchRL, How to use that memory-based policy with a replay buffer and a loss module. Prerequisites PyTorch v2.0.0, gym[mujoco], tqdm. Overview: Memory-based policies are crucial not only when the observations are..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Author: Vincent Moens What you will learn How to incorporating an RNN in an actor in TorchRL, How to use that memory-based policy with a replay buffer and a loss module. Prerequisites PyTorch v2.0.0, gym[mujoco], tqdm. Overview: Memory-based policies are crucial not only when the observations are..." />
<meta property="og:ignore_canonical" content="true" />

    <title>Recurrent DQN: Training recurrent policies &#8212; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=536c50fe" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=4d2595e5" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/pytorch_theme.css?v=c326296f" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=097efa9c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom2.css?v=b169ea90" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=725a5b95"></script>
    <script src="../_static/doctools.js?v=92e14aea"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/translations.js?v=b5f768d8"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intermediate/dqn_with_rnn_tutorial';</script>
    <link rel="canonical" href="https://tutorials.pytorch.kr/intermediate/dqn_with_rnn_tutorial.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="색인" href="../genindex.html" />
    <link rel="search" title="검색" href="../search.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2024년 06월 09일"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->

<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<script type="text/javascript" src="../_static/js/dropdown-menu.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&family=Nanum+Gothic+Coding:wght@400;700&family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="../_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="tutorials">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LZRD6GXDLF" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'G-LZRD6GXDLF');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'v2.8.0+cu128');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="ko"/>
    <meta name="docbuild:last-update" content="2024년 06월 09일"/>

  </head>

<body data-feedback-url="https://github.com/PyTorchKorea/tutorials-kr" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorchKR"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="learnDropdownButton" data-toggle="learn-dropdown" class="learn-dropdown">
              <a class="with-down-arrow">
                <span>배우기</span>
              </a>
              <div class="learn-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.kr/get-started/locally/">
                  <span class="dropdown-title">PyTorch 시작하기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/beginner/basics/intro.html">
                  <span class="dropdown-title">기본 익히기</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/hub/">
                  <span class="dropdown-title">한국어 모델 허브</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <a href="https://pytorch.kr/blog/">
              <span>블로그</span>
            </a>
          </li>

          <li class="main-menu-item">
          <div id="docsDropdownButton" data-toggle="docs-dropdown" class="docs-dropdown">
              <a class="with-down-arrow">
              <span>문서</span>
              </a>
              <div class="docs-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/" target="_blank">
                  <span class="dropdown-title">PyTorch API</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/domains/">
                  <span class="dropdown-title">Domain API 소개</span>
                </a>
                <a class="nav-dropdown-item" href="https://tutorials.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 튜토리얼</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials/" target="_blank">
                  <span class="dropdown-title">Official Tutorials</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="communityDropdownButton" data-toggle="community-dropdown" class="community-dropdown">
              <a class="with-down-arrow">
              <span>커뮤니티</span>
              </a>
              <div class="community-dropdown-menu dropdown-menu">
                <a class="nav-dropdown-item" href="https://discuss.pytorch.kr/" target="_self">
                  <span class="dropdown-title">한국어 커뮤니티</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.kr/resources/">
                  <span class="dropdown-title">개발자 정보</span>
                </a>
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
              </div>
            </div>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
          <i class="fa-solid fa-xmark"></i>
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>배우기</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.kr/get-started/locally/">PyTorch 시작하기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/beginner/basics/intro.html">기본 익히기</a>
           </li>
           <li>
             <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
           </li>
           <li>
             <a href="https://pytorch.kr/hub/">한국어 모델 허브</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
           </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a href="https://pytorch.kr/blog/">블로그</a>
         </li>
         <li class="resources-mobile-menu-title">
           <a>문서</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/" target="_blank">PyTorch API</a>
          </li>
          <li>
            <a href="https://pytorch.kr/domains/">Domain API 소개</a>
          </li>
          <li>
            <a href="https://tutorials.pytorch.kr/">한국어 튜토리얼</a>
          </li>
          <li>
            <a href="https://docs.pytorch.org/tutorials/" target="_blank">Official Tutorials</a>
          </li>
        </ul>
        <li class="resources-mobile-menu-title">
          <a>커뮤니티</a>
        </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://discuss.pytorch.kr/">한국어 커뮤니티</a>
          </li>
          <li>
            <a href="https://pytorch.kr/resources/">개발자 정보</a>
          </li>
          <li>
            <a href="https://landscape.pytorch.org/" target="_blank">Landscape</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">v2.8.0+cu128</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../intro.html">
    Intro
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../compilers_index.html">
    Compilers
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../domains.html">
    Domains
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../distributed.html">
    Distributed
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../deep-dive.html">
    Deep Dive
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../extension.html">
    Extension
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../ecosystem.html">
    Ecosystem
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../recipes_index.html">
    Recipes
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<div class="search-container-wrapper">
  <div id="sphinx-search" class="search-container">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </div>

  <div id="google-search" class="search-container" style="display:none;">
    <div class="gcse-search-wrapper">
      <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
      <div class="gcse-search"></div>
    </div>
  </div>

  <div class="search-toggle-container" data-bs-title="Google Search Off" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <div class="search-toggle-inner">
      <label class="switch">
        <input type="checkbox" id="search-toggle">
        <span class="slider round"></span>
      </label>
    </div>
  </div>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
  // Define the search callback
  const myWebSearchStartingCallback = (gname, query) => {
    if (typeof dataLayer !== 'undefined' && query) {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'google_search',
        'search_term': query,
        'event_category': 'Search',
        'event_label': 'Google Search'
      });
    }
    return '';
  };

  // Set up the GCSE search callbacks
  window.__gcse || (window.__gcse = {});
  window.__gcse.searchCallbacks = {
    web: { starting: myWebSearchStartingCallback }
  };

  if (window.location.pathname.includes('/search.html')) {
    document.body.classList.add('search-page');
  }

  // Function to reinitialize Google CSE
  function reinitializeGoogleSearch() {
    if (window.__gcse && window.__gcse.initializationCallback) {
      window.__gcse.initializationCallback();
    }
  }

  // Function to handle search toggle
  function handleSearchToggle(toggle, sphinxSearch, googleSearch) {
    if (!toggle || !sphinxSearch || !googleSearch) return;

    // Check if the URL contains /stable/ or /tutorials/
    const currentUrl = window.location.href;
    // TODO: We've had reports that the google programmable search is returning stale documentation,
    //       Simple reproduction is to turn google search on and search for multinomial which will
    //       result in returning 1.8.1 documentation.
    //       We should turn this back on when we resolve that bug.
    const shouldDefaultToGoogle = false;
    const savedPreference = localStorage.getItem('searchPreference');

    // Set initial state
    if (savedPreference === 'google' || (savedPreference === null && shouldDefaultToGoogle)) {
      toggle.checked = true;
      sphinxSearch.style.display = 'none';
      googleSearch.style.display = 'block';
      if (savedPreference === null) {
        localStorage.setItem('searchPreference', 'google');
      }
      reinitializeGoogleSearch();
    } else {
      toggle.checked = false;
      sphinxSearch.style.display = 'block';
      googleSearch.style.display = 'none';
    }

    // Update tooltip
    updateTooltip(toggle.checked);

    // Skip if already initialized
    if (toggle.hasAttribute('data-initialized')) return;
    toggle.setAttribute('data-initialized', 'true');

    toggle.addEventListener('change', function() {
      if (this.checked) {
        sphinxSearch.style.display = 'none';
        googleSearch.style.display = 'block';
        localStorage.setItem('searchPreference', 'google');
        reinitializeGoogleSearch();
        trackSearchEngineSwitch('Google');
      } else {
        sphinxSearch.style.display = 'block';
        googleSearch.style.display = 'none';
        localStorage.setItem('searchPreference', 'sphinx');
        trackSearchEngineSwitch('Sphinx');
      }

      updateTooltip(this.checked);
      updateMobileSearch();
    });
  }

  // Update tooltip based on toggle state
  function updateTooltip(isChecked) {
    const tooltipElement = document.querySelector('.search-toggle-container');
    if (!tooltipElement) return;

    tooltipElement.setAttribute('data-bs-title', isChecked ? 'Google Search On' : 'Google Search Off');

    if (bootstrap && bootstrap.Tooltip) {
      const tooltipInstance = bootstrap.Tooltip.getInstance(tooltipElement);
      if (tooltipInstance) tooltipInstance.dispose();
      new bootstrap.Tooltip(tooltipElement);
    }
  }

  // Track search engine switch
  function trackSearchEngineSwitch(engine) {
    if (typeof dataLayer !== 'undefined') {
      window.dataLayer = window.dataLayer || [];
      dataLayer.push({
        'event': 'search_engine_switch',
        'event_category': 'Search',
        'event_label': engine
      });
    }
  }

  // Function to update mobile search based on current toggle state
  function updateMobileSearch() {
    const toggle = document.getElementById('search-toggle');
    if (!toggle) return;

    const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
    if (!mobileSearchContainer) return;

    const mobileSphinxSearch = mobileSearchContainer.querySelector('#sphinx-search');
    const mobileGoogleSearch = mobileSearchContainer.querySelector('#google-search');

    if (mobileSphinxSearch && mobileGoogleSearch) {
      mobileSphinxSearch.style.display = toggle.checked ? 'none' : 'block';
      mobileGoogleSearch.style.display = toggle.checked ? 'block' : 'none';

      if (toggle.checked) {
        reinitializeGoogleSearch();
      }
    }
  }

  // Initialize desktop search toggle
  const toggle = document.getElementById('search-toggle');
  const sphinxSearch = document.getElementById('sphinx-search');
  const googleSearch = document.getElementById('google-search');
  handleSearchToggle(toggle, sphinxSearch, googleSearch);

  // Set placeholder text for Google search input
  const observer = new MutationObserver(function() {
    document.querySelectorAll('.gsc-input input').forEach(input => {
      if (input && !input.hasAttribute('data-placeholder-set')) {
        input.setAttribute('placeholder', 'Search the docs ...');
        input.setAttribute('data-placeholder-set', 'true');
      }
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // Fix for scroll jump issue - improved approach
  function setupSearchInputHandlers(input) {
    if (input.hasAttribute('data-scroll-fixed')) return;
    input.setAttribute('data-scroll-fixed', 'true');

    let lastScrollPosition = 0;
    let isTyping = false;
    let scrollTimeout;

    // Save position before typing starts
    input.addEventListener('keydown', () => {
      lastScrollPosition = window.scrollY;
      isTyping = true;

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);
    });

    // Only maintain scroll position during typing
    function maintainScroll() {
      if (document.activeElement === input && isTyping) {
        window.scrollTo(0, lastScrollPosition);
        requestAnimationFrame(maintainScroll);
      }
    }

    input.addEventListener('focus', () => {
      // Just store initial position but don't force it
      lastScrollPosition = window.scrollY;
    });

    input.addEventListener('input', () => {
      isTyping = true;
      window.scrollTo(0, lastScrollPosition);

      // Reset typing state after a short delay
      clearTimeout(scrollTimeout);
      scrollTimeout = setTimeout(() => {
        isTyping = false;
      }, 100);

      requestAnimationFrame(maintainScroll);
    });
  }

  // Apply to all search inputs and observe for new ones
  function applyToSearchInputs() {
    document.querySelectorAll('.search-container input, .gsc-input input').forEach(setupSearchInputHandlers);
  }

  applyToSearchInputs();

  const searchObserver = new MutationObserver(applyToSearchInputs);
  searchObserver.observe(document.body, { childList: true, subtree: true });

  // Watch for mobile menu creation
  const mobileMenuObserver = new MutationObserver(function(mutations) {
    for (const mutation of mutations) {
      if (!mutation.addedNodes.length) continue;

      // Style mobile search inputs
      document.querySelectorAll('.sidebar-header-items__end .navbar-item .search-container-wrapper .gsc-input input').forEach(input => {
        if (input) {
          input.setAttribute('placeholder', 'Search the docs ...');
          input.style.paddingLeft = '36px';
        }
      });

      // Check for mobile search container
      const mobileSearchContainer = document.querySelector('.sidebar-header-items__end .navbar-item .search-container-wrapper');
      if (!mobileSearchContainer) continue;

      const mobileToggle = mobileSearchContainer.querySelector('#search-toggle');
      if (mobileToggle && toggle) {
        // Sync mobile toggle with desktop toggle
        mobileToggle.checked = toggle.checked;
        updateMobileSearch();

        // Add event listener to mobile toggle if not already added
        if (!mobileToggle.hasAttribute('data-initialized')) {
          mobileToggle.setAttribute('data-initialized', 'true');
          mobileToggle.addEventListener('change', function() {
            // Sync desktop toggle with mobile toggle
            toggle.checked = this.checked;
            // Trigger change event on desktop toggle to update both
            toggle.dispatchEvent(new Event('change'));
          });
        }
      }
    }
  });

  mobileMenuObserver.observe(document.body, { childList: true, subtree: true });

  // Ensure Google CSE is properly loaded
  if (window.__gcse) {
    window.__gcse.callback = function() {
      // This will run after Google CSE is fully loaded
      if (toggle && toggle.checked) {
        reinitializeGoogleSearch();
      }
    };
  } else {
    window.__gcse = {
      callback: function() {
        if (toggle && toggle.checked) {
          reinitializeGoogleSearch();
        }
      }
    };
  }
});
</script>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/PyTorchKorea/tutorials-kr" title="한국어 튜토리얼 GitHub 저장소" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">한국어 튜토리얼 GitHub 저장소</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.kr/" title="파이토치 한국어 커뮤니티" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">파이토치 한국어 커뮤니티</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Recurrent...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Recurrent DQN: Training recurrent policies">
        <meta itemprop="position" content="1">
      </div>
    </div>

    
    <script>
      if ((window.location.href.indexOf("/unstable/") != -1) && (window.location.href.indexOf("/unstable/unstable_index") < 1)) {
        var div = '<div class="prototype-banner"><i class="fa fa-flask" aria-hidden="true"></i> <strong>Unstable feature:</strong> Not typically available in binary distributions like PyPI. Early stage for feedback and testing.</div>'
        document.addEventListener('DOMContentLoaded', function () {
          document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div);
        });
      }
    </script>
    
    
    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">intermediate/dqn_with_rnn_tutorial</div>
      <a id="colab-link" data-behavior="call-to-action-event" data-response="Run in Google Colab" target="_blank">
        <div id="google-colab-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-colab.svg" />
          <div class="call-to-action-desktop-view">Run in Google Colab</div>
          <div class="call-to-action-mobile-view">Colab</div>
        </div>
      </a>
      <a id="notebook-link" data-behavior="call-to-action-event" data-response="Download Notebook">
        <div id="download-notebook-link">
          <img class="call-to-action-notebook-img" src="../_static/img/pytorch-download.svg" />
          <div class="call-to-action-desktop-view">Download Notebook</div>
          <div class="call-to-action-mobile-view">Notebook</div>
        </div>
      </a>
      <a id="github-link" data-behavior="call-to-action-event" data-response="View on Github" target="_blank">
        <div id="github-view-link">
          <img class="call-to-action-img" src="../_static/img/pytorch-github.svg" />
          <div class="call-to-action-desktop-view">View on GitHub</div>
          <div class="call-to-action-mobile-view">GitHub</div>
        </div>
      </a>
    </div>
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p><a class="reference internal" href="#sphx-glr-download-intermediate-dqn-with-rnn-tutorial-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="recurrent-dqn-training-recurrent-policies">
<span id="sphx-glr-intermediate-dqn-with-rnn-tutorial-py"></span><h1>Recurrent DQN: Training recurrent policies<a class="headerlink" href="#recurrent-dqn-training-recurrent-policies" title="Link to this heading">#</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/vmoens">Vincent Moens</a></p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-mortar-board" viewBox="0 0 16 16" aria-hidden="true"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> What you will learn</div>
<ul class="simple">
<li><p class="sd-card-text">How to incorporating an RNN in an actor in TorchRL</p></li>
<li><p class="sd-card-text">How to use that memory-based policy with a replay buffer and a loss module</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm card-prerequisites docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-list-unordered" viewBox="0 0 16 16" aria-hidden="true"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Prerequisites</div>
<ul class="simple">
<li><p class="sd-card-text">PyTorch v2.0.0</p></li>
<li><p class="sd-card-text">gym[mujoco]</p></li>
<li><p class="sd-card-text">tqdm</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>Memory-based policies are crucial not only when the observations are partially
observable but also when the time dimension must be taken into account to
make informed decisions.</p>
<p>Recurrent neural network have long been a popular tool for memory-based
policies. The idea is to keep a recurrent state in memory between two
consecutive steps, and use this as an input to the policy along with the
current observation.</p>
<p>This tutorial shows how to incorporate an RNN in a policy using TorchRL.</p>
<p>Key learnings:</p>
<ul class="simple">
<li><p>Incorporating an RNN in an actor in TorchRL;</p></li>
<li><p>Using that memory-based policy with a replay buffer and a loss module.</p></li>
</ul>
<p>The core idea of using RNNs in TorchRL is to use TensorDict as a data carrier
for the hidden states from one step to another. We’ll build a policy that
reads the previous recurrent state from the current TensorDict, and writes the
current recurrent states in the TensorDict of the next state:</p>
<figure class="align-default">
<img alt="Data collection with a recurrent policy" src="../_images/rollout_recurrent.png" />
</figure>
<p>As this figure shows, our environment populates the TensorDict with zeroed recurrent
states which are read by the policy together with the observation to produce an
action, and recurrent states that will be used for the next step.
When the <code class="xref py py-func docutils literal notranslate"><span class="pre">step_mdp()</span></code> function is called, the recurrent states
from the next state are brought to the current TensorDict. Let’s see how this
is implemented in practice.</p>
<p>If you are running this in Google Colab, make sure you install the following dependencies:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>!pip3<span class="w"> </span>install<span class="w"> </span>torchrl
!pip3<span class="w"> </span>install<span class="w"> </span>gym<span class="o">[</span>mujoco<span class="o">]</span>
!pip3<span class="w"> </span>install<span class="w"> </span>tqdm
</pre></div>
</div>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDictModule</span> <span class="k">as</span> <span class="n">Mod</span><span class="p">,</span> <span class="n">TensorDictSequential</span> <span class="k">as</span> <span class="n">Seq</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors</span><span class="w"> </span><span class="kn">import</span> <span class="n">SyncDataCollector</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">LazyMemmapStorage</span><span class="p">,</span> <span class="n">TensorDictReplayBuffer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Compose</span><span class="p">,</span>
    <span class="n">ExplorationType</span><span class="p">,</span>
    <span class="n">GrayScale</span><span class="p">,</span>
    <span class="n">InitTracker</span><span class="p">,</span>
    <span class="n">ObservationNorm</span><span class="p">,</span>
    <span class="n">Resize</span><span class="p">,</span>
    <span class="n">RewardScaling</span><span class="p">,</span>
    <span class="n">set_exploration_type</span><span class="p">,</span>
    <span class="n">StepCounter</span><span class="p">,</span>
    <span class="n">ToTensorImage</span><span class="p">,</span>
    <span class="n">TransformedEnv</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.libs.gym</span><span class="w"> </span><span class="kn">import</span> <span class="n">GymEnv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConvNet</span><span class="p">,</span> <span class="n">EGreedyModule</span><span class="p">,</span> <span class="n">LSTMModule</span><span class="p">,</span> <span class="n">MLP</span><span class="p">,</span> <span class="n">QValueModule</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.objectives</span><span class="w"> </span><span class="kn">import</span> <span class="n">DQNLoss</span><span class="p">,</span> <span class="n">SoftUpdate</span>

<span class="n">is_fork</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">get_start_method</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;fork&quot;</span>
<span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_fork</span>
    <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="environment">
<h2>Environment<a class="headerlink" href="#environment" title="Link to this heading">#</a></h2>
<p>As usual, the first step is to build our environment: it helps us
define the problem and build the policy network accordingly. For this tutorial,
we’ll be running a single pixel-based instance of the CartPole gym
environment with some custom transforms: turning to grayscale, resizing to
84x84, scaling down the rewards and normalizing the observations.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>The <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.StepCounter.html#torchrl.envs.transforms.StepCounter" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">StepCounter</span></code></a> transform is accessory. Since the CartPole
task goal is to make trajectories as long as possible, counting the steps
can help us track the performance of our policy.</p>
</div>
<p>Two transforms are important for the purpose of this tutorial:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.InitTracker.html#torchrl.envs.transforms.InitTracker" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">InitTracker</span></code></a> will stamp the
calls to <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.EnvBase.html#id1" title="(torchrl v0.0에서)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reset()</span></code></a> by adding a <code class="docutils literal notranslate"><span class="pre">&quot;is_init&quot;</span></code>
boolean mask in the TensorDict that will track which steps require a reset
of the RNN hidden states.</p></li>
<li><p>The <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.TensorDictPrimer.html#torchrl.envs.transforms.TensorDictPrimer" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDictPrimer</span></code></a> transform is a bit more
technical. It is not required to use RNN policies. However, it
instructs the environment (and subsequently the collector) that some extra
keys are to be expected. Once added, a call to <cite>env.reset()</cite> will populate
the entries indicated in the primer with zeroed tensors. Knowing that
these tensors are expected by the policy, the collector will pass them on
during collection. Eventually, we’ll be storing our hidden states in the
replay buffer, which will help us bootstrap the computation of the
RNN operations in the loss module (which would otherwise be initiated
with 0s). In summary: not including this transform will not impact hugely
the training of our policy, but it will make the recurrent keys disappear
from the collected data and the replay buffer, which will in turn lead to
a slightly less optimal training.
Fortunately, the <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.LSTMModule.html#torchrl.modules.LSTMModule" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTMModule</span></code></a> we propose is
equipped with a helper method to build just that transform for us, so
we can wait until we build it!</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">TransformedEnv</span><span class="p">(</span>
    <span class="n">GymEnv</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">,</span> <span class="n">from_pixels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
    <span class="n">Compose</span><span class="p">(</span>
        <span class="n">ToTensorImage</span><span class="p">(),</span>
        <span class="n">GrayScale</span><span class="p">(),</span>
        <span class="n">Resize</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">84</span><span class="p">),</span>
        <span class="n">StepCounter</span><span class="p">(),</span>
        <span class="n">InitTracker</span><span class="p">(),</span>
        <span class="n">RewardScaling</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
        <span class="n">ObservationNorm</span><span class="p">(</span><span class="n">standard_normal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pixels&quot;</span><span class="p">]),</span>
    <span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
Users of this version of Gym should be able to simply replace &#39;import gym&#39; with &#39;import gymnasium as gym&#39; in the vast majority of cases.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
</pre></div>
</div>
<p>As always, we need to initialize manually our normalization constants:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">transform</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">init_stats</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">reduce_dim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">cat_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">td</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="policy">
<h2>Policy<a class="headerlink" href="#policy" title="Link to this heading">#</a></h2>
<p>Our policy will have 3 components: a <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.ConvNet.html#torchrl.modules.ConvNet" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvNet</span></code></a>
backbone, an <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.LSTMModule.html#torchrl.modules.LSTMModule" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTMModule</span></code></a> memory layer and a shallow
<a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.MLP.html#torchrl.modules.MLP" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLP</span></code></a> block that will map the LSTM output onto the
action values.</p>
<section id="convolutional-network">
<h3>Convolutional network<a class="headerlink" href="#convolutional-network" title="Link to this heading">#</a></h3>
<p>We build a convolutional network flanked with a <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d" title="(PyTorch v2.9에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.AdaptiveAvgPool2d</span></code></a>
that will squash the output in a vector of size 64. The <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.ConvNet.html#torchrl.modules.ConvNet" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvNet</span></code></a>
can assist us with this:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">feature</span> <span class="o">=</span> <span class="n">Mod</span><span class="p">(</span>
    <span class="n">ConvNet</span><span class="p">(</span>
        <span class="n">num_cells</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span>
        <span class="n">squeeze_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">aggregator_class</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">,</span>
        <span class="n">aggregator_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;output_size&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)},</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pixels&quot;</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;embed&quot;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>we execute the first module on a batch of data to gather the size of the
output vector:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">n_cells</span> <span class="o">=</span> <span class="n">feature</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">())[</span><span class="s2">&quot;embed&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="lstm-module">
<h3>LSTM Module<a class="headerlink" href="#lstm-module" title="Link to this heading">#</a></h3>
<p>TorchRL provides a specialized <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.LSTMModule.html#torchrl.modules.LSTMModule" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTMModule</span></code></a> class
to incorporate LSTMs in your code-base. It is a <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="(tensordict v0.10에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDictModuleBase</span></code></a>
subclass: as such, it has a set of <code class="docutils literal notranslate"><span class="pre">in_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">out_keys</span></code> that indicate
what values should be expected to be read and written/updated during the
execution of the module. The class comes with customizable predefined
values for these attributes to facilitate its construction.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p><em>Usage limitations</em>: The class supports almost all LSTM features such as
dropout or multi-layered LSTMs.
However, to respect TorchRL’s conventions, this LSTM must have the <code class="docutils literal notranslate"><span class="pre">batch_first</span></code>
attribute set to <code class="docutils literal notranslate"><span class="pre">True</span></code> which is <strong>not</strong> the default in PyTorch. However,
our <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.LSTMModule.html#torchrl.modules.LSTMModule" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTMModule</span></code></a> changes this default
behavior, so we’re good with a native call.</p>
<p>Also, the LSTM cannot have a <code class="docutils literal notranslate"><span class="pre">bidirectional</span></code> attribute set to <code class="docutils literal notranslate"><span class="pre">True</span></code> as
this wouldn’t be usable in online settings. In this case, the default value
is the correct one.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">lstm</span> <span class="o">=</span> <span class="n">LSTMModule</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="n">n_cells</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">in_key</span><span class="o">=</span><span class="s2">&quot;embed&quot;</span><span class="p">,</span>
    <span class="n">out_key</span><span class="o">=</span><span class="s2">&quot;embed&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Let us look at the LSTM Module class, specifically its in and out_keys:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;in_keys&quot;</span><span class="p">,</span> <span class="n">lstm</span><span class="o">.</span><span class="n">in_keys</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;out_keys&quot;</span><span class="p">,</span> <span class="n">lstm</span><span class="o">.</span><span class="n">out_keys</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>in_keys [&#39;embed&#39;, &#39;recurrent_state_h&#39;, &#39;recurrent_state_c&#39;, &#39;is_init&#39;]
out_keys [&#39;embed&#39;, (&#39;next&#39;, &#39;recurrent_state_h&#39;), (&#39;next&#39;, &#39;recurrent_state_c&#39;)]
</pre></div>
</div>
<p>We can see that these values contain the key we indicated as the in_key (and out_key)
as well as recurrent key names. The out_keys are preceded by a “next” prefix
that indicates that they will need to be written in the “next” TensorDict.
We use this convention (which can be overridden by passing the in_keys/out_keys
arguments) to make sure that a call to <code class="xref py py-func docutils literal notranslate"><span class="pre">step_mdp()</span></code> will
move the recurrent state to the root TensorDict, making it available to the
RNN during the following call (see figure in the intro).</p>
<p>As mentioned earlier, we have one more optional transform to add to our
environment to make sure that the recurrent states are passed to the buffer.
The <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.LSTMModule.html#id0" title="(torchrl v0.0에서)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">make_tensordict_primer()</span></code></a> method does
exactly that:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">append_transform</span><span class="p">(</span><span class="n">lstm</span><span class="o">.</span><span class="n">make_tensordict_primer</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>TransformedEnv(
    env=GymEnv(env=CartPole-v1, batch_size=torch.Size([]), device=cpu),
    transform=Compose(
            ToTensorImage(keys=[&#39;pixels&#39;]),
            GrayScale(keys=[&#39;pixels&#39;]),
            Resize(w=84, h=84, interpolation=InterpolationMode.BILINEAR, keys=[&#39;pixels&#39;]),
            StepCounter(keys=[]),
            InitTracker(keys=[]),
            RewardScaling(loc=0.0000, scale=0.1000, keys=[&#39;reward&#39;]),
            ObservationNorm(keys=[&#39;pixels&#39;]),
            TensorDictPrimer(primers=Composite(
                recurrent_state_h: UnboundedContinuous(
                    shape=torch.Size([1, 128]),
                    space=ContinuousBox(
                        low=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True),
                        high=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True)),
                    device=cpu,
                    dtype=torch.float32,
                    domain=continuous),
                recurrent_state_c: UnboundedContinuous(
                    shape=torch.Size([1, 128]),
                    space=ContinuousBox(
                        low=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True),
                        high=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True)),
                    device=cpu,
                    dtype=torch.float32,
                    domain=continuous),
                device=cpu,
                shape=torch.Size([]),
                data_cls=None), default_value={&#39;recurrent_state_h&#39;: 0.0, &#39;recurrent_state_c&#39;: 0.0}, random=None)))
</pre></div>
</div>
<p>and that’s it! We can print the environment to check that everything looks good now
that we have added the primer:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>TransformedEnv(
    env=GymEnv(env=CartPole-v1, batch_size=torch.Size([]), device=cpu),
    transform=Compose(
            ToTensorImage(keys=[&#39;pixels&#39;]),
            GrayScale(keys=[&#39;pixels&#39;]),
            Resize(w=84, h=84, interpolation=InterpolationMode.BILINEAR, keys=[&#39;pixels&#39;]),
            StepCounter(keys=[]),
            InitTracker(keys=[]),
            RewardScaling(loc=0.0000, scale=0.1000, keys=[&#39;reward&#39;]),
            ObservationNorm(keys=[&#39;pixels&#39;]),
            TensorDictPrimer(primers=Composite(
                recurrent_state_h: UnboundedContinuous(
                    shape=torch.Size([1, 128]),
                    space=ContinuousBox(
                        low=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True),
                        high=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True)),
                    device=cpu,
                    dtype=torch.float32,
                    domain=continuous),
                recurrent_state_c: UnboundedContinuous(
                    shape=torch.Size([1, 128]),
                    space=ContinuousBox(
                        low=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True),
                        high=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True)),
                    device=cpu,
                    dtype=torch.float32,
                    domain=continuous),
                device=cpu,
                shape=torch.Size([]),
                data_cls=None), default_value={&#39;recurrent_state_h&#39;: 0.0, &#39;recurrent_state_c&#39;: 0.0}, random=None)))
</pre></div>
</div>
</section>
<section id="mlp">
<h3>MLP<a class="headerlink" href="#mlp" title="Link to this heading">#</a></h3>
<p>We use a single-layer MLP to represent the action values we’ll be using for
our policy.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
    <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_cells</span><span class="o">=</span><span class="p">[</span>
        <span class="mi">64</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>and fill the bias with zeros:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">mlp</span> <span class="o">=</span> <span class="n">Mod</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span> <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;embed&quot;</span><span class="p">],</span> <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;action_value&quot;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="using-the-q-values-to-select-an-action">
<h3>Using the Q-Values to select an action<a class="headerlink" href="#using-the-q-values-to-select-an-action" title="Link to this heading">#</a></h3>
<p>The last part of our policy is the Q-Value Module.
The Q-Value module <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.QValueModule.html#torchrl.modules.tensordict_module.QValueModule" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QValueModule</span></code></a>
will read the <code class="docutils literal notranslate"><span class="pre">&quot;action_values&quot;</span></code> key that is produced by our MLP and
from it, gather the action that has the maximum value.
The only thing we need to do is to specify the action space, which can be done
either by passing a string or an action-spec. This allows us to use
Categorical (sometimes called “sparse”) encoding or the one-hot version of it.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">qval</span> <span class="o">=</span> <span class="n">QValueModule</span><span class="p">(</span><span class="n">spec</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>TorchRL also provides a wrapper class <code class="xref py py-class docutils literal notranslate"><span class="pre">torchrl.modules.QValueActor</span></code> that
wraps a module in a Sequential together with a <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.QValueModule.html#torchrl.modules.tensordict_module.QValueModule" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QValueModule</span></code></a>
like we are doing explicitly here. There is little advantage to do this
and the process is less transparent, but the end results will be similar to
what we do here.</p>
</div>
<p>We can now put things together in a <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictSequential.html#tensordict.nn.TensorDictSequential" title="(tensordict v0.10에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDictSequential</span></code></a></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">stoch_policy</span> <span class="o">=</span> <span class="n">Seq</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">lstm</span><span class="p">,</span> <span class="n">mlp</span><span class="p">,</span> <span class="n">qval</span><span class="p">)</span>
</pre></div>
</div>
<p>DQN being a deterministic algorithm, exploration is a crucial part of it.
We’ll be using an <span class="math">\(\epsilon\)</span>-greedy policy with an epsilon of 0.2 decaying
progressively to 0.
This decay is achieved via a call to <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.EGreedyModule.html#torchrl.modules.EGreedyModule.step" title="(torchrl v0.0에서)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">step()</span></code></a>
(see training loop below).</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">exploration_module</span> <span class="o">=</span> <span class="n">EGreedyModule</span><span class="p">(</span>
    <span class="n">annealing_num_steps</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">,</span> <span class="n">spec</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">,</span> <span class="n">eps_init</span><span class="o">=</span><span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">stoch_policy</span> <span class="o">=</span> <span class="n">Seq</span><span class="p">(</span>
    <span class="n">stoch_policy</span><span class="p">,</span>
    <span class="n">exploration_module</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-the-model-for-the-loss">
<h3>Using the model for the loss<a class="headerlink" href="#using-the-model-for-the-loss" title="Link to this heading">#</a></h3>
<p>The model as we’ve built it is well equipped to be used in sequential settings.
However, the class <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM" title="(PyTorch v2.9에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.LSTM</span></code></a> can use a cuDNN-optimized backend
to run the RNN sequence faster on GPU device. We would not want to miss
such an opportunity to speed up our training loop!
To use it, we just need to tell the LSTM module to run on “recurrent-mode”
when used by the loss.
As we’ll usually want to have two copies of the LSTM module, we do this by
calling a <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.LSTMModule.html#torchrl.modules.LSTMModule.set_recurrent_mode" title="(torchrl v0.0에서)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_recurrent_mode()</span></code></a> method that
will return a new instance of the LSTM (with shared weights) that will
assume that the input data is sequential in nature.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">policy</span> <span class="o">=</span> <span class="n">Seq</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">lstm</span><span class="o">.</span><span class="n">set_recurrent_mode</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span> <span class="n">mlp</span><span class="p">,</span> <span class="n">qval</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-pytb notranslate"><div class="highlight"><pre><span></span><span class="gt">Traceback (most recent call last):</span>
  File <span class="nb">&quot;/workspace/tutorials-kr/intermediate_source/dqn_with_rnn_tutorial.py&quot;</span>, line <span class="m">345</span>, in <span class="n">&lt;module&gt;</span>
<span class="w">    </span><span class="n">policy</span> <span class="o">=</span> <span class="n">Seq</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">lstm</span><span class="o">.</span><span class="n">set_recurrent_mode</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span> <span class="n">mlp</span><span class="p">,</span> <span class="n">qval</span><span class="p">)</span>
<span class="w">                          </span><span class="pm">^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span>
  File <span class="nb">&quot;/opt/conda/lib/python3.11/site-packages/torchrl/modules/tensordict_module/rnn.py&quot;</span>, line <span class="m">671</span>, in <span class="n">set_recurrent_mode</span>
<span class="w">    </span><span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
<span class="gr">RuntimeError</span>: <span class="n">The lstm.set_recurrent_mode() API has been removed in v0.8. To set the recurrent mode, use the :class:`~torchrl.modules.set_recurrent_mode` context manager or the `default_recurrent_mode` keyword argument in the constructor.</span>
</pre></div>
</div>
<p>Because we still have a couple of uninitialized parameters we should
initialize them before creating an optimizer and such.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">policy</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">())</span>
</pre></div>
</div>
</section>
</section>
<section id="dqn-loss">
<h2>DQN Loss<a class="headerlink" href="#dqn-loss" title="Link to this heading">#</a></h2>
<p>Out DQN loss requires us to pass the policy and, again, the action-space.
While this may seem redundant, it is important as we want to make sure that
the <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.DQNLoss.html#torchrl.objectives.DQNLoss" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DQNLoss</span></code></a> and the <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.modules.tensordict_module.QValueModule.html#torchrl.modules.tensordict_module.QValueModule" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QValueModule</span></code></a>
classes are compatible, but aren’t strongly dependent on each other.</p>
<p>To use the Double-DQN, we ask for a <code class="docutils literal notranslate"><span class="pre">delay_value</span></code> argument that will
create a non-differentiable copy of the network parameters to be used
as a target network.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">DQNLoss</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">,</span> <span class="n">delay_value</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Since we are using a double DQN, we need to update the target parameters.
We’ll use a  <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.objectives.SoftUpdate.html#torchrl.objectives.SoftUpdate" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">SoftUpdate</span></code></a> instance to carry out
this work.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">updater</span> <span class="o">=</span> <span class="n">SoftUpdate</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="collector-and-replay-buffer">
<h2>Collector and replay buffer<a class="headerlink" href="#collector-and-replay-buffer" title="Link to this heading">#</a></h2>
<p>We build the simplest data collector there is. We’ll try to train our algorithm
with a million frames, extending the buffer with 50 frames at a time. The buffer
will be designed to store 20 thousands trajectories of 50 steps each.
At each optimization step (16 per data collection), we’ll collect 4 items
from our buffer, for a total of 200 transitions.
We’ll use a <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.data.replay_buffers.LazyMemmapStorage.html#torchrl.data.replay_buffers.LazyMemmapStorage" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LazyMemmapStorage</span></code></a> storage to keep the data
on disk.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>For the sake of efficiency, we’re only running a few thousands iterations
here. In a real setting, the total number of frames should be set to 1M.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">collector</span> <span class="o">=</span> <span class="n">SyncDataCollector</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">stoch_policy</span><span class="p">,</span> <span class="n">frames_per_batch</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">total_frames</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">rb</span> <span class="o">=</span> <span class="n">TensorDictReplayBuffer</span><span class="p">(</span>
    <span class="n">storage</span><span class="o">=</span><span class="n">LazyMemmapStorage</span><span class="p">(</span><span class="mi">20_000</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">prefetch</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-loop">
<h2>Training loop<a class="headerlink" href="#training-loop" title="Link to this heading">#</a></h2>
<p>To keep track of the progress, we will run the policy in the environment once
every 50 data collection, and plot the results after training.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">utd</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">)</span>
<span class="n">longest</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">traj_lens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">collector</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;Let us print the first batch of data.</span><span class="se">\n</span><span class="s2">Pay attention to the key names &quot;</span>
            <span class="s2">&quot;which will reflect what can be found in this data structure, in particular: &quot;</span>
            <span class="s2">&quot;the output of the QValueModule (action_values, action and chosen_action_value),&quot;</span>
            <span class="s2">&quot;the &#39;is_init&#39; key that will tell us if a step is initial or not, and the &quot;</span>
            <span class="s2">&quot;recurrent_state keys.</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">data</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
    <span class="c1"># it is important to pass data that is not flattened</span>
    <span class="n">rb</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to_tensordict</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">utd</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">rb</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">loss_vals</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">loss_vals</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">longest</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">longest</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;step_count&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;steps: </span><span class="si">{</span><span class="n">longest</span><span class="si">}</span><span class="s2">, loss_val: </span><span class="si">{</span><span class="n">loss_vals</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2">, action_spread: </span><span class="si">{</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;action&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">exploration_module</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
    <span class="n">updater</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">set_exploration_type</span><span class="p">(</span><span class="n">ExplorationType</span><span class="o">.</span><span class="n">DETERMINISTIC</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">rollout</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">rollout</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">stoch_policy</span><span class="p">)</span>
        <span class="n">traj_lens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rollout</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;step_count&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
<p>Let’s plot our results:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">traj_lens</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_lens</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Test collection&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Test trajectory lengths&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>We have seen how an RNN can be incorporated in a policy in TorchRL.
You should now be able:</p>
<ul class="simple">
<li><p>Create an LSTM module that acts as a <a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModule.html#tensordict.nn.TensorDictModule" title="(tensordict v0.10에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDictModule</span></code></a></p></li>
<li><p>Indicate to the LSTM module that a reset is needed via an <a class="reference external" href="https://docs.pytorch.org/rl/stable/reference/generated/torchrl.envs.transforms.InitTracker.html#torchrl.envs.transforms.InitTracker" title="(torchrl v0.0에서)"><code class="xref py py-class docutils literal notranslate"><span class="pre">InitTracker</span></code></a>
transform</p></li>
<li><p>Incorporate this module in a policy and in a loss module</p></li>
<li><p>Make sure that the collector is made aware of the recurrent state entries
such that they can be stored in the replay buffer along with the rest of
the data</p></li>
</ul>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The TorchRL documentation can be found <a class="reference external" href="https://pytorch.org/rl/">here</a>.</p></li>
</ul>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 6.720 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-intermediate-dqn-with-rnn-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/224d2179034ef4c00cd9b86f2976062a/dqn_with_rnn_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">dqn_with_rnn_tutorial.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/14b68b9764c79afe8ef88b11fc27bff7/dqn_with_rnn_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">dqn_with_rnn_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/0fc9a70355f8b8b9fb173bb9e1f5c7e0/dqn_with_rnn_tutorial.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">dqn_with_rnn_tutorial.zip</span></code></a></p>
</div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
</div>

<div class="footer-info">
  <p class="copyright">
    
      
        © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      
      <br/>
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environment">Environment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy">Policy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-network">Convolutional network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-module">LSTM Module</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp">MLP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-q-values-to-select-an-action">Using the Q-Values to select an action</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-model-for-the-loss">Using the model for the loss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dqn-loss">DQN Loss</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collector-and-replay-buffer">Collector and replay buffer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop">Training loop</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a></li>
</ul>
  </nav></div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>PyTorchKorea @ GitHub</h2>
        <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
        <a class="with-right-arrow" href="https://github.com/PyTorchKorea">GitHub로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 튜토리얼</h2>
        <p>한국어로 번역 중인 파이토치 튜토리얼을 만나보세요.</p>
        <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>한국어 커뮤니티</h2>
        <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
        <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.kr/" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
          <li><a href="https://pytorch.kr/about">사용자 모임 소개</a></li>
          <li><a href="https://pytorch.kr/contributors">기여해주신 분들</a></li>
          <li><a href="https://pytorch.kr/resources/">리소스</a></li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a></li>
        </ul>
      </div>
    </div>

    <div class="trademark-disclaimer">
      <ul>
        <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
        <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 규정은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
      </ul>
    </div>
  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2018-2025, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorchKR).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6 버전으로 생성되었습니다.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Recurrent DQN: Training recurrent policies",
       "headline": "Recurrent DQN: Training recurrent policies",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/intermediate/dqn_with_rnn_tutorial.html",
       "articleBody": "\ucc38\uace0 Go to the end to download the full example code. Recurrent DQN: Training recurrent policies# Author: Vincent Moens What you will learn How to incorporating an RNN in an actor in TorchRL How to use that memory-based policy with a replay buffer and a loss module Prerequisites PyTorch v2.0.0 gym[mujoco] tqdm Overview# Memory-based policies are crucial not only when the observations are partially observable but also when the time dimension must be taken into account to make informed decisions. Recurrent neural network have long been a popular tool for memory-based policies. The idea is to keep a recurrent state in memory between two consecutive steps, and use this as an input to the policy along with the current observation. This tutorial shows how to incorporate an RNN in a policy using TorchRL. Key learnings: Incorporating an RNN in an actor in TorchRL; Using that memory-based policy with a replay buffer and a loss module. The core idea of using RNNs in TorchRL is to use TensorDict as a data carrier for the hidden states from one step to another. We\u2019ll build a policy that reads the previous recurrent state from the current TensorDict, and writes the current recurrent states in the TensorDict of the next state: As this figure shows, our environment populates the TensorDict with zeroed recurrent states which are read by the policy together with the observation to produce an action, and recurrent states that will be used for the next step. When the step_mdp() function is called, the recurrent states from the next state are brought to the current TensorDict. Let\u2019s see how this is implemented in practice. If you are running this in Google Colab, make sure you install the following dependencies: !pip3 install torchrl !pip3 install gym[mujoco] !pip3 install tqdm Setup# import torch import tqdm from tensordict.nn import TensorDictModule as Mod, TensorDictSequential as Seq from torch import nn from torchrl.collectors import SyncDataCollector from torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer from torchrl.envs import ( Compose, ExplorationType, GrayScale, InitTracker, ObservationNorm, Resize, RewardScaling, set_exploration_type, StepCounter, ToTensorImage, TransformedEnv, ) from torchrl.envs.libs.gym import GymEnv from torchrl.modules import ConvNet, EGreedyModule, LSTMModule, MLP, QValueModule from torchrl.objectives import DQNLoss, SoftUpdate is_fork = multiprocessing.get_start_method() == \"fork\" device = ( torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device(\"cpu\") ) Environment# As usual, the first step is to build our environment: it helps us define the problem and build the policy network accordingly. For this tutorial, we\u2019ll be running a single pixel-based instance of the CartPole gym environment with some custom transforms: turning to grayscale, resizing to 84x84, scaling down the rewards and normalizing the observations. \ucc38\uace0 The StepCounter transform is accessory. Since the CartPole task goal is to make trajectories as long as possible, counting the steps can help us track the performance of our policy. Two transforms are important for the purpose of this tutorial: InitTracker will stamp the calls to reset() by adding a \"is_init\" boolean mask in the TensorDict that will track which steps require a reset of the RNN hidden states. The TensorDictPrimer transform is a bit more technical. It is not required to use RNN policies. However, it instructs the environment (and subsequently the collector) that some extra keys are to be expected. Once added, a call to env.reset() will populate the entries indicated in the primer with zeroed tensors. Knowing that these tensors are expected by the policy, the collector will pass them on during collection. Eventually, we\u2019ll be storing our hidden states in the replay buffer, which will help us bootstrap the computation of the RNN operations in the loss module (which would otherwise be initiated with 0s). In summary: not including this transform will not impact hugely the training of our policy, but it will make the recurrent keys disappear from the collected data and the replay buffer, which will in turn lead to a slightly less optimal training. Fortunately, the LSTMModule we propose is equipped with a helper method to build just that transform for us, so we can wait until we build it! env = TransformedEnv( GymEnv(\"CartPole-v1\", from_pixels=True, device=device), Compose( ToTensorImage(), GrayScale(), Resize(84, 84), StepCounter(), InitTracker(), RewardScaling(loc=0.0, scale=0.1), ObservationNorm(standard_normal=True, in_keys=[\"pixels\"]), ), ) Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality. Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade. Users of this version of Gym should be able to simply replace \u0027import gym\u0027 with \u0027import gymnasium as gym\u0027 in the vast majority of cases. See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information. As always, we need to initialize manually our normalization constants: env.transform[-1].init_stats(1000, reduce_dim=[0, 1, 2], cat_dim=0, keep_dims=[0]) td = env.reset() Policy# Our policy will have 3 components: a ConvNet backbone, an LSTMModule memory layer and a shallow MLP block that will map the LSTM output onto the action values. Convolutional network# We build a convolutional network flanked with a torch.nn.AdaptiveAvgPool2d that will squash the output in a vector of size 64. The ConvNet can assist us with this: feature = Mod( ConvNet( num_cells=[32, 32, 64], squeeze_output=True, aggregator_class=nn.AdaptiveAvgPool2d, aggregator_kwargs={\"output_size\": (1, 1)}, device=device, ), in_keys=[\"pixels\"], out_keys=[\"embed\"], ) we execute the first module on a batch of data to gather the size of the output vector: n_cells = feature(env.reset())[\"embed\"].shape[-1] LSTM Module# TorchRL provides a specialized LSTMModule class to incorporate LSTMs in your code-base. It is a TensorDictModuleBase subclass: as such, it has a set of in_keys and out_keys that indicate what values should be expected to be read and written/updated during the execution of the module. The class comes with customizable predefined values for these attributes to facilitate its construction. \ucc38\uace0 Usage limitations: The class supports almost all LSTM features such as dropout or multi-layered LSTMs. However, to respect TorchRL\u2019s conventions, this LSTM must have the batch_first attribute set to True which is not the default in PyTorch. However, our LSTMModule changes this default behavior, so we\u2019re good with a native call. Also, the LSTM cannot have a bidirectional attribute set to True as this wouldn\u2019t be usable in online settings. In this case, the default value is the correct one. lstm = LSTMModule( input_size=n_cells, hidden_size=128, device=device, in_key=\"embed\", out_key=\"embed\", ) Let us look at the LSTM Module class, specifically its in and out_keys: print(\"in_keys\", lstm.in_keys) print(\"out_keys\", lstm.out_keys) in_keys [\u0027embed\u0027, \u0027recurrent_state_h\u0027, \u0027recurrent_state_c\u0027, \u0027is_init\u0027] out_keys [\u0027embed\u0027, (\u0027next\u0027, \u0027recurrent_state_h\u0027), (\u0027next\u0027, \u0027recurrent_state_c\u0027)] We can see that these values contain the key we indicated as the in_key (and out_key) as well as recurrent key names. The out_keys are preceded by a \u201cnext\u201d prefix that indicates that they will need to be written in the \u201cnext\u201d TensorDict. We use this convention (which can be overridden by passing the in_keys/out_keys arguments) to make sure that a call to step_mdp() will move the recurrent state to the root TensorDict, making it available to the RNN during the following call (see figure in the intro). As mentioned earlier, we have one more optional transform to add to our environment to make sure that the recurrent states are passed to the buffer. The make_tensordict_primer() method does exactly that: env.append_transform(lstm.make_tensordict_primer()) TransformedEnv( env=GymEnv(env=CartPole-v1, batch_size=torch.Size([]), device=cpu), transform=Compose( ToTensorImage(keys=[\u0027pixels\u0027]), GrayScale(keys=[\u0027pixels\u0027]), Resize(w=84, h=84, interpolation=InterpolationMode.BILINEAR, keys=[\u0027pixels\u0027]), StepCounter(keys=[]), InitTracker(keys=[]), RewardScaling(loc=0.0000, scale=0.1000, keys=[\u0027reward\u0027]), ObservationNorm(keys=[\u0027pixels\u0027]), TensorDictPrimer(primers=Composite( recurrent_state_h: UnboundedContinuous( shape=torch.Size([1, 128]), space=ContinuousBox( low=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True), high=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous), recurrent_state_c: UnboundedContinuous( shape=torch.Size([1, 128]), space=ContinuousBox( low=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True), high=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous), device=cpu, shape=torch.Size([]), data_cls=None), default_value={\u0027recurrent_state_h\u0027: 0.0, \u0027recurrent_state_c\u0027: 0.0}, random=None))) and that\u2019s it! We can print the environment to check that everything looks good now that we have added the primer: print(env) TransformedEnv( env=GymEnv(env=CartPole-v1, batch_size=torch.Size([]), device=cpu), transform=Compose( ToTensorImage(keys=[\u0027pixels\u0027]), GrayScale(keys=[\u0027pixels\u0027]), Resize(w=84, h=84, interpolation=InterpolationMode.BILINEAR, keys=[\u0027pixels\u0027]), StepCounter(keys=[]), InitTracker(keys=[]), RewardScaling(loc=0.0000, scale=0.1000, keys=[\u0027reward\u0027]), ObservationNorm(keys=[\u0027pixels\u0027]), TensorDictPrimer(primers=Composite( recurrent_state_h: UnboundedContinuous( shape=torch.Size([1, 128]), space=ContinuousBox( low=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True), high=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous), recurrent_state_c: UnboundedContinuous( shape=torch.Size([1, 128]), space=ContinuousBox( low=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True), high=Tensor(shape=torch.Size([1, 128]), device=cpu, dtype=torch.float32, contiguous=True)), device=cpu, dtype=torch.float32, domain=continuous), device=cpu, shape=torch.Size([]), data_cls=None), default_value={\u0027recurrent_state_h\u0027: 0.0, \u0027recurrent_state_c\u0027: 0.0}, random=None))) MLP# We use a single-layer MLP to represent the action values we\u2019ll be using for our policy. mlp = MLP( out_features=2, num_cells=[ 64, ], device=device, ) and fill the bias with zeros: mlp[-1].bias.data.fill_(0.0) mlp = Mod(mlp, in_keys=[\"embed\"], out_keys=[\"action_value\"]) Using the Q-Values to select an action# The last part of our policy is the Q-Value Module. The Q-Value module QValueModule will read the \"action_values\" key that is produced by our MLP and from it, gather the action that has the maximum value. The only thing we need to do is to specify the action space, which can be done either by passing a string or an action-spec. This allows us to use Categorical (sometimes called \u201csparse\u201d) encoding or the one-hot version of it. qval = QValueModule(spec=env.action_spec) \ucc38\uace0 TorchRL also provides a wrapper class torchrl.modules.QValueActor that wraps a module in a Sequential together with a QValueModule like we are doing explicitly here. There is little advantage to do this and the process is less transparent, but the end results will be similar to what we do here. We can now put things together in a TensorDictSequential stoch_policy = Seq(feature, lstm, mlp, qval) DQN being a deterministic algorithm, exploration is a crucial part of it. We\u2019ll be using an \\(\\epsilon\\)-greedy policy with an epsilon of 0.2 decaying progressively to 0. This decay is achieved via a call to step() (see training loop below). exploration_module = EGreedyModule( annealing_num_steps=1_000_000, spec=env.action_spec, eps_init=0.2 ) stoch_policy = Seq( stoch_policy, exploration_module, ) Using the model for the loss# The model as we\u2019ve built it is well equipped to be used in sequential settings. However, the class torch.nn.LSTM can use a cuDNN-optimized backend to run the RNN sequence faster on GPU device. We would not want to miss such an opportunity to speed up our training loop! To use it, we just need to tell the LSTM module to run on \u201crecurrent-mode\u201d when used by the loss. As we\u2019ll usually want to have two copies of the LSTM module, we do this by calling a set_recurrent_mode() method that will return a new instance of the LSTM (with shared weights) that will assume that the input data is sequential in nature. policy = Seq(feature, lstm.set_recurrent_mode(True), mlp, qval) Traceback (most recent call last): File \"/workspace/tutorials-kr/intermediate_source/dqn_with_rnn_tutorial.py\", line 345, in \u003cmodule\u003e policy = Seq(feature, lstm.set_recurrent_mode(True), mlp, qval) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/opt/conda/lib/python3.11/site-packages/torchrl/modules/tensordict_module/rnn.py\", line 671, in set_recurrent_mode raise RuntimeError( RuntimeError: The lstm.set_recurrent_mode() API has been removed in v0.8. To set the recurrent mode, use the :class:`~torchrl.modules.set_recurrent_mode` context manager or the `default_recurrent_mode` keyword argument in the constructor. Because we still have a couple of uninitialized parameters we should initialize them before creating an optimizer and such. policy(env.reset()) DQN Loss# Out DQN loss requires us to pass the policy and, again, the action-space. While this may seem redundant, it is important as we want to make sure that the DQNLoss and the QValueModule classes are compatible, but aren\u2019t strongly dependent on each other. To use the Double-DQN, we ask for a delay_value argument that will create a non-differentiable copy of the network parameters to be used as a target network. loss_fn = DQNLoss(policy, action_space=env.action_spec, delay_value=True) Since we are using a double DQN, we need to update the target parameters. We\u2019ll use a SoftUpdate instance to carry out this work. updater = SoftUpdate(loss_fn, eps=0.95) optim = torch.optim.Adam(policy.parameters(), lr=3e-4) Collector and replay buffer# We build the simplest data collector there is. We\u2019ll try to train our algorithm with a million frames, extending the buffer with 50 frames at a time. The buffer will be designed to store 20 thousands trajectories of 50 steps each. At each optimization step (16 per data collection), we\u2019ll collect 4 items from our buffer, for a total of 200 transitions. We\u2019ll use a LazyMemmapStorage storage to keep the data on disk. \ucc38\uace0 For the sake of efficiency, we\u2019re only running a few thousands iterations here. In a real setting, the total number of frames should be set to 1M. collector = SyncDataCollector(env, stoch_policy, frames_per_batch=50, total_frames=200, device=device) rb = TensorDictReplayBuffer( storage=LazyMemmapStorage(20_000), batch_size=4, prefetch=10 ) Training loop# To keep track of the progress, we will run the policy in the environment once every 50 data collection, and plot the results after training. utd = 16 pbar = tqdm.tqdm(total=1_000_000) longest = 0 traj_lens = [] for i, data in enumerate(collector): if i == 0: print( \"Let us print the first batch of data.\\nPay attention to the key names \" \"which will reflect what can be found in this data structure, in particular: \" \"the output of the QValueModule (action_values, action and chosen_action_value),\" \"the \u0027is_init\u0027 key that will tell us if a step is initial or not, and the \" \"recurrent_state keys.\\n\", data, ) pbar.update(data.numel()) # it is important to pass data that is not flattened rb.extend(data.unsqueeze(0).to_tensordict().cpu()) for _ in range(utd): s = rb.sample().to(device, non_blocking=True) loss_vals = loss_fn(s) loss_vals[\"loss\"].backward() optim.step() optim.zero_grad() longest = max(longest, data[\"step_count\"].max().item()) pbar.set_description( f\"steps: {longest}, loss_val: {loss_vals[\u0027loss\u0027].item(): 4.4f}, action_spread: {data[\u0027action\u0027].sum(0)}\" ) exploration_module.step(data.numel()) updater.step() with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad(): rollout = env.rollout(10000, stoch_policy) traj_lens.append(rollout.get((\"next\", \"step_count\")).max().item()) Let\u2019s plot our results: if traj_lens: from matplotlib import pyplot as plt plt.plot(traj_lens) plt.xlabel(\"Test collection\") plt.title(\"Test trajectory lengths\") Conclusion# We have seen how an RNN can be incorporated in a policy in TorchRL. You should now be able: Create an LSTM module that acts as a TensorDictModule Indicate to the LSTM module that a reset is needed via an InitTracker transform Incorporate this module in a policy and in a loss module Make sure that the collector is made aware of the recurrent state entries such that they can be stored in the replay buffer along with the rest of the data Further Reading# The TorchRL documentation can be found here. Total running time of the script: (0 minutes 6.720 seconds) Download Jupyter notebook: dqn_with_rnn_tutorial.ipynb Download Python source code: dqn_with_rnn_tutorial.py Download zipped: dqn_with_rnn_tutorial.zip",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "../_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/intermediate/dqn_with_rnn_tutorial.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>